[{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don’t think that would be very different from public the source code. ","date":"2022-03-19","objectID":"/cmu15445_project2/:0:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Task #1 - Page Layouts Because we want to persist the hash table instead of rebuild it everytime, we need to design the layout that we use to store the hash table in the disks. We have implemented the BufferPoolManager in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of Page is stored in the page frame? In this task, we need to create two kinds of Pages for our hash table: Hash table directory page Hash table bucket page Since we are using the previous allocated memory space (we cast the data_ field of Page to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary. For bitwise operations, because the bitmap is based on char arraies, we can only do bitwise operations char by char (at least, this is what I find). ","date":"2022-03-19","objectID":"/cmu15445_project2/:1:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Hash Table Directory Page This kind of page stores metadata for the hash table. The most important part is the bucket address table. At this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks. ","date":"2022-03-19","objectID":"/cmu15445_project2/:1:1","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Hash Bucket Page Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs. There are two bitmaps that we used to indicate if a slot contains valid KV: readable_ occupied_ occupied_ actually has no specially meaning in enxtendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases. The page layout itself is very straightforward. Only the bitwise operations are a little annoying. ","date":"2022-03-19","objectID":"/cmu15445_project2/:1:2","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Task #2 - Hash Table Implementation This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages. I recommand to implement the search at the very beginning then other funtions, since other operations also need search to check if a specific KV pair is existed. ","date":"2022-03-19","objectID":"/cmu15445_project2/:2:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Search For a given key, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key. Because we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair. ","date":"2022-03-19","objectID":"/cmu15445_project2/:2:1","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Insert The core conponents of insertion part is split. Highly recommand to use pen and paper to figure how bucket split works before coding. The insertion procedure is as follows: Find the right bucket If there is room in the bucket, insert the KV pair If there is no room -\u003e split the bucket How to split one bucket? Assume we call the split target split bucket and the newly created bucket image bucket. If $ global\\_depth == local\\_depth $: Increase the global_depth by 1, so double the table size The following steps are same as situation $ global\\_depth \u003e local\\_depth $ If $global\\_depth \u003e local\\_depth$: Allocate a new page for the image bucket Adjust the entries in the bucket address table leave the half of the entries pointing to the split bucket set all the remaining entries to point to the image bucket also increase the local_depth by 1 because we need one more bit to separate them Rehash KV pairs in the split bucket Reattemp the insertion Should use the Insert() function because we may need more splits Add your own test cases. The given test case is so small and cannot cover all situations. ","date":"2022-03-19","objectID":"/cmu15445_project2/:2:2","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Remove Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us. The project describtion gives a fairly thorough instructions for merge. Follow the instruction is enough. Shrinking the table is very straightford since we use LSB to assign KV pairs to different buckets. ","date":"2022-03-19","objectID":"/cmu15445_project2/:2:3","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Task #3 Concurrency Control Try coarse-grained latch (lock) first, then reduce the latch range. No special comments for this. You can do it! ","date":"2022-03-19","objectID":"/cmu15445_project2/:3:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":"Result Because the clang-tidy issue of Gradescope, I cannot test my code yet. However, I tried some local test cases that covers splits (two types) and small buffer pools (to enforce page swap). It seems the code works well in the basic one thread situation. ","date":"2022-03-19","objectID":"/cmu15445_project2/:4:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/cmu15445_project2/"},{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don’t think that would be very different from public the source code. ","date":"2022-03-14","objectID":"/cmu15445_project1/:0:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/cmu15445_project1/"},{"categories":["projects"],"content":"Task #1 - LRU Replacement Policy BufferPoolManger contains all the frames. LRUReplacer is an implementation of the Replacer and it helps BufferPoolManger to manage these frames. This LRU policy is not very “LRU” in my opinion. Refer the test cases we can see, if we Unpin the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation). You may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how BufferPoolManager utlizes the LRUReplacer. Actually, it is the BufferPoolMangerInstance managing the pages in the buffer. The LRUReplacer itself only contains page frames that we can use for storing new pages. In other words, the reference (pin) count of pages that existed in the frames that in the LRUReplacer is zero, and we can swap them out in anytime. Since we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the include/common/rwlatch.h to learn how Bustub (i.e., the DBMS that we are implementing) uses them. ","date":"2022-03-14","objectID":"/cmu15445_project1/:1:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/cmu15445_project1/"},{"categories":["projects"],"content":"Task #2 - Buffer Pool Manager Instance We use Page as the container to manage the pages of our DB storage engine. Page objects are pre-allocated for each frame in the buffer pool. We reuse existed Page objects instead of creating a new one for every newly read in pages. We pin a page when we want to use it, and we unpin a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, pin and unpin are hints to tell the pool manager which page it can swap out if there is no free space. Caution, frame and page are refering to different concepts. page is a chunk of data that stored in our DBMS; frame is a slot in the page buffer that has the same size as the page. So, use frame_id_t and page_id_t at the right place. The comments in the base code is not very clear. They use “page” to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding. BufferPoolManager uses four components to manage pages and frames: page_table_: a map that stores the mapping relationship between page_id and frame_id. free_list_: a linked-list that stores the free frames. replacer_: a LRUReplacer that stores used frames with zero pin count. pages_: stores pre-allocated Page objects. In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable. BufferPoolManager is the friend of Page, so we can access the private members of Page. (This is a good example about when to use friend – when we need to change some member variables but we do not want give setters so that every one can change them.) If we can do three things right, this task is not that difficult: Move page to/from LRU. Know when to flush a page. (Read points are very clear). Which page metadata we need to update. Critical hints: Do read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned false in a function, however, they assume we should return true in that case. Do not use your own judgement, just follow the description.) What will happen if we NewPage() then Unpin() the same page immediately? Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you. ","date":"2022-03-14","objectID":"/cmu15445_project1/:2:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/cmu15445_project1/"},{"categories":["projects"],"content":"Task #3 - Parallel Buffer Pool Manager Task 3 is very straightforward. If our BufferPoolManagerInstance is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance. Some people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us. ","date":"2022-03-14","objectID":"/cmu15445_project1/:3:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/cmu15445_project1/"},{"categories":["projects"],"content":"Result Passed all test cases with full grades. ","date":"2022-03-14","objectID":"/cmu15445_project1/:4:0","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/cmu15445_project1/"},{"categories":["papers"],"content":"Short Summary This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore. The main ideas of BlueStore are: Avoid using local file system to store and represent Ceph objects Use KV-store to provide transaction mechanism instead of build it by ourself ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:1:0","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"What’s the problem There is a software called storage backend in Ceph. The storage backend is responsible to accept requests from upper layer of Ceph and do the real I/O on storage devices. Ceph used to use commonly used local file systems based storage backend (i.e., FileStore). FileStore works, but not that well. There are mainly three drawbacks in FileStore: Hard to implement efficient transactions on top of existing file systems The local file system' metadata performance is not great (e.g., enumerating directories, ordering in the return result) Hard to adopt emerging storage hardware that abandon the venrable block interface (e.g., Zone divecies) ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:2:0","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"Why the problem is interesting Because storage backends do the real I/O job, the performance storage backends domains the performance of the whole Ceph system. For years, the developers of Ceph have had a lot of troubles when using local file systems to build storage backends ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:3:0","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"The Core Ideas ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:4:0","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"The problems of FileStore Transaction: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL leveraging file system’s transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces. User space WAL: has consistency problem (not atom operations, because it is a logical WAL, it use read/write system calls to write/read data to/from logging part). The cost for handle the problem is expensive. Also slow read-modify-write and double-write (logging all data) problems. Using KV-store: This is the cure. However, there is still some unnecessary file system overhead like journaling of journal problem Slow metadata operations: enumeration is slow. The read result from a object sets should in order, which file systems do not do. We need to do sorting after read. To reduce the sorting overhead, Ceph limits the number of files in a directory, which introduces directory splition. The dir splition has overhead. Does not support new storage hardware: new storage devices may need modifications in the existing file systems. If Ceph uses local file systems, the Ceph team can only wait for the developers of the file systems to adopt the new storage. In the paper, there are more details. In summary, the reasons of above problems are: File system overhead Ceph uses file system metadata to represent Ceph object metadata. (i.e., object to file, object group to diectory) The file system metadata operations are not fast and also may have some consistent issues. ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:4:1","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"BlueStore Does not use local file system anymore. Instead, store Ceph objects into raw storage directly. This method avoids the overhead of file systems. Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata. This method provides much faster metadata operations and also avoid building transtion mechanism by Ceph developer. Because RocksDB runs on top of file systems. BlueStore has a very simply file system that only works for RocksDB called BlueFS. The BlueFS stores all the contents in logging space and cleans invalid data periodly. If you understand the reason of why FileStore performs not well, you can simply understand the choices they did when build BlueStore. BlueStore still has some issues. For example, because BlueStore do not use file systems, it cannot leverage the OS page cache and need to build the cache by itself. However, build a effective cache is hard. ","date":"2021-01-17","objectID":"/aghayev-2019-filesystemunfit/:4:2","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/aghayev-2019-filesystemunfit/"},{"categories":["papers"],"content":"Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wasts the utilization of HDDs. The authors of this paper find that the write requets can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer. This strategy can reduce SSD pressure which prolongs SSD life and still provide relative good performance. ","date":"2020-12-31","objectID":"/wang-2020-bcw/:1:0","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/wang-2020-bcw/"},{"categories":["papers"],"content":"What is the problem In hybrid storage system (e.g. SSDs with HDDs), there is a unbalancing storage utilization problem. More specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer requests first, so consumer requests can have shorter lantecy. However, in a real system, SSDs handle most of the write requests and HDDs are idle in more than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation. Also, deep queue depth makes requests sufferring long latency even when we using SSDs. ","date":"2020-12-31","objectID":"/wang-2020-bcw/:2:0","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/wang-2020-bcw/"},{"categories":["papers"],"content":"Why the problem is interesting (important)? The authors find a latency pattern of requests on HDD, which is introduced by the using of in disk buffer. The request latency of HDD can be classified as three catagories: fast, middle, and slow. Write requets data is put to the buffer first, then to the disk. When the buffer is full, HDD will block the coming requests until it flushes all the data in the buffer into disk. When there are free space in the buffer, request latency is in fast or middle range, otherwise in slow range. The fast and middle latency is in $μ s$-level which similar with the performance of SSD. If we can control the buffer in disk to handle requests which their size is in the buffer size range, then we can get SSD-level performance when using HDD to handle small write requests. ","date":"2020-12-31","objectID":"/wang-2020-bcw/:3:0","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/wang-2020-bcw/"},{"categories":["papers"],"content":"The idea Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also provides resonable performance. To achieve the goal, there are two key components in this paper: Make sure requests to HDD are in the fast and middle lantency range Determing which wirte requests should be dispatch to HDD To handle the first challenge, the authors provided a prediction model. The model itself is simply comparing the current request size with pre-defined threshold. We cannot know the write buffer size of HDD directly. However, we can get an approximate value of the buffer size through profiling. The threshold are the cumulative amount of written data for the fast/mid/slow stages. Since we only want to use the fast and middle stages, we need to skip the slow stage. There are two methods to do this. First, sync system call from host can enforce the buffer flush; second, HDD controler will flush the buffer when the buffer is full. sync is a expensive operation, so the authors choose to use padding data to full fill the buffer, which can let controller to flush the data in the buffer. The second reason of why we need padding data is we want to make sure the prediction model working well. That means the prediction model needs a sequencial continuous write requests. When HDD is idle, the controller will empty the buffer even when the buffer is not full, which break the prediction. Read requests also break the prediction. Using padding data can help the system to maintain and adjust the prediction. More specifically, when HDD is idle, the system use small size padding data to avoid disk control flush the buffer; when read requests finished, since we cannot know if the disk controller flushes the buffer, the system use large size padding data to quickly full fill the buffer, which can help recorrect the prediction model. These padding data will be remove during the GC procedure. Steering requests to HDDs is much easier to understand. The latency of request is related to the I/O queue depth. We do porofiling to find the relation between SSD’s queue depth and the request request latency. In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs fast stage. We use the queue depth value as the threshold. When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD. ","date":"2020-12-31","objectID":"/wang-2020-bcw/:4:0","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/wang-2020-bcw/"},{"categories":["papers"],"content":"Drawbacks and personal questions about the study Only works for small size of write requests The consistency is not guaranteed The disk cannot be managed as RAID (can we?) GC is still a problem ","date":"2020-12-31","objectID":"/wang-2020-bcw/:5:0","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/wang-2020-bcw/"},{"categories":["papers"],"content":"Short Summary This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requestsed. They reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive. They also tried to increase the request size to exploit SSD internal parallelism. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:1:0","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"What Is the Problem Search engines pose great challenges to storage systems: low latency high data throughput high scalability The datasets become too large to fit into the RAM. Simply use RAM as a cache cannot achieve the goal. SSD and NVRAM can boost performance well. For example, flash-based SSDs provide much higher throughput and lower latency compared to HDD. However, since SSDs exhibit vastly different characteristic from HDDs, we need to evolve the software on top of the storage stack to exploit the full potential of SSDs. In this paper, the authors rebuild a search engine to better utilize SSDs to achieve the necessary performance goals with main memory that is significantly smaller than the data set. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:2:0","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"Why the Problem Is Interesting There are already many studies that work on making SSD-friendly softwares whithin storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processsing); and SFS, F2fS (file system). However, there is no optimation for full-text search engines. Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:3:0","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"The Idea The key idea is: read as needed. The reason behind of the idea is SSD can provide millisecond-level read latency, which is fast enough to avoid cache data into main memory. There are three challenges: reduce read amplification hide I/O latency issue large requests to explit SSD performance (This work is focus on the data structure, inverted index. This is a commonly used data structed in information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.) Category Techniques Reduce read applification - cross-stage data grouping - two-way cost-aware bloom filters - trade disk space for I/O Hide I/O latency adaptive prefetching Issue large requests cross-stage data grouping ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:4:0","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"Cross-stage data grouping This technique is used to reduce read amplification and issue large requests. WiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query. Inverted index of WiSER places data of different stages in the order that it will be accessed. Previous search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requets. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:4:1","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"Two-way Cost-aware Filters When doing phrase queries we need to use the position informantion to check the terms around a specific term to improve search precision. The naive appraoch is to read the positions from all the terms in the phrase then iterate the position list. To reduce the unnecessary reading, WiSER emploies a bitmap-based bloom filter. The reason to use bitmap is to reduce the size of bloom filter. There are many empty entries in the filter array, use bitmap can avoid the waste. Cost-aware means comparing the size of position list with that of the bloom filters. If the size of position list is smaller than that of bloom filters, WiSER reads the position list directly. Two-way filters shares the same idea. WiSER chooses to read the smaller bloom filter to reduce the read amplification. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:4:2","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"Adaptive Prefetching Prefetching is one of the commonly used technique to hide the I/O latency. Eventhough, the read latency of SSD is small. Compare to DRAM, the read latency of SSD still much larger. Previous search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e. Linux readahead) which increases the read amplification. WiSER defines an area called prefetch zone. A prefetch zone is further divided into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a quey are larger than a threshold. To enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calles madvise() with the MADV_SEQUENTIAL hint to readahead in the prefetch zone. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:4:3","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":["papers"],"content":"Trade Disk Space for I/O Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves. Compressing all data in the buffer together achieves better compression. However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries. In the evaluation, WiSER uses 25% more storage than Elasticsearch. The authers argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER. ","date":"2020-12-26","objectID":"/he-2020-readasneeded/:4:4","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/he-2020-readasneeded/"},{"categories":null,"content":"I rebuilt my personal blog by using hugo. I have cleaned all my old blogs (not so much actually). I will write something related to my research reading or personal learning. Hope I can meet you soon. ","date":"2020-11-08","objectID":"/my-first-post/:0:0","tags":null,"title":"My First Post","uri":"/my-first-post/"}]