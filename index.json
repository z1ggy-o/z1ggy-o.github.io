[{"content":"","date":"December 28 2025","externalUrl":null,"permalink":"/","section":"Gy's Blog","summary":"","title":"Gy's Blog","type":"page"},{"content":"","date":"December 28 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"December 28 2025","externalUrl":null,"permalink":"/weekly/","section":"Weeklies","summary":"","title":"Weeklies","type":"weekly"},{"content":"","date":"December 28 2025","externalUrl":null,"permalink":"/tags/weekly-log/","section":"Tags","summary":"","title":"Weekly Log","type":"tags"},{"content":"看了下记录，最近一次发周记正好是一年之前。其实今年自己大概也记录了 20 周左右的内容，之后又懈怠了。又因为大多是和具体工作相关，没有经过脱敏，就没有发出来。 正好又是一个年末，趁这个机会，就把这一篇当作一年回顾和明年展望吧。\n“你今天是在进步，还是又在巩固错误？”这是重庆炫阳特技东哥馆里的标语，也是我今年刷短视频学到的最大收获，是我的新座右铭。\n前两天同事问我，又是一年，去年设定的目标实现了吗？我想了一会儿，不记得当时设了啥目标了，哈哈哈。没有记录的日子确实是这样的，时间就像凭空消失了一样。 没有记录，也就无法知道自己做过哪些事，犯过哪些错。那么，我一定会重复错误，巩固错误。 工作时的周报大家都不愿意写，但回头才发现这些记录是宝藏。今年读了一本职场书里说得很好，日报，周报，月报是一定要写的。周报是短期的总结回顾，月报整理整理 就是新的简历。\n在这先记录下 2026 年的目标。简单但富有挑战，就是记录。做到每天，每周，每月都进行记录。\n为了减少行动阻力，我做了以下一些准备：\n固定工具：初步记录全部放在 logseq 的 daily log 中。后续总结的内容放到这里。 固定时间：完成的所有事项都记录；每日 18 点做当日总结；每周六做当周总结；每月底做月总结。 心态准备：只管记录，不管质量 Okay, 现在来回顾下脑子里 2025 的痕迹。\n1 月 \u0026ndash; 3 月 # 工作上，TLC 项目还在挣扎着收尾，但是我们自己的问题不多，主要是其他部件没能有效推进。后面我们自己强行介入，把一些坑给摆平了（没想到年末又有故事重现）。 因项目进展未达预期，团队组织架构进行了调整。具体细节不便叙述，但后续来看，对团队有了当时未能预料到的影响。 我负责的功能基本稳定，人力有余量，被调动支持 QLC 项目。\n生活上，过年前预期 25 年会有一些空余时间来学习沉淀，所以这几个月过得比较丰富些，读了许多专业以外的书籍，例如《要钱还是要生活》，《金钱心理学》，《猎魔人》等等。 但实际上这是今年最悠闲的时候了，因为之后 QLC 项目长期投入，一直延续到了 9 月份。\n3 月 \u0026ndash; 5 月 # QLC 项目上作为平台引导，向其他同事输出主控架构，并一起完成了写入数据分流，非对其写入，Write Uncorrectable 等功能的实现。 因为由不同的 leader 负责项目，在负责模块之外没有做很多强硬的介入。事后来看，当时几个月也还是做了很多事情，也学习了许多。后续架构也没有变化， 是值得鼓励的。\n4 月份开始对投资标的进行思考，认为大 A 很多时候都是在做板块轮动，但是大盘指数可能变动不大，那么不宜依赖大盘指数基金，而是决定自己主动进行股票交易。 在 4 月的最后一天，在四大行各买入了一万元，开始玩儿票。之后机缘巧合在小红书上知道了富贵哥，一直跟到了现在，26 年也会一起跟。\n期间的状态其实不错，一个是因为之前读《要钱还是要生活》的时候，对生活本身多了一些思考，把关注度放到了当前工作以外的地方，给了自己一些信心去梦想自由 生活是可能的。另外当时对交易也很乐观，觉得只要本金足够，盈利是可以达成需求的。\n偶然间完成了一个人生第一次\u0026ndash;一个人单独出游。跑到浙江小城嵊州玩儿了一圈。\n6 月 \u0026ndash; 9 月 # 其实从 4 月向后工作节奏就打得比较满，有几次晚上一个人干到快 12 点才回家。去年高强度了一年，今年继续上强度，加上其他同事反而闲了下来，心情不是很好。\n这段时间读了很多交易相关的书籍，比如《价格行为》，《笑傲牛熊》等等。 投资开始上仓位，事后来看，当时抓住了一些机会，比如平安，但是仓位分配有问题，导致整年的收益没有达到应有的高度。不过还是新手阶段，加上是牛市，总是不亏的。\n这几个月过得突然变得迷茫，不像前几个月那么有干劲。想来一个是休息得时间不多，再一个是投资资金的压力很大，想要尽可能地减少开支，把多的钱都放到投资市场上。\n10 月 \u0026ndash; 12 月 # QLC 项目逃出来一段时间，转到验证下一代芯片上。 除此之外，因为对公司未来的不解，陆续有同事离职，我也尝试了面试。但是因为准备得太过随意，导致浪费了一个绝佳的目标公司的机会，搞得人有些消极。 不过也借由这次机会，做了一次回顾，反而让自己从前段时间的疲劳感中解放了出来。\n这几个月的大量人员流动，让大家对工作都没法上心，接下来不知道还会继续有怎样的变动，一切还没尘埃落定。\n投资上，9 月迎来了一个小回调，我因为害怕错过机会，没有按照纪律操作，而是追了风头正盛的有色金属和存储。而且盲目听从“消息”，没有自己动脑，最终亏损三万多， 交了学费离场。\n这一年学到了啥？ # 要做积极主义者。消极主义者看似犀利，但那是因为搞砸一件事情很简单。积极主义，就是每天坚持做对的事，用日积月累的付出所创造的复利来最终形成好的结果 化繁为简。如无必要，勿增实体。复杂的设计，规则，和假设会阻拦我们产生真正的行动。但只有行动才会带来变化。 没有限制。今年最难受的时候，觉得人生已经被安排了，连基本的日常被没有自主权。但后来才明白，我们是完全自由的，没有人强制我们要在什么时间做什么事情。 我们感到的限制，来自于我们自己。是我们放弃选择，把自己交了出去。我们可以，也必须自己来做决定。 ","date":"December 28 2025","externalUrl":null,"permalink":"/weekly/2025/25-12-28/","section":"Weeklies","summary":"","title":"Weekly: 2025-12-28","type":"weekly"},{"content":"新年后的第一周，计划重新养成早起的习惯，多腾挪一些时间出来给最重要的事情。但是天气严寒，温暖的被窝总是太诱人 ;) 下周再努力吧。\nThe digital problems do not always have digital solutions \u0026ndash; Cal Newport\n这是我最近感触最深的事实。我们花费了太多时间在数字化的世界中，但有些时候更先进的数字化解决方案并不是最有效，最让人感到幸福的。不停地刷手机最后只得到空虚；各种所谓的效率软件层出不穷，但最后帮助我最大的还是最朴素的纸笔。\n学到啥 # 时间管理相关\n如何超过99%的人: 时间管理的奥秘 \u0026ndash; MoneyXYZ 时间管理的本质，是将时间花在真正重要的事情上面。如果你想获得诺贝尔奖，那么你就得花时间研究值得获得诺贝尔奖的课题上。不重要的事情上面，不需要浪费时间。 这个观点也和《最重要的事只有一件》这本书一致 坚持 5 点起床第二年，我都干了点儿啥？ \u0026ndash; MoneyXYZ 一份可行的早起执行计划 理财相关\n实现财富自由一定要零负债吗？ \u0026ndash; MoneyXYZ MoneyXYZ 提供了一个简单的公式来检验我们的负债情况，以帮助我们进行财务方面的决策：$$(收入 - 目标储蓄额 - 非债务性支出) / 负债支出 $$ 通过该公式我们可以简单快速地审查当前的负债状况和自己目前的财务安全边际。我通过该公式的计算，很快就打消了购车的念头。因为购车这类消费本身会带来负债的同时，还会带来附加消费，使得我们的支出大幅增加，但是又没有提升我们的收入 良性的负债是我们可以承担的负债，且最好能够提振我们的收入 躺平是通往财富自由的第一步 \u0026ndash; MoneyXYZ 人们感到厌倦，想要躺平，是因为现代社会系统已经从教育，就业，消费，个人财务，养老五个方面牢牢将我们困住，让人们成为了薪资的奴隶。 而走向自由的方式，就是逐渐从这五个方面与系统脱钩。最容易的是消费，因为消费是我们直接可控的。消费脱钩之后，才可能从个人财务上脱钩，即把消费剩下的钱积累为自己的资本。然后是职业上的脱钩，退休的脱钩，最终让自己的子女可以从教育上脱钩，实现自由。 推荐读物：Early Retirement Extreme 财经知识\n了解国债 \u0026ndash; 小 Lin 说 央行和国债的关系，是通过有稳定的借债方给政府提供资金，继而进行经济调控。没有央行介入的情况下，国债也可以发行。但现今世界，主流国家都在大力发债，没有央行的帮助，债务调控会很困难 正好最近的新闻是停止了对国债的购买。通过这个视频，了解了，其目的大概也是为了控制国债的利息。因为中央大幅借债才刚刚开始，现在如果长期国债的利率就拔高，之后借债成本就会变高 Vim User Manual: 3. Moving around\n从来没有真正读过 vim 的文档，最近计划先把 user manual 给读一遍。 Vim 的移动从方式来说其实大致就是 5 种方式，上下左右这种去除，就剩下 4 种： Word movement。w, b 之类命令的移动，每次按照一个词跳转，如何判定什么是一个 word 由 iskeyword option 来指定 Pair match。% 命令，哪些内容是一对 pair 是由 matchpairs option 指定 Search。Search 其实是最常用的移动方式，且有通用性。:set ignorecase 可以忽略大小写 Marks。Jump list 记录其实也是通过 marks 来工作的，它将位置信息记录在寄存器中，用于在之后跳转使用。有些特殊的寄存会在一些操作的时候自动将对应的位置记录下来。比如 jump 的时候，就会将跳转前的位置自动记录 干了啥 # 买了一个软木的托盘。之前有博主推荐过，自己入手一看，确实很神奇。还是同样的桌子，不知道为什么，有了托盘之后，东西放进去就感觉规整漂亮了许多 给小米 Sound Pro 买了个支架。PDD 上瞎逛看到的，之前有共振只是简单垫了下纸板。有个专门的小架子看起来还是漂亮得多 逛小红书买了几瓶织物喷雾。说是有除螨的功效，我主要是想用它来除臭。不知道是人老了还是不爱干净了，早上睡起来自己都能闻到屋子里一股怪味儿。臭男人不是浪得虚名 《猎魔人》读了一半了。多亏这本小说，都快给我养成睡前读书的习惯了。微信读书连续两周满勤 工作呢 # 本来手头的工作已经进入尾声，可以阶段性休息了。但是又出现了几个恼人的问题。羡慕纯软件的工作，自己对实现的掌控性要高得多 Asynchronous Multi-Plane Independent (AMPI) 有些也称为 Asynchronous Independent Plane Read (AIPR)，是实现对同一个 Die 内不同 plane 同时访问的指令。与之对应的是 SMPI，同步多 plane read。AMPI 的好处就是更加灵活，可以在前一个 read 请求工作的过程中发送新的请求 闲来无事，就学习了一下。这是 YMTC 的一份公开专利，实现原理也并不复杂。还是通过添加每个 plane 单独的处理 unit 来达成独立访问的目标 ","date":"January 12 2025","externalUrl":null,"permalink":"/weekly/2025/25-01-12/","section":"Weeklies","summary":"","title":"Weekly Digest: 2025-01-12","type":"weekly"},{"content":"","date":"January 2 2025","externalUrl":null,"permalink":"/books/","section":"Books","summary":"","title":"Books","type":"books"},{"content":"","date":"January 2 2025","externalUrl":null,"permalink":"/tags/books/","section":"Tags","summary":"","title":"Books","type":"tags"},{"content":"","date":"January 2 2025","externalUrl":null,"permalink":"/tags/self-development/","section":"Tags","summary":"","title":"Self-Development","type":"tags"},{"content":"Title: 最重要的事: 只有一件\nAuthor: Gary Keller, Jay Papasan\n适合对当前的状态感到迷茫时读的一本书。整本书的内容和其标题一致，围绕 “一个目标 + 一件优先事务 + 生产力” 展开：\n“目标” 指的是人生的大目标。知道方向，才知道向哪边努力 “优先事务” 是每天首先需要保证完成的任务，我们通过完成一件件的优先事务慢慢走向目标 “生产力”体现了我们完成优先事务的能力，一个高效的系统帮助我们完成更多的优先事务，更早的达成目标 What I Learned # 成功并不因忙碌而靠近我们。我们需要一份“成功清单”，上面所有的内容都围绕着我们的终极目标。全身心投入到我们的事业中，从而使我们的生活更有意义，是获取持久幸福最可靠的方法。当我们每天的所作所为逐渐成就了一个目标时，巨大的幸福感也就随之而来了，并且久久不会消散 你本人就是你所做的事情的集合，如果你不断重复正确的行为，那么成功就不再只是一个动作，而是一个你亲手打造的习惯。但无须方方面面都自律，只需要挑最有用的那件事，坚持下去 我们不应该追求平衡是因为奇迹不会在中间点上发生，奇迹只有在追求极致的过程中才会发生。在最重要的事情没有做好之前，你总会觉得事情还没有结束——这就是失衡感。在生活中，你应随时保持短线调整，避免长时间处于失衡状态。在事业中，你需要长线调整。在生活中，所有的要素一个都不能少，都需要顾及；而对于工作来说，取舍是常态 尽可能地为自己设立远大的理想，为实现更高的目标而规划做什么、怎么做以及和谁做。若要突破这种思维的巨型“盒子”​，你可能要花费毕生精力才能梦想成真。你不要畏惧志存高远，而是要恐惧平庸、恐惧浪费、恐惧没有全力以赴地活过 和伟大的目标一样，好问题也要大而具体。当你提出一个好问题时，你其实是在追求一个伟大的目标。每当你这样做的时候，其实都是在往大而具体的方向走。大而具体的问题会带着你找到大而具体的答案，而这种答案对于宏大目标的实现非常有必要。 提出好的问题是获得好的答案的最佳途径。关键问题将所有问题都归纳为一个问题：​“我能做的最重要的事是什么？为何做了这件事就会让其他事都变得更简单或是不必要了呢？​” 你首先需要考虑长期目标，然后一步步往回考虑，倒推出现在应该做的最重要的一件事。这有点儿像俄罗斯套娃，此刻的最重要的一件事就藏在今天的最重要的一件事之中，今天的最重要的一件事就藏在这周的最重要的一件事之中，这周的最重要的一件事就藏在本月的最重要的一件事之中……一件小事就这样一步步变大，实际上你是在摆一副多米诺骨牌 想要完成优先事务，就必须为其预留时间。通过预留时间的办法达到理想效果需要三个承诺。第一，必须达到精通的程度。精通就是做最好的自己。如果希望获得最佳的效果，就要明白你需要最大努力。第二，要不断寻找最佳的解决方案。令人最沮丧的事情莫过于自己尽最大努力却无法达到最好的结果。第三，主动反思自己为完成最重要的一件事所做的一切。将这三条铭记于心，通过奋斗以达到卓越 What I Can Do # 寻找属于自己的那个“大而具体”的目标。根据这个目标反推 5 年计划，3 年计划，1 年计划。每周回顾这个目标并安排本周的优先事务 面对任何问题的问题，都问问自己，对此我能做的最重要的是什么？做了这件事后，哪些剩下的事情就变得简单或不必要了 每天都预留出完成优先事务的时间。在完成优先事务之前，其他的事情都要让步 每年都先为生活在时间表上留出时间，然后再围绕这些时间安排工作。生活的要素一个都不能少，工作的内容则要精心取舍 ","date":"January 2 2025","externalUrl":null,"permalink":"/books/%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E5%8F%AA%E6%9C%89%E4%B8%80%E4%BB%B6-gary_keller/","section":"Books","summary":"","title":"最重要的事只有一件","type":"books"},{"content":"标题: 30岁人生开挂7步法: 没资源、没背景的90后宅男每月获利50万元的简单方法\n作者: 自青（韩）\n这是一本韩国的畅销书。畅销书本来一般都是比较套路的写法，浓缩了其他一些经典书籍的名言。这本书也不列外，但我还是把它读完并且记录下来有两个原因。一个是最近生活不在状态，需要一些此类书籍来打打鸡血；二是本书的作者很诚恳，不空洞。他用自己的故事作为激励，从心理和行动上给出了几个立即可行的方案，鼓励大家都立刻动起来。\nWhat I learned # 自我意识是指我们对自己身体，情感，行为以于外界关系的理解。自我意识是高阶的思维，帮助我们维持自己存在的合理性。但过剩的自我意识就会导致自负或者自卑，反而阻拦我们获得自由。进化的目的不是完美，而是生存。我们的进化就像是 kluge 机器，是在旧的系统上打补丁，骨子里的很多本能反应在当今社会都并不能带来最优的结果，比如我们的恐惧和忧虑心理。要通过观察和反思来避免这些本能反应所带来的不良结果。 塑造身份认同。换句话来说就是自信，相信我们能够达成我们想要达成的任何目标。实际行动才能带来实质改变。身份认同是一种燃料，让我们敢想敢做。构建身份认同最好的方式就是利用环境，因为输入影响输入，当我们把自己丢到适当的群体中时，我们的群体潜意识会自然而然地引导我们进行改变 训练大脑带来的效益是有复利效应的，因为大脑能力的提升是基本能力的提升，一个领域中的知识有可能迁移到另外的领域，帮助其他领域发展得更快更好。坚持每天两个小时的读写就能见到意想不到的成长。 成功者的工具：肯付出的人才能更到更多的回报，因为他们重视价值。理性决策，只押注概率。不要追求完美，而是应该利用28原则，抓住更多方面的 80%，并利用这些多方面的组合来获得更大的回报。提升元认知能力，避免主观判断。执行，执行，执行，还是执行！绝大多数人都是只想不做。 What I Can Do # 训练自己在对外反应之前停顿一下，确认自己的回应来自理性的思考而不是自我意识自然的反应。《清晰思考》一书里提过的一个方法是，养成在回应前深吸一口气的习惯。 通过加入圈子的方式来帮助自己塑造身份认同，增强我们的执行力。如果没法找到圈子，就找 20 本对应领域的书籍来阅读。 决策的时候只押注概率，不要纠结于独立事件的结果。只要坚持押注概率，最终赢的就会更多。 不要追求完美，要先动起来。不论想到达成何种目标，关键都在于做。光想不做，一切都是徒劳。 ","date":"December 28 2024","externalUrl":null,"permalink":"/books/30%E5%B2%81%E4%BA%BA%E7%94%9F%E5%BC%80%E6%8C%827%E6%AD%A5%E6%B3%95-%E8%87%AA%E9%9D%92/","section":"Books","summary":"","title":"30岁人生开挂7步法","type":"books"},{"content":" 学到啥 # Neovim TJ Advent 系列 LSP in Neovim help: vim.lsp.* 查看 neovim 内建 LSP client 所提供的 API nvim-lspconfig package 为各类语言的 LSP client 配置提供了基本的支持 :help lspconfig-all 可以查看想要对应语言的 server 安装方法和配置方法 一般现在都会搭配 mason.nvim package 来自动下载，但是了解最基本的方法更好一些 LSP Format 就是由 vim.lsp.buf.format() 来对当前 buffer 实现的 format 因为是 LSP 提供的功能，有可能有一些 server 并不支持 TJ 介绍了一个 autocmd 的在 server attach 的时候确认是否支持 format，并动态的绑定键位 editorconfig 是一个更加通用的格式定义工具，比起直接对 LSP 进行配置，更推荐这样的方式，因为很多我们知道的 editor，比如 neovim 就支持对它的解析。这样，书写一次规则，就可以在各个 editor 中都使用了 Telescope telescope 这个 fuzzy finder 的核心就是 finder, sorter, previewer 三个部件。Finder 搜索内容，sorter 对搜索到的内容进行排序，previewer 展示内容，三者组合起来就成了一个可用的 picker 了。 Advanced Telescope 里就展示了如何实现一个自定义的 picker 来满足自己的搜索需求 干了啥 # 这周重新把 blog 组织起来。这次的目标是使用简单的工具，把时间专注到内容而不是格式上去。本篇就是这唯一更新的内容 :) 脚可能是上周末团建的时候扭伤了，导致只跑了一次步。估计下周也没法跑了 才刚刚为冬天跑步买了装备呢 :( 房子重新续约。不让中间商赚差价，每个月省了几百块的租金😁 工作呢 # 项目进入阶段尾声，没有什么可以公开分享的。学习了一些内部代码 ","date":"December 28 2024","externalUrl":null,"permalink":"/weekly/2024/24-12-28/","section":"Weeklies","summary":"","title":"Weekly Digest: 2024-12-28","type":"weekly"},{"content":"","date":"September 17 2023","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Short Summary # HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.\nThe authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer.\nThis strategy can reduce SSD pressure which prolongs SSD life and still provide relative good performance.\nWhat is the problem # In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.\nMore specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer requests first, so consumer requests can have shorter latency.\nHowever, in a real system, SSDs handle most of the write requests and HDDs are idle in more than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation. Also, deep queue depth makes requests suffering long latency even when we using SSDs.\nWhy the problem is interesting (important)? # The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer. The request latency of HDD can be classified as three categories: fast, middle, and slow. Write requests data is put to the buffer first, then to the disk. When the buffer is full, HDD will block the coming requests until it flushes all the data in the buffer into disk. When there are free space in the buffer, request latency is in fast or middle range, otherwise in slow range.\nThe fast and middle latency is in $μ s$-level which similar with the performance of SSD. If we can control the buffer in disk to handle requests which their size is in the buffer size range, then we can get SSD-level performance when using HDD to handle small write requests.\nThe idea # Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also provides reasonable performance.\nTo achieve the goal, there are two key components in this paper:\nMake sure requests to HDD are in the fast and middle latency range Determining which write requests should be dispatch to HDD To handle the first challenge, the authors provided a prediction model. The model itself is simply comparing the current request size with pre-defined threshold. We cannot know the write buffer size of HDD directly. However, we can get an approximate value of the buffer size through profiling. The threshold are the cumulative amount of written data for the fast/mid/slow stages.\nSince we only want to use the fast and middle stages, we need to skip the slow stage. There are two methods to do this. First, sync system call from host can enforce the buffer flush; second, HDD controller will flush the buffer when the buffer is full. sync is a expensive operation, so the authors choose to use padding data to full fill the buffer, which can let controller to flush the data in the buffer.\nThe second reason of why we need padding data is we want to make sure the prediction model working well. That means the prediction model needs a sequential continuous write requests. When HDD is idle, the controller will empty the buffer even when the buffer is not full, which break the prediction. Read requests also break the prediction. Using padding data can help the system to maintain and adjust the prediction. More specifically, when HDD is idle, the system use small size padding data to avoid disk control flush the buffer; when read requests finished, since we cannot know if the disk controller flushes the buffer, the system use large size padding data to quickly full fill the buffer, which can help recorrect the prediction model. These padding data will be remove during the GC procedure.\nSteering requests to HDDs is much easier to understand. The latency of request is related to the I/O queue depth. We do profiling to find the relation between SSD\u0026rsquo;s queue depth and the request request latency. In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs fast stage. We use the queue depth value as the threshold. When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.\nDrawbacks and personal questions about the study # Only works for small size of write requests The consistency is not guaranteed The disk cannot be managed as RAID (can we?) GC is still a problem ","date":"September 17 2023","externalUrl":null,"permalink":"/papers/wang-2020-bcw/","section":"Papers","summary":"","title":"FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","type":"papers"},{"content":"","date":"September 17 2023","externalUrl":null,"permalink":"/categories/notes/","section":"Categories","summary":"","title":"Notes","type":"categories"},{"content":"","date":"September 17 2023","externalUrl":null,"permalink":"/categories/papernotes/","section":"Categories","summary":"","title":"PaperNotes","type":"categories"},{"content":"","date":"September 17 2023","externalUrl":null,"permalink":"/papers/","section":"Papers","summary":"","title":"Papers","type":"papers"},{"content":"","date":"August 22 2022","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"August 22 2022","externalUrl":null,"permalink":"/categories/articles/","section":"Categories","summary":"","title":"Articles","type":"categories"},{"content":"","date":"August 22 2022","externalUrl":null,"permalink":"/tags/dbms/","section":"Tags","summary":"","title":"DBMS","type":"tags"},{"content":"最近一周花了些时间阅读了 LevelDB 的代码，了解一下 LSM-Tree 的具体实现。在阅读的过程中简略的记录了一些笔记，放在了我的 Github repo 里。笔记是用蹩脚英语写的，也没有做任何的插图，建议配合此分析文档一起食用。\nLSM-Tree 虽然很早被提出，但是在 LevelDB 这个开源实现被公开之后才引发了比较广泛的研究。毕竟，拿来的是最舒服的，虽然概念简单，但是细节很多。有些时候代码比语言更清晰，所以，我的笔记里主要还是将概念和具体的代码做了关联，细节还是要到代码里去扣。\n在这里，主要想要用简单的语言介绍一下 LevelDB 的主要框架。方便想要花五分钟大致了解一下 LevelDB 的朋友。\nLevelDB，一个 LSM-Tree 的开源实现 # LSM-Tree 的特点在于其对存储内容的排序和 append-only 的写入模式。此工作模式主要有两个优点：\n极佳的写入性能。不需要做 read-modify-write。 便利的并行控制。写入的内容不会再被修改，可以放心的做并行读取。 但是物理世界里，一切都是有利弊的。LSM-Tree 的读取性能并不优秀。因为有过期数据的存在，读取过程中可能会需要读取多个文件，会有 read-amplification 的问题。过期数据也会浪费存储空间。\n插一句，对 write-amplification (WA) 问题，LSM-Tree 和 B-Tree 两者都有，但是具体的情景是不一样的。LSM-Tree 的 WA 来源于 compaction 过程，需要将已有的数据重新写入。而 B-Tree 的 WA 来源于其固定 page 大小的写入单位。就算一个 page 内只有一个 byte 被修改了，整个 page 也需要写回。\n接下来主要针对 LevelDB 的写入和读取两个部分的设计部件作一下简介。\n写入：Memtable 和 SSTable # LevelDB 只允许写入请求（即插入，更新，删除）发生在内存内的 memtable 中。Memtable 是实现于 skiplist 数据结构，在提供良好插入性能的情况下也保持 item 有序。\n当一个 memtable 达到预定的大小之后，memtable 会被写入到磁盘中，成为一个 SSTable 文件。因为在 memtable 写出的时候就不能再接收写请求了，LevelDB 使用了 double-buffer 的策略，另起一个 memtable 接收写请求。\n为了避免故障带来的数据损失，LevelDB 也使用了 WAL 的方式来记录请求内容。不过因为 LevelDB 只是一个简单的 KV 引擎，log 几乎就是请求内容的复制记录。而且，在 memtable 被持久化之后，对应的 log 文件就可以抛弃了。\n总的来说，log-structuring 本身已经为写入带来了极大的便利，所以这个部分的特别设计并不多。\n读取：SSTable 结构和 MVCC # 因为 LSM-Tree 的读取性能并不好，LevelDB 为了提升读取性能还是花了一番力气的。\nSSTable 结构 # Memtable 只是一个 skiplist，其中一个 node 代表一个写入请求指令的内容。如何将这些内容进行安排，决定了读取过程的效率。LevelDB 主要做了一下几个设计：\nSSTable 文件按新旧序号排列。更新的文件中自然存放着更新的数据。 SSTable 文件的底部设计了一个 footer 部分，可以帮助快速识别目标 key 所在的位置，减少读取量。 SSTable 内部嵌入 bloom filter，进一步减少不必要的读取。 更加具体的内容请参照笔记和源码。\nMVCC # LevelDB 是支持针对数据进行 snapshot 读取的。它的实现方式倒也简单直接。在 DB 的每一条记录中，都包含一个单调递增的 sequence number。当要进行 snapshot 读取的时候，LevelDB 会告诉读请求当时的最新 sequence number。如此一来通过比较，就可以避免读到更新的数值了。这个设计的好处在于，读不会阻塞写。\nLevelDB 抽象了一个 Version 对象，记录了一个版本的文件。每当有 sstable 生成或删除的时候，就形成一个新的版本。如果有请求在使用某个版本的文件，那么即使这个文件已经过期，也会等到所有的使用都完毕之后才会被清理。\nCache # 除此之外，LevelDB 也实现了一个比较标准的 LRUCache。该 cache 主要缓存打开的文件句柄，以及读取的数据块。\n用户也可以选择不使用这个 cache。比如，如果做大范围的 range read 的话，用 cache 的意义不大。\n清理，Compaction # 因为 append-only 的写入方式会让 DB 中存有过期的数据，我们需要定期的对数据进行清理，释放不需要的存储空间。\nLevelDB 使用 leveling compaction 的方式来做这个清理。具体来说，sstable 文件被按层级安排在了一起，低层的数据存放量少，数据更新；高层的数据存放量更大，但是数据旧。\ncompaction 是在两个相邻层级中的文件中进行的。先是在上层中找到一个对象文件，然后根据其 key 的范围，在下一层中找到有 key 重叠的文件。最后，因为所有的记录都是排好序的，利用 k-way merge 的方式就可以将过期的数据筛除。有效的数据被写入到新的 sstable 文件中，放入到下层里。这个过程可能在各个层级递归进行。\n寻找 compaction 对象文件的过程略微复杂，有兴趣可以参照笔记和源码学习。\n因为 compaction 是一个比较昂贵的操作，LevelDB 定义了三个条件来决定是否需要做 compaction：\nLevel 0 （最低层）的文件数量超过了限制； 其他层的数据量超过了限制； 一个文件的无效读取次数超过了限制。 结语 # LevelDB 的代码量并不算很大，但是确实设计的很好。早就听说这是一个学习 C++ 的好材料，如今一读，确实如此。特别是对于我这种常年打 C 补丁的选手，LevelDB 实打实的给上了一节 OOP 的实践课。也推荐给所有想了解 LSM-Tree，学习 C++ OOP 范式的同学阅读。\n","date":"August 22 2022","externalUrl":null,"permalink":"/posts/leveldb-analysis/","section":"","summary":"","title":"LevelDB 源码分析笔记","type":"posts"},{"content":"","date":"August 22 2022","externalUrl":null,"permalink":"/tags/lsm-tree/","section":"Tags","summary":"","title":"LSM-Tree","type":"tags"},{"content":"","date":"August 7 2022","externalUrl":null,"permalink":"/tags/cloud-computing/","section":"Tags","summary":"","title":"Cloud-Computing","type":"tags"},{"content":" Motivation # Cloud computing system is high virtualised for higher resource utilization. Recently, cloud providers have developed harvesting techniques to allow evictable virtual machines to use unallocated resources.\nPeople mostly focus on how to use unallocated computing resources and memory resources, however, there is no harvesting techniques that built for storage resources.\nThis paper proposes a harvesting framework for storage resources that provides isolation of space, bandwidth, and data security.\nContribution # They conduct a characterization study of the storage efficiency in different cloud platforms through trace logs provided by these platforms.\nThey come up a new abstraction of storage virtualization for software-defined SSDs. And base on that, they build a learning-based storage harvesting framework, BlockFlex.\nSolution # First, they analyze the tracks provided by Alibaba and Google and find out that there are a lot of unused storage resources (both in capacity and bandwidth) in allocated and unallocated VMs. That’s the resource that we can use for evictable VMs temporarily.\nA SSD consists of multiple channel internally. Each channel has its own storage chips and can operate independently. In the context of SSD virtualization, software-defined SSD (SDF) allows the upper-level VM to map its virtual SSD (vSSD) to a set of flash channels. That’s how we control capacity and bandwidth allocation.\nBlockFlex proposes a new abstraction, ghost vSSD (gSSD), on top of SDF, attached to existed vSSD. A gSSD is represented by a data structure that records its bandwidth, capacity, expire time etc. gSSDs are organized by a set of linked list, each linked list contains gSSDs with bandwidth and capacity in a specific range. Because the allocation/deallocation does not happen frequently, this organization is enough for finding appropriate gSSDs efficiently.\nCreate gSSDs from unsold (unallocated) VMs\nBecause the storage usage of unsold VMs is quit stable. BlockFlex tags each unsold VM with a heuristic-based predicated duration time using the histogram of previous available times for the unsold VM with the same storage capacity. Create gSSDs from sold VMs\nSince sold VMs are currently using by clients, the heuristic-based prediction does not work well.\nBlockFlex allocates read, write, and erase operations at the block layer of each vSSD for online predictions. They choose LSTM as the models because they can do time-series predictions. The operation tracks are used to infer the bandwidth, throughput (IOPS), and current storage utilization, then these info is used as input for LSTMs.\nThere are bandwidth predictor, capacity predictor, and duration predictors. Both bandwidth predictor and capacity predictor use the same LSTM model, but with different learning rate and hidden layer size. The predicted bandwidth and capacity are then used as the input for the duration predictors.\nReclaim gSSDs\nUpon the gSSD reclamation, the corresponding entries in the address mapping table of the vSSD will be removed.\nIf a gSSD will expire soon, BlockFlex will erase the flash blocks for data safety, and remove the gSSD instance.\nEvaluation # The workloads they used are TeraSort, ML Prep, PageRank, and YCSB. The capacity is ranged from around 60GB to 220GB (not much) The main takeaway # It seems people trying to give more control of storage devices to the host side instead of using them as a black box. ","date":"August 7 2022","externalUrl":null,"permalink":"/papers/reidys-2022-blockflexenablingstorage/","section":"Papers","summary":"","title":"OSDI'22 - BlockFlex: Enabling Storage Harvesting with Software-Defined Flash in Modern Cloud Platforms","type":"papers"},{"content":"","date":"August 7 2022","externalUrl":null,"permalink":"/categories/paper-notes/","section":"Categories","summary":"","title":"Paper Notes","type":"categories"},{"content":"","date":"August 7 2022","externalUrl":null,"permalink":"/tags/storage-systems/","section":"Tags","summary":"","title":"Storage-Systems","type":"tags"},{"content":"最近在准备面试的过程中看到有这样一个问题，就是让比较一下 2PC 和 3PC。在网上找了一些文章来读，感觉都没有十分简洁地说明两者之间最基本的区别点。所以，在这里写一篇小文，表达一下自己对 2PC，3PC 最核心区别的理解。\n最重要的最先说，在我看来两者的核心区别在于：参与者间是否对 transaction commit/abort 建立了共识。3PC 的参与者之间对 commit 的成立是具有共识的，2PC 则没有。\n3PC 相比 2PC 带来了什么？ # 大家都知道，3PC 相比于 2PC 来说多了两个东西：\n增加了 time out commit phase 被分割为了 prepare commit 和 do commit 两个部分 增加一个 prepare commit phase 带来了什么呢？是集群对 commit 这一决定的共识。\n在 2PC 协议中，协调者单方面向参与者发送一次 commit 消息。这个消息有两个含义：\n让参与者进行 commit 所有参与者都认可此 commit 但需要注意的是，对于一个 transaction，是 commit 还是 abort 这个决定本身只有协调者知道。一旦协调者故障，这部分信息就消失了。所以我们说 2PC 的协调者是单点故障点。\n3PC 协议中，prepare commit 消息让一个 transaction 该 commit 还是 abort 这个决定本身被传导到了所有参与者处。如此一来，如果协调者故障，参与者们可以根据 prepare commit 的情况继续工作。\n举例来说，time out 之后，我们从参与者中选出一个新的协调者。该协调者向其他参与者询问对于当前未完成 transaction 的信息。如果所有参与者都表示已经收到了 prepare commit，则证明我们可以对此 transaction 进行 commit；如果有部分参与者没有收到 prepare commit，我们可以选择 abort 或者再次尝试 commit 流程；如果谁都没有收到，则 rollback。\n2PC 的参与者为什么不能接替工作？ # 读完上面的叙述中，大家可能会有疑问。明明 2PC 中的 commit 信息已经携带了所有参与者都同意 commit 的隐藏信息，为什么不能像 3PC 一样让参与者成为新的协调者继续工作呢？为了解决这个疑问，我们需要考虑一个极端情况：协调者在发送部分 commit 信息后故障，收到 commit 信息的参与者在 commit 后也发生故障。\n在这个情况下，如果剩余的参与者之一成为新的协调者，因为所有参与者都没有接收到 commit 消息，我们只能认定这个 transaction 需要被 abort。要是在此之后，之前已经 commit 的故障参与者恢复了，灾难就到来了 \u0026ndash; 集群内各个服务器的数据不一致。\n基于以上原因，2PC 在协调者发生故障后，只能阻塞等待。因为参与者自己不能判断能否对进行着的 transaction 进行 commit 或 abort。\n3PC 则没有这个问题。同样的情况，因为只有在确认所有的参与者都收到 prepare commit 之后才会实际进行 commit。即，在可能出现 abort 决定情况下，系统中谁都还没有对此 transaction 进行 commit。所以，当故障服务器恢复后，不会有 2PC 例子中的数据不一致问题。\n3PC 的问题 # 敏锐的人可能发现了，上面对 3PC 参与者可以安全接替工作的表述并不是在所有的情况下都成立的。这是因为 3PC 是基于 reliable network 环境设计的。\n具体来说，如果发生了 network partition，又恰好把接收到和未接收到 prepare commit 消息的服务器们分别划到了不同的 partition 中。在 time out 后，一边会选择 commit，而另一边会选择 abort。一旦 network 重新恢复，我们也同样面对数据不一致问题。\n因为在实际的应用中，我们的网络都是不稳定的，加之 3PC 增加了一轮通信，所以很少在工程中使用。\n","date":"July 31 2022","externalUrl":null,"permalink":"/posts/2pc-vs-3pc/","section":"","summary":"","title":"2PC v.s. 3PC: 一句话的总结","type":"posts"},{"content":"","date":"July 31 2022","externalUrl":null,"permalink":"/tags/distributed-systems/","section":"Tags","summary":"","title":"Distributed-Systems","type":"tags"},{"content":" Motivation # Some applications need relation data model and strong consistency which BigTable cannot gives.\nSo, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.\nContribution # Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.\nProvide SQL-like interface, strong consistency, and high read performance.\nSolution # Replication\nUse Paxos to replicate data to several nodes, which provides higher availability. Local Transactions (within a paxos group)\nEach Paxos group leader has a lock table to implement concurrency control.\nSpanner chooses to use 2PL as the protocol because it is designed for long-lived txns, which performs poor under optimistic protocols.\nDistributed Transactions (across paxos groups)\nEach Paxos group leader has a transaction manager to support distributed txns.\nThe transaction manager is used as a participant leader.\nSpanner uses 2PC to support distributed txns. Paxos group leaders become the coordinator and participants.\nSnapshot Isolation\nFor better read performance, Spanner uses MVCC to enable read without locking TrueTime\nMVCC needs timestamp. Since Spanner is a globally distributed system, we cannot use logical timestamp (otherwise, the centric timestamp manager can be the bottleneck), we also cannot use physical time (not accurate).\nTo resolve this problem, Spanner introduces TrueTime, which is a time range based on physical time instead of a single time point.\nThis time range can cover the error of physical clocks from different datacenters. We use GPS and atomic clocks to sync timestamp on different datacenters. So we can ensure the txn order without a centric timestamp manager.\nEvaluation # Micro benchmark that shows the performance of replication, transactions and availability on a setup that the network distance between each other is less than 1ms. (A common layout, most apps do not need to distribute all data worldwide)\nAnalysis of a real world workload, F1.\nThe main takeaways # A real cool system that shows how to combine techniques from different fields together. ","date":"July 31 2022","externalUrl":null,"permalink":"/papers/2.c-2013-spanner/","section":"Papers","summary":"","title":"TOCS'13 - Spanner: Google’s Globally Distributed Database","type":"papers"},{"content":"","date":"July 10 2022","externalUrl":null,"permalink":"/tags/computer-organization/","section":"Tags","summary":"","title":"Computer Organization","type":"tags"},{"content":" 到了该系列笔记的最后一个章节了。之前的几个章节中，我们讨论了最直白的 8086 分段访问模型，又讲到了保护模式下的分段访问模式，现在我们来讲一讲实际生活中的默认内存管理方式\u0026ndash;分页。\n本文中我们只谈寻址部分的内容，关于分页所提供的保护机能不作涉及。\n其余章节如下：\n第一部分: 计算机基础和实模式 第二部分：保护模式下的分段寻址和权限 第三部分：多任务支持 第四部分：分页机制 为什么分页？ # 聊到分页，我们经常将其和虚拟内存联系在一起。而虚拟内存又是多任务的好帮手。但是虚拟内存的实现并不需要分页。分页是为了更高效的进行内存管理。\n在多任务的系统中，我们之前已经通过 LDT 隔离了各个任务的内存空间。而段描述符中的 P 位标志着该段是否存在于内存中，使得我们可以利用它将一个段交换到二级存储器中，为内存腾出空间。这似乎和广泛了解的分页虚拟内存没什么区别，但实际上有两个显著的问题：\n段是不定长的。操作系统想要在内存里找一个合适的位置不容易。会出现“外部碎片化”的问题，即，有些内存空闲空间因为太小，不能被使用而浪费。 段只能以整体进行交换。因为只有段描述符有一个 P 位，我们只能选择将整个段换出，或导入。如果想要载入一个大段，可能要换出多个现有的段。 综上两个原因，基于段的内存管理模式较为复杂，效率也较低。还有，一个段的大小不能大于物理内存的空间，否则无法被载入。\n分页机制则是以固定的大小对内存进行分割，每个单位就是所谓的“页”。对于每个页，我们都有相应的数据结构对其进行描述和管理。\n对固定大小的页进行管理有两个好处：\n我们的管理模式变得更加简单。定长总是比不定长好处理。 因为一般页的单位较小（例如 4KB）我们能够对内存进行更细颗粒度的管理，也突破了段大于物理内存则无法被载入运行的限制。 使用分页机制也会有碎片化的问题，相对的，是“内部碎片化”，即一个页没有被完全使用。不过因为页的单位大小一般不大，相对外部碎片化来说，内部碎片化带来的内存浪费要少许多。\n分页机制的工作原理 # 在 Intel 的设计中，分页机制是基于分段机制之上的。无论是否使用分页，分段机制都必须被开启。其结果就是，我们之前提到的寻址和保护机制等在分页模型下继续存在，我们是在之前的基础上追加分页。即，将段分成更小的页。\n在之前的分段模型中，不论是实模式还是保护模式下，都是基于“段基地址 + 段内偏移”的方式获得的。处理器中有专门对其进行计算的模块，我们叫它段部件。我们把由段部件获得的地址称为线性地址。\n在不开启分页机制时，线性地址就是内存物理地址。它被直接用到地址线上，帮助我们进行寻址。在使用分页机制之后，段被进一步拆分为更小的页。操作系统按照页为单位进行内存的分配和回收。其结果就是，一个段在逻辑上是连续的，但是，被分割为多个页之后，各个页可能被分配在不同的位置。这使得段和页之间有了一层映射关系。\n因为段的分割，以及其对应页的随机位置分配，之前由段部件提供的线性地址不再能被直接使用于内存寻址，而是要经过页部件的处理，才能得到最终可用的内存物理地址。所以，在开启分页之后，段部件生成的线性地址，又被叫做虚拟地址。\n图示大致如下： 如何通过分页机制进行寻址 # 上面我们提到，在使用分页后，之前我们熟悉的线性地址不能再被直接使用，而是需要再被进行一次转换。\n这个转换倒也不复杂。假设我们有一个段，它被分成了 3 个页。因为页的大小是固定的，我们可以根据具体的线性地址，通过计算，知道对应的页是谁。基于那个页的基地址，再加上偏移量，我们就从线性地址转换到了内存的物理地址。\n从上面的过程可以发现，要把通过分段获得的线性地址（虚拟地址）转换为分页后的物理地址，我们需要两个信息：\n段和页的映射信息。 一个页的相关信息。最基本的，这个页具体的物理地址。 具体的，我们把内存空间按页的大小进行分割之后，使用页表来存放各个页的相关信息。因为页的大小固定，通过简单的除法就能从线性地址得知页在页表中的 index；而对应的页表项中存放了该页的信息。\n页表分级 # 上面其实已经体现了分页的工作核心。但是，实际应用起来，我们会发现一个问题，就是页表太大了。\n以 32 位系统为例。系统的寻址空间是 4GB，每个页表项大小为 4 bytes (32 位地址），假设以最常见的 4KB 为一个页的大小，则页表需要占用 4MB 的空间。\n这个空间占用看似不大，但是，接下来我们会提到，为了加强任务间的隔离，各个任务会使用自己的一份页表。这样一来，当同时运行的任务数量变多时，光页表自己就已然成为了内存消耗大户。\n为了解决这个问题，人们选择使用页表分级的方式来节约内存。以二级表为例，除了上述的页表之外，再添加一个页目录表。我们把页表里的表项分组，将上述的一个大的页表，分成多个小的页表。然后，页目录表中的表项记录页表们的信息。如此一来，对应页所在的页表就可以在需要的时候才创建，在不被使用的时候被交换到二级存储器中去，节约内存空间。\n有些朋友可能会问，只用一层级的页表，我们按需扩容不行吗？没用到的内存地址，就不为它生成对应的页表区域，需要的时候再扩展，那不也行吗？可惜的是，我们的线性地址和页表项的映射关系计算是静态的，如果一开始就需要用到高位的内存空间，又只有一级页表，那页表必须一开始就覆盖整个地址空间。碰巧的是，我们还真的需要在一开始就使用到高位和低位的地址空间，所以答案是否定的。为什么会用到，卖个关子，稍后再说。\n页目录表，页表，以及对应内存的关系大致如下图所示： 页目录表以及页表各自的表项内容如下。除了基地址外，还有一些域，它们和页的使用情况记录有关。感兴趣的朋友可以另外查询手册了解。 地址变换过程 # 有了地址变换所需的数据结构，现在我们就来看一看具体的地址变换过程。我们假设一个页的大小是 4KB，页表的页表项为 4 bytes，页目录的表项也为 4 bytes，每 1024 个页表项被分为一组，登记在页目录中。因为映射方式是静态的，过程其实很直接：\n页部件将段部件给出的线性地址分为三段：高 10 位，中 10 位，低 12 位。 高 10 位作为页目录表的 index，帮助我们找到对应页表区段的起始地址。 中 10 位作为页表的 index，帮我们从页表中找到对应页的页表项。 页表项中有着该页的基地址，线性地址的低 12 位作为页内偏移量，和基地址相加，最终获得物理地址。 看图理解会来得更直接一些。 分页以及虚拟地址空间 # 文章开头我们提到，保护模式下的分段机制已经可以支持虚拟内存。不过，在分段机制下，所有的任务共享者一个地址空间。虽然每个任务有自己的 LDT，但是各个 LDT 中的段描述符内的地址肯定是不同的，除非我们故意要指向相同的区域。\n在分页机制下，因为通过页表，我们引入了另一层的间接寻址访问。基于这个间接访问层，我们可以通过使用不同页表的方式，创建多个虚拟内存空间。这也是处理器设计者推荐的，所以之前我们说，一般我们会为每个任务创建它自己的页表。\n通过使用不同的页表，就算两个任务使用相同的虚拟地址进行寻址，因为页表内最终的映射的位置不同，它们会访问到不同的内存地址。这提供了更好的任务间隔离。\n硬件支持 # 有多个页表，就自然有了去哪找到页表的问题。和之前 LDT 相似，处理器为记录页表的基地址，提供了一个单独的寄存器，这个寄存器就是 CR3。它存放着当前任务的第一级页表（在我们的例子中，页目录表）的起始物理地址。每个任务的 TSS 中也有相应的域记录第一级页表的起始地址，所以任务切换的时候，处理器会自动的帮我们进行切换页表。\n全局空间和局部空间的映射 # 每个任务有了自己的页表，这使得每个任务都访问自己的私有页面。但是除了各个用户任务私有的内容之外，它们还要访问一些公共资源，比如操作系统内核提供的公共函数。\n在之前的分段模型中，我们通过各种门来进行内核公共函数的调用。因为分页是基于分段之上的，所以之前的内容并没有改变。但，因为现在任务完全使用自己的虚拟空间，为了让它们能够找到那些公共的资源，我们需要相应的为它们设置页表来映射到对应的位置。使得每个任务可以通过自己的虚拟空间地址和页表找到这些资源。\n解决方法倒也不复杂。我们将整个虚拟内存空间分为局部空间和全局空间两个部分。全局空间的地址范围内，我们将各个任务的页表都映射到共享的资源去；对于每个任务自己的内存需求，则限制在局部空间的地址范围内。\n一般来说，我们把地址空间的高地址区作为全局空间，低地址区作为局部空间。这就是之前页表分级部分中提到的，我们要同时使用高低地址区域的原因。\n开启分页功能的小技巧 # 平坦模型 # 32 位以上架构中，不需要分段也可以拥有完全的寻址能力。加上分页也提供了任务间的隔离，使得分段不再具有太大的意义。\n在实际实现中，人们会仅仅声明一个拥有整个空间寻址能力的段，基于这个段之上利用分页进行内存管理。这就是所谓的分段模型。如此一来，一个程序中的所有数据都按照统一的基地址来安排，省去一些切换段寄存器的麻烦。\n解决页表的“蛋鸡问题” # 敏锐的朋友可能发现了，上述的分页访问模式中，在修改页目录表的时候，有一个“鸡生蛋，蛋生鸡”的问题。它发生在访问页目录表的时候。因为开启分页后，一切的访问都需要通过页目录表，页表，以及页部件的配合，来将虚拟地址转换为物理地址。\n我们可以通过 CR3 获得页目录表的物理地址，但没有用。就算它是我们想要的结果，我们现在却不能用它进行直接的寻址。想要修改页目录表，我们需要通过它自己先来找到它的物理地址。\n为了解决这个问题，我们需要在开启分页前，在页目录表中添加指向自己的页表信息。一个巧妙的办法是将页目录表中一个表项指向目录表自己，如此一来，它既是页目录表，也是页表。书中给的做法是在页目录表最后的一个表项内添上目录表的物理地址，便于构造相应的虚拟地址。\n总结 # 因为分段机制不能很好的应对多任务情况下的内存管理，我们引入了分页机制。 Intel 的架构中，分页机制工作在分段机制之上。分页以固定长度为单位对段进行分割，并进行管理。 为了记录段和页之间的映射关系，我们添加了页表这样一个数据信息。相应的，为了适应多任务切换，处理中有 CR3 寄存器存储页表基地址，TSS 中也有相应的域来存储该地址。 分页可以提供多个虚拟空间，使得任务间的隔离更加完全。 ","date":"July 10 2022","externalUrl":null,"permalink":"/books/x86-assembly-shuangwang/04.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/","section":"Books","summary":"","title":"Note - x86 汇编语言（4）：分页机制","type":"books"},{"content":"x86汇编语言 从实模式到保护模式\n李忠 / 王晓波 / 余洁\n","date":"July 10 2022","externalUrl":null,"permalink":"/books/x86-assembly-shuangwang/","section":"Books","summary":"","title":"x86 汇编语言 -- 李忠 / 王晓波 / 余洁","type":"books"},{"content":" 这是系列笔记的第三篇。在上一篇中，我们介绍了保护模式基于段选择子的寻址方式，这一篇中我们来讲一讲该架构的另一个特点，多任务支持。\n其余章节如下：\n第一部分: 计算机基础和实模式 第二部分：保护模式下的分段寻址和权限 第三部分：多任务支持 第四部分：分页机制 任务：运行程序的表达 # 在之前的篇章中我们尽量避免了任务概念的出现。在 80286 之前，计算机主要以单任务（single-tasking）的方式运行。即，先集中于完成一个程序的运行，再运行下一个程序。有些类似于最初 mainframe 的 batching 的感觉。80286 首次在处理器硬件上提供了对多任务的支持，80386 时期多任务得到了广泛的应用。\n所谓任务，就是我们所熟知的进程。它是一个运行中的程序的抽象表达。我们知道，一个处理器只能在一个时间内运行一个指令。所以，多个程序的“同时”运行是一个假象。它的本质是处理器为各个程序分别运行一段时间，然后快速地在各个程序之间切换。\n一个程序在运行的时候，它会使用到计算机的各个部件，或者叫资源。要在多个程序之间快速切换，不仅要切走，还要能切回来。即，我们在载入其他程序之前，需要把当前程序所用到的资源保存起来，这样以后才能被重新载入，继续运行。这些与一个运行的程序相关的资源，可以叫做 context。所以，我们也会把任务切换叫做 context switching。\n这些与一个任务相关的信息被存放在一个叫做 Task State Segment (TSS) 的内存区域中，它的格式和保存的内容如下：\n任务间隔离 # 在上一篇中我们提到，保护模式下，我们使用段选择子从段描述符表中获取段描述符的方式来进行寻址。我们也提到了一个系统全局的描述符表，GDT。\n但是，如果仅有一个 GDT，基于段的保护措施是无法实现真正的保护的。因为每个任务都需要访问段描述符表来进行寻址，那么 GDT 所在的内存区间需要对所有的任何开放。其结果就是，一个任务可以访问同级别或低级别其他任务的段，只需出给相应段的段选择子就好了。\n为了杜绝任务间的非法访问，我们不能让每个程序都通过 GDT 来进行寻址，而是让每个任务有自己的一个描述符表，这就是 Local Descriptor Table (LDT)。\nLDT 在结构上和 GDT 没有不同，只是每个任务都有自己单独的 LDT，而且进行内存访问的时候只能使用自己的 LDT 来进行。因为 LDT 内只有和该任务自己相关的描述符存在，上述问题就得到了解决。\n处理器对支持多任务所做的设计 # 为了应对多任务，我们引入了 TSS 和 LDT。那么为了便捷地找到 TSS 和 LDT 我们的处理器就增加了两个相应的寄存器：TR 和 LDTR。\nTR 寄存器中我们存放当前任务 TSS 的地址，LDTR 中我们存放当前任务 LDT 的地址。段选择子中有一个 TI 位，指明我们需要使用 GDT 还是 LDT 进行寻址。\n在实际的运行过程中，任务间的切换是相当频繁的。为了提高切换的速度，处理器会协助我们进行任务切换。具体的来说，只要我们给出想要切换的目标任务的 TSS，处理器会自动帮我们存储当前任务的信息，并且载入目标任务的信息。\n如何管理任务 # 任务的切换由处理器来协助，但是什么时候进行切换，切换到哪一个任务是由操作系统说了算的。\n为了进行任务调度，操作系统需要维护各个任务相关的信息，当前其中必须要有 TSS 和 LDT 的位置信息。所以，操作系统需要有一个内存区域来存放关于任务的信息，这个区域就叫做 Task Control Block (TCB)。\nTCB 并不是处理器的工作要求，而是存粹的操作系统软件内容，所以，每个操作系统都可以设计自己管理 TCB 的方式，以及存放的信息。最最简单的模型就是，TCB 中只有 TSS 和 LDT 域，然后每个 TCB 用链表连接起来。当然，实际生活中的操作系统会用更加成熟高效的管理方式。\n如何进行任务切换 # 任务切换有两种方式：\n协同式：即当前工作的任务主动放弃执行权；或者在其通过调用门请求操作系统系统服务的时候，由操作系统趁机把控制转换到另外的任务。 抢占式：一般情况下，是利用定时器中断，并在中断服务程序中实施任务切换。因为硬件中断信号总是会定时出现，任务切换总能够进行。 使用控制转移指令，我们可以进行协同式的切换；利用中断，我们可以进行抢占式的中断。\n这里追加一个信息。前一章中我们提到，可以使用调用门来进行特权转换。实际上，除了调用门之外，我们还有三类与中断相关的门，它们被存放在中断描述符表（IDT）中：\n中断门 陷阱门 任务门 中断门和陷阱门在一般的中断处理中使用，指向的是处理函数的代码段选择子和段内偏移；而任务门则是用来进行任务切换的，存放着任务的 TSS 选择子。\n利用控制转移指令进行任务切换 # 我们可以利用 call 和 jmp 指令并给出一个 GDT 中的 TSS 描述符。处理器转移过程中，发现对象是 TSS 描述符，就会相应的进行任务切换。\n相似的，我们也可以利用 call 和 jmp 指令并给出一个任务门描述符。处理器判断出描述符指向任务门后，进行相应的任务切换。\n利用异常和中断进行切换 # 如果异常或中断发生的时候，中断号指向的是中断描述符表中的任务门，处理器会进行相应的切换。\n使用中断的时候有一个要点，即，不论是中断门，陷阱门，还是任务门，它们都是中断相关，所以最后都是用 iret 指令返回。前者是返回同一个任务的不同代码段，后者是返回到之前被中断暂停的任务。那么问题是，处理器如何知道自己该怎么做呢？\n答案在 EFLAGS 寄存器的 NT 位上。NT 即 Nested，如果 NT 位被置为 1，则说明当前的任务是嵌套于其他任务，或者说，是从其他任务切换而来的。\n处理器每次执行 iret 指令的时候都会检测当前 EFLAGS 寄存器的 NT 位。如果是 0 则说明是中断，直接返回即可；如果是 1 说明是任务切换，需要保存当前任务信息并载入之前的任务（之前任务的 TSS 位置被记录在当前任务的 TSS 中）。\n这里也仅仅是简单地做一个介绍，实际的运行过程中还有相应的权限检测等内容。有需要的话请参照手册学习。\n总结 # 任务是一个运行中的程序的逻辑表达。也就是所谓的进程。 为了支持多任务，我们引入了 TSS 来存放一个任务的运行环境信息，以及 LDT 来进行任务间的空间隔离。 处理器在任务切换的时候会自动地帮助我们存储当前任务的信息，并载入新任务。但是什么时候进行任务切换，切换哪个任务，是由操作系统控制的。 ","date":"July 6 2022","externalUrl":null,"permalink":"/books/x86-assembly-shuangwang/03.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/","section":"Books","summary":"","title":"Note - x86 汇编语言（3）：多任务支持","type":"books"},{"content":" 这是本系列笔记的第二个部分。这个部分里，我们来看一看保护模式的一大改变——寻址方法，以及配合而来的保护功能。另一个大改变是对多任务的支持，我们在之后的篇章里再谈。\n多提一句，保护模式，protected mode，其实是 protected virtual-address mode 的略称。虚拟内存在保护模式下成为可能，不过这一篇中我们不谈它。另外，本文描述的保护机制皆基于段而言，分页模式下有另外的保护方法。\n其余章节如下：\n第一部分: 计算机基础和实模式 第二部分：保护模式下的分段寻址和权限 第三部分：多任务支持 第四部分：分页机制 段寄存器的改变 # 上一篇中我们提到，8086 为了增强自己的寻址能力，提出了分段模型。它在段寄存器里存储段基地址，然后利用此地址与偏移地址结合实现寻址。\nIntel 在 80286 处理器中提出了新的段选择器设计。80826 还是一个 16 位的处理器，但是它的地址线被继续扩充至 24 位。如此一来，分段访问是不可避免的。 但是，处理器的设计者没有选择之前的左移策略，而是提出了一个新的段寄存器设计以及随之而来的新的寻址方式，提升了寻址的速度。先来看看新的段寄存器的结构：\n如图所示，段寄存器被分为了两个部分：第一个部分是 16 位的段选择器，还有一个对我们隐藏的描述符高速缓存器。我们可以向段选择器部分赋值，但是高速缓存器部分是由处理器自己控制的。\n在这个结构之下，我们不再是直接向段选择器赋值段基地址，而是给与一个叫做段选择子的值。然后通过它找到对应的段描述符，最终通过描述符中的记录的段基实现寻址。也就是说，我们现在需要间接性的获得段基地址。\n有些人把 80286 的这种工作模式叫做“16 位保护模式”。这样的方式使得 80286 不再需要使用左移的方式来获得段基地址，也同时提供了保护能力。因为 80286 还是 16 位的处理器，虽然寻址能力增强了，但是每个段内还是只能有 16 位的寻址能力。但因为当时软件的缘故，很多人没有利用 286 的保护模式。所以，一般谈到保护模式的时候，都是以 80386 的 32 位架构为例，本文也是如此。但是 286 才是第一个引入保护模式和多任务的处理器，是一个颇具影响力的处理器。\n新结构下的访问方法 # 上面提到，在新的结构下，我们通过段选择子来找到段描述符，最终实现寻址。那么从哪里找呢？答案是描述符表。我们得先在内存中创建一个描述符表，然后将描述符放入其中，在此之后，才能够进行正常的寻址，这也是处理器总是从实模式开始运行的原因。\n描述符的大小是固定的。依据描述符表的基地址以及描述符在表中的 index，就能找到该描述符。描述符在表中的 index 由段选择子提供，而为了记录描述符表的基地址，处理器增加了新的寄存器。\n首先，对于整个系统，有一个全局的描述符表，叫做 global descriptor table (GDT)。为了存放它的起始地址，我们有了一个新的寄存器，GDTR。之所以叫“全局”，是相对任务私有空间而言，在这个篇章内，我们先按下不表。\n当我们为段寄存器赋值新的段选择子时，处理器会自动的从 GDTR 获取基地址，并根据计算获得相应的段描述符。获取到的段描述符被缓存在段寄存器的高速缓存区中。在不改变段选择子的情况下，处理器会直接使用缓存区中的描述符，而不是每次都访问 GDT。\n通过描述符表和描述符来进行寻址的方法还可以任务之间的隔离。这一篇章我们先集中于寻址方法，多任务在下一篇再说。\n保护模式保护些什么？ # 在 Intel 提出 IA32 的 32 位处理器架构之后，数据线和地址线的长度得到了统一，都是 32 位。如此一来，不需要分段模型，我们也可以随意地访问 32 位数值所能表达的内存空间（4GB）。但是，处理器的设计者没有抛弃分段模型，而是将其目的由增强寻址能力，变为了提供保护能力。\n那么保护模式保护了些什么呢？最重要的是三个部分：\n访问范围的保护 类型保护 访问权限保护 下图是段描述符内的各种信息情况。可以看到除了我们需要的段基地址之外，描述符中还添加了许多其他的域。\n其中，段界限部分告诉我们段的长度是多少。处理器会在我们发送请求的时候检测我们给出的 offset 是否超出了段的范围，这是访问范围的保护。\n图中还可以看到一个 Type 区域。它表明该段的类型，是数据段，还是代码段？如果我们想赋值一个数据段给 CS 寄存器从而运行里面的内容，那是被禁止的。这是类型保护。\n最后，在图中还有一个 DPL 区域，即 descriptor privilege level，代表该段的访问权限。处理器会在更改段寄存器内容前，检测访问请求者是否有访问该内存段的权限。\n对于各个部分的保护，其具体的检查方式就不在此赘述。大家可以参阅书籍以及处理器手册。\n保护模式的权级保护 # 从刚才的图中我们可以看到 DPL 一共是 2 位。所以，我们可以有四个不同的等级，数值越低，则权限等级越高。即，0 级是最高的等级。\n权限等级设计使得程序指令所能访问的空间可以得到限制。这是我们想要的，特别是 i32 作为一个为多任务而生的架构，防止恶意程序对其他任务搞破坏是很重要的。\n例如，操作系统内核的代码和数据十分重要。将内核所使用的内存段设为高权限，而让一般的程序运行在低权限，如此一来，一般的程序不能访问到高权限的内存地址，也就不能接触内核，从而使得系统的安全性得到了提高。\n那么，随之而来的问题是，我们知道段有了自己的访问权限，并且在段描述符中得以表达。可是访问请求者，即指令的权级是如何表达的呢？\n这个问题乍一看有些没头脑。我们都知道，操作系统它有高权限等级，一般用户程序的进程有低权限等级，这不就分开了呗？可是，对于处理器来说，它可不知道什么操作系统和用户程序的区别，这些是我们抽象的逻辑单位。所有程序都是指令流，指令本身可没什么高低的说法。\n应对这个问题，聪明的处理器设计者想出了一个法子，就是让 DPL 分饰两角。即，指令所在的段的等级，就是指令的等级。\n可执行的指令被存放在代码段中，代码段被赋值给 CS 寄存器。CS 和 IP 两个寄存器引导着下一个将要执行的指令的位置。当一个代码段的描述符被加载到 CS 寄存器后，该段的 DPL 就成为了该段内指令的等级。CS 寄存器中描述符的 DPL 就有了一个新名字，叫 CPL，current privilege level。\n特权级转换 # 上面我们举过一个例子，我们将内核程序的段描述符的 DPL 设置为高等级，将一般用户程序的段描述符的 DPL 设置为低等级。如此一来，就算一般用户知道内核程序的代码和数据在哪，处理器也会阻止它进行访问。\n但是这也带来了一个问题，那就是，其实有些内核的内容我们是想要给一般用户程序使用的。例如，内核提供给其他程序的公共函数。现在，这些内容没法被用户程序直接使用了。\n解决这个问题的方法倒也直接，把用户指令的权限等级给提高了就行了呗。但是，用户程序是不可能自己给自己提升权限等级的，不然谁想在低等级呢？处理器设计者给出的方案是，提供一些特定的入口，使得正在运行的代码通过这些入口后特权级得到转换。\n这种入口有两个，分别是依从代码和门：\n依从代码：代码段的描述符中 Type 域中有一个 C 位，表明该代码段是否是依存代码。 依存代码可以被比它低等级的程序直接调用。 在跳转控制，载入代码段时，处理器不会改变 CS 中的 CPL 字段。使得依从代码在其调用者的权限等级下运行。 门：是一种描述符。和段描述符的使用方法类似，也是被存放在描述符表中。门描述符内存放着目标函数所在代码段的段选择子和段内偏移。（简直是描述符的连接大联欢^.^) 我们可以使用 jmp far 和 call far 来调用门，实现代码控制转移。 jmp far 和依从代码类似，处理器在转移时，不会更改 CS 的 CPL 字段，使得被调用函数工作在请求者的特权级上。 call far 则使用目标代码段的特权级别运行。而且，除了 return 之外，不能把控制转移到低特权级的代码段（即，高级别函数不能调用低级别函数）。 为了配合特权级的切换和数据保护，每个任务针对每个特权级别都有单独的栈空间。在切换特权级的同时，也会切换到相应等级的栈。\n权级保护的最后一块拼图——RPL # 根据之前的描述，在拥有了 CPL 和 DPL 控制访问权限，也拥有了依从代码和门使得低特权程序能够调用操作系统提供的公共函数，似乎保护模式已经完整了。可惜的是，我们没有考虑一个特殊情况，就是有人会搞破坏！\n以一个 I/O 请求为例，它是通过操作系统提供的接口来实现的。用户程序调用 I/O 请求函数，同时给出读写的磁盘地址和内存地址。我们通过门调用了系统的 I/O 函数，相应函数的地址被导入 CS 寄存器，CPL 也随着被更改到更高的等级。I/O 函数由操作系统提供，我们暂且简单地认为操作系统是可靠的。那么，整个函数的运行过程是安全的。\n但是，I/O 请求函数的运行过程中，并不是所有的元素都是操作系统控制的。具体的来说，读写请求的磁盘地址和内存地址是由用户程序提供的。如下图所示，如果低特权程序将高特权级的内存地址作为参数给到 I/O 请求函数，因为该函数拥有最高特权，它就代替了低特权程序访问到了该程序本不该访问到的内存空间。\n这其实就是保护模式工作原理的一个漏洞。即，保护模式仅仅在修改寄存器的时候检测当前指令的特权级。而在上面的例子中，用户程序没有直接访问高级别的数据段，处理器就提供不了保护。\n上述漏洞的问题在于，处理器只是单纯的执行指令，它可不知道这个指令属于谁。为了解决这个问题，处理器设计者提出了一个软硬结合方案，即让操作系统参与到权限检测的过程中来。\n具体来说，因为是操作系统提供的高特权函数，它可以在函数代码中包含一些信息，使得处理器可以知道数据的来源。这个信息就是 RPL，request priviledge level. 因为我们依靠段选择子来进行寻址，所以，RPL 是被嵌入到段选择子中的。\n在引入了 RPL 之后，低特权程序在调用操作系统提供的函数时，函数中会设置参数中的选择子的 RPL 为低特权程序的权级。处理器在使用选择子的时候，除了比较 DPL 和 CPL 之外，也会比较 DPL 和 RPL。如此一来，就杜绝了上述系统函数代替低特权程序访问其不能访问的空间的问题。用 Intel 的话来说，RPL 的引入“确保了特权代码不会代替应用程序访问一个段，除非应用程序自己拥有访问那个段的权限”。\n需要注意的是，处理器本身仅仅是添加了检查 RPL 的步骤而已。设置 RPL 是软件自己来进行的。\n基本的特权级检查规则可以请查看本书的 14.1 节或者处理器手册。\n总结 # 新架构中，段寄存器的结构进行了升级。由段选择器和缓存区组成。 开启保护模式之后，我们使用段选择子，到描述符表中获取段描述符的方式进行寻址。 通过描述符中的 DPL，CS 寄存器中的 CPL，以及段选择子中的 RPL，我们得以实现了保护功能。 ","date":"July 5 2022","externalUrl":null,"permalink":"/books/x86-assembly-shuangwang/02.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/","section":"Books","summary":"","title":"Note - x86 汇编语言（2）：保护模式下的分段寻址方法和权限设计","type":"books"},{"content":" 最近读完了《x86 汇编语言：从实模式到保护模式》一书。感觉此书是了解 Intel 架构 CPU 运行方式的好材料。所以，我准备用四个篇章来写一份读书笔记，简略地介绍一下内存寻址方式，以及处理器对多任务的支持。笔记的内容意在指出书中的框架重点，作为一个快速复习和检索的工具，而非详实的内容。\n本篇是该系列的第一个章节，专注于计算机的运行，8086 处理器，以及实模式。内容较为简单直接。其他章节如下：\n第一部分: 计算机基础和实模式 第二部分：保护模式下的分段寻址和权限 第三部分：多任务支持 第四部分：分页机制 计算机的基本结构和运行本质 # 我们当今使用的计算机结构为冯诺依曼结构。这个结构的特点就是以存储为中心。具体些说，这个结构中，我们将程序指令和数据以二进制的形式，不加区分地放在内存中。\n程序的编写者将提前编写好的程序载入内存，并告诉计算机将要执行的指令所在的位置，计算机便会到指定的地方找到指令并根据指令运行。抽象来说，计算机运行有 4 个步骤：\n取指 译码 执行 回写 可见，整个运行的过程主要就是两个内容：内存的读写，和计算。为了支持这个运作方式，硬件上，我们的计算机有五个重要的部分：\n运算器：进行计算 控制器：引导指令运行顺序 储存器：存放指令和数据 输入设备：和计算机进行沟通 输出设备：同上 虽然我们每天接触的计算机看似非凡超能，但是它其实是一个十分单纯的机器。从通电的一刻开始，计算机就是在不断地重复我们上面讲的四个运行步骤。\n以计算机的启动过程为例 # 我们使用的计算机，在每次通电的时候，都会从一个提前指定好的内存地址开始，进行取指和运算。\n那个起始位置，存放的就是 BIOS 的第一行指令。BIOS 在完成自己的工作后，其最后的一部分指令就是把其他的程序读取到指定的内存位置，并让计算机从那个位置继续运行。\n一般来说，BIOS 的最后一部分指令会将启动磁盘的第一个扇区读取到内存中，并跳转执行。也是因为这个原因，我们把启动磁盘的第一个扇区叫做主引导扇区。\n因为我们可以方便地向磁盘写入数据，主引导扇区是我们最先能接触到的代码区域。它的容量不大，其中的指令一般是继续从磁盘中读取其他程序并运行。例如，它可以载入 bootloader 并最终引导操作系统。\n8086 的基本结构 # 8086 是一枚极其成功的处理器，它是应用最广泛个人计算机行业的基础，x86 的 86 就是说这个 86。它极大地影响了 Intel 接下来的处理器功能设计。可以说，更加新型的 Intel 处理器都是在 8086 的底子上进行功能的完善和添加。\n8086 是一枚 16 位的处理器。它的寄存器，以及内外部的数据线的位宽都是 16 位，不过外部地址线是 20 位的，这给了它更强的寻址能力，也引出了分段模型。\n8086 一共有三种不同的寻址模式 (即数据的读取和存放方式)：\n寄存器寻址 立即寻址 内存寻址 从寻址模式上可以看出，除了内存之外，寄存器也是一个相当重要部件。而对于这个章节的内容来说，最重要的寄存器就是指令寄存器 IP，以及多个段寄存器。\n指令寄存器之所以重要，是因为处理器根据它的内容来进行取指。而段寄存器之所以重要，是因为我们需要借助它们来进行内存寻址。\n8086 的分段模型 # 上面我们提到除了外部地址线是 20 位的，8086 的其他部件都是 16 位。之所以提出内存的分段访问模型，就是为了能够实现 20 位的寻址能力。\n其方式为：段寄存器内的值左移 4 位，加上 16 位段内偏移。如此一来，便拼凑出了 20 位的地址。处理器会根据此地址直接和内存沟通，实现数据的读写。\n段寄存器内的值叫做段基地址。通过修改段寄存器内的段基地址，我们就可以在逻辑上将内存分段，每个段最大有 16 位的寻址能力。分段不仅使得我们有了更强的寻址能力，也让程序载入重定位变得更加容易：程序编写的时候使用相对某个段的相对地址，使得载入位置变得随意。\n具体的寻址指令以及处理器的其他信息就不在此叙述，手册才是它们的归宿。\nIntel 处理器的实模式 # 实模式，又叫实地址模式。它其实就是 8086 的运行模式。这个“实”字其实有两个含义：\n第一个含义是，我们在指令中给出的内存地址，即为处理器使用的地址。我们直接向段寄存器赋值段基地址，然后与段内偏移结合，获得内存物理地址。\n实模式的另一个含义，其实也是上面提到的 8086 的特点，就是它无条件的相信我们给出的指令。在实模式下，所有的段都可以随意读、写，其中存放的内容也可以被随意执行。\n接下来我们要讲的保护模式，就是对针对各个段的读写，以及执行加以权限控制的模式。\n总结 # 我们现在使用的冯诺依曼结构计算机，将数据和指令无差别地以二进制存放在存储装置中的。 计算机很单纯。它就是在不断地读取指令，运行指令，然后将运行结果存放。 8086 处理器通过段寄存器实现了分段模型进行内存访问，作为一个 16 位处理器，提供了 20 位的寻址能力。 8086 很单纯。它允许我们任意地读、写、执行内存中的内容。 ","date":"July 3 2022","externalUrl":null,"permalink":"/books/x86-assembly-shuangwang/01.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/","section":"Books","summary":"","title":"Note - x86 汇编语言（1）：计算机基础和实模式","type":"books"},{"content":" Motivation # File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution # A real system that shows us how to do cache coherence and distributed transactions. Solution # The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache. Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers. All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured. Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.) Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily. The underlying distributed storage and the lock server are both using Paxos to ensure availability. Evaluation # They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c. They tested both the local performance and the scalability of the proposed system. The Main Finding of the Paper # Complex clients sharing simple storage can have better scalability. Distributed lock can be used to achieve cache coherence and distributed transaction. ","date":"July 1 2022","externalUrl":null,"permalink":"/papers/hekkath-1997-frangipaniscalabledistributed/","section":"Papers","summary":"","title":"SOSP'97 - Frangipani: a scalable distributed file system","type":"papers"},{"content":" Motivation # MVCC is the most popular scheme used in DBMSs developed in the last decade. However, there is no standards about how to implement MVCC. Many DBMSs tell people that they use MVCC and how they implement it. There are several design choices people need to make, however, no one tells why they choose such way to implement their MVCC. This paper gives a comprehensive evaluation about MVCC in main memory DBMSs and shows the trade-offs of different design choices. Contribution # A good introduction about MVCC. Evaluation about how concurrency control protocol, version storage, garbage collection, and index management affect performance on a real in-memory DBMS. Evaluation of different configurations that used by main stream DBMSs. Advisings about how to achieve higher scalability. Solution # This is an evaluation paper. There is no \u0026ldquo;solution\u0026rdquo; but evaluation analyses. People mainly focus on concurrency control protocols when they talk about scalability. However, the evaluation results show that the version storage scheme is also one of the most important components to scaling an in-memory MVCC DBMS in a multi-core environment. Delta storage scheme is good for write-intensive workloads especially only a subset of the attributes is modified. However, delta storage can have slow performance on read-heavy analytical workloads because it spends more time on traversing version chains. Most DBMSs choose to use tuple-level GC. However, the evaluation result shows that transaction-level can provide higher performance (at least in main memory DBMS) because it has smaller memory footprint. In terms of index management, logical pointer is always a better choice than physical pointers. The design choices that Oracle/MySQL and NuoDB made seems have the best performance. Evaluation # They uses a DBMS implemented in CMU, called Peloton. The evaluation platform has 40 cores with 128 GB memory. Workloads are YCSB and TPC-C (both OLTP) The main finding of this paper # If you want to learn MVCC, read this one. Still, \u0026ldquo;Measure, Then build\u0026rdquo;. ","date":"July 1 2022","externalUrl":null,"permalink":"/papers/wu-2017-empiricalevaluationinmemory/","section":"Papers","summary":"","title":"VLDB'17 - An empirical evaluation of in-memory multi-version concurrency control","type":"papers"},{"content":" Motivation # We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.\nTo understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.\nThis paper focus on concurrency control schemes. (spoiler: the [following research]([[@An empirical evaluation of in-memory multi-version concurrency control]]) of [[Andrew Pavlo]] found out concurrency control schemes are not the most important component that affect the scalability of main memory DBMS on many-core environment )\nContribution # Evaluation of the scalability of seven commonly used concurrency control schemes (OLTP, in-memory DBMS) Evaluation is processed on a simulated machine with 1000 cores (actually is a cluster of 22 real machines) Solution # This is evaluation paper, thus there is no solutions but evaluation analyses.\nAll concurrency control schemes fail to scale to a large number of cores. The bottlenecks are:\nlock thrashing preemptive aborts deadlocks timestamp allocation memory-to-memory copying Lock thrashing happens in any waiting-based algorithm. Using the \u0026ldquo;non-waiting\u0026rdquo; can alleviate this problem, however, we will have more aborting.\nIn high contention workloads, a non-waiting deadlock prevention scheme is better than deadlock detection. (Restart is fast in main memory DBMS)\nMemory allocation (includes timestamp allocation) is usually managed by a centric data structure, which becomes the bottleneck. Avoiding shared, centric data structure is important to achieve higher scalability.\nAdd new hardware to offload some tasks (e.g., memory copying) from CPU is a feasible way to improve the scalability\nEvaluation # The evaluation platform is based on a CPU simulator, Graphite (from MIT). The authors implemented a lightweight main memory DBMS only for this testing. Workloads are YCSB and TPC-C Main finding of the paper # Like other systems, to improve the scalability, how to avoid shared data is the key. \u0026ldquo;Measure, Then Build\u0026rdquo;, and a thorough measurement is always needed. This paper is good enough on analyzing concurrency control schemes, however, the following research revels that this is not the most important part\u0026hellip; ","date":"July 1 2022","externalUrl":null,"permalink":"/papers/yu-2014-staringabyssevaluation/","section":"Papers","summary":"","title":"VLDB'14 - Staring into the abyss: An evaluation of concurrency control with one thousand cores","type":"papers"},{"content":" Motivation # Large-scale distributed applications require different forms of coordination.\nUsually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.\nContribution # Exposes APIs that enables application developers to implement their own primitives, without changes to the service core. Achieve high performance by relaxing consistency guarantees Solution # ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.\nZooKeeper provides a set of simplified APIs to access these data nodes. Application develops can use these APIs to implement different forms of coordination that they want.\nZooKeeper servers use an atomic broadcast protocol, Zab (similar to Raft) to sync state between each other.\nIt is vague about how to implement ZooKeeper. However, since it is an open source project, the source code is always available.\nIn short, ZooKeeper guarantees correct coordination with high performance through:\nUsing wait-free data objects instead of blocking. FIFO client ordering of all operations. Linearizing all writes to the leader, then propagate to other servers (they call this A-linearizability). Read locally. Can have stale data, however, much faster. Asynchronous enables batching to reduce networking and storage I/O overhead. Main finding of the paper # Abstraction of basic building components can help create more general systems. ZooKeeper is one of the best examples that shows us how to weak consistency in favor of higher performance. ","date":"May 23 2022","externalUrl":null,"permalink":"/papers/hunt-2010-zookeeperwaitfreecoordination/","section":"Papers","summary":"","title":"ATC'10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems","type":"papers"},{"content":" MOTIVATION 1 # A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.\nCONTRIBUTION # Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the \u0026ldquo;state-machine approach\u0026rdquo;, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.\nThere are three challenges:\ncorrectly capturing all the input and non-determinism correctly applying the inputs and non-determinism to the backup low performance degradation To handle these challenges, they implemented a fault-tolerant virtual machines in VMware vSphere 4.0. This system reduces the performance of real applications by less than 10%, and needs less than 20 Mb/s network bandwidth.\nSOLUTION # There is one primary VM and one backup VM on different hosts. All input goes to the primary. And the input is sent to the backup VM via logging channel (network). A VM has a broad set of inputs and some additional information is needed for non-deterministic operations. They use the VMware deterministic replay to records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file. They design a protocol to ensure the failover (i.e., switching to the backup) is transparent to the clients. The core idea is that before send an output to the external world, we must need to make sure the backup VM has received the log entry that produces the output. The failure detection is handled by UDP heartbeating messages and logging traffic. To avoid \u0026ldquo;split-brain\u0026rdquo; situation, they store a flag in the shared storage, so VMs can know if there is any other running primary. LIMITATION # The limitation is that the implementation only works for uni-processor machines because recording and replaying the execution of a multi-processor VM can lead to significant performance issues (accessing to shared memory is non-deterministic operation).\nMAIN TAKEAWAY # It is helpful to distinguish between internal and external and internal events of the system. For an infrastructure, only external events can really affect other applications.\nD. J. Scales, M. Nelson, and G. Venkitachalam, “The design of a practical system for fault-tolerant virtual machines,” SIGOPS Oper. Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010, doi: 10.1145/1899928.1899932.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"May 23 2022","externalUrl":null,"permalink":"/papers/scales-2010-designpracticalsystem/","section":"Papers","summary":"","title":"SOSP '97 - The design of a practical system for fault-tolerant virtual machines","type":"papers"},{"content":" MOTIVATION # Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).\nBecause Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.\nCONTRIBUTION # They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.\nThis system is developed only for Google’s own programs. As a result, GFS does not provide POSIX APIs. Programs are designed and implemented based on GFS, which simplifies the design of GFS.\nSOLUTION # GFS uses a single master multiple chunkservers architecture. The master maintains all file system metadata and the chunkservers handle the file data. The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state. Considering the characteristics of the workloads in Google (append only, sequential read), they decide to divide files into fixed-size chunks (64MB). Each chunk has a globally unique chunk handle assigned by the master, that’s how we can find a chunk of a specific file. The client gives file name and in file offset to the master. Then, the master will send back the corresponding chunk handle and the chunkservers that have that chunk. After that, clients will communicate with chunkservers directly. This approach avoids the single master becoming the bottleneck. To ensure high availability and also improve parallelism, each file chunk is replicated on multiple chunservers on different racks. The metadata in master is protected by the operation log, also this log is replicated on multiple machines. When write happens, the data mutation propagates along the chunkservers incrementally. As a result, the write becomes faster, but clients can read stale data. EVALUATION # Micro-benchmarks on a small cluster with 16 chunkserver. Tested the read, write, and record append performance. Two real world clusters in Google. One for production, another one for research and development. LIMITATION # With the increasing volume of data, the single master design can no longer cope with the demands.\nMAIN TAKEAWAY # There are times when we can discard generalization and design a dedicated system for a specific scenario, which leads to a more simply design.\n","date":"May 22 2022","externalUrl":null,"permalink":"/papers/ghemawat-2003-googlefilesystem/","section":"Papers","summary":"","title":"SOSP'03 - The Google file system","type":"papers"},{"content":" MOTIVATION # Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.\nTo parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.\nCONTRIBUTION # They implement a library that can provide a simple interface that enables automatic parallelization and distribution of large-scale computations. In addition, the library handles machine failures without interaction with programmers.\nSOLUTION # Inspired by the map and reduce primitives present in functional languages, they provide a restricted programming model that parallelizes the computation automatically (the computation must be deterministic). Both Map and Reduce functions are written by the user. The input data will be partitioned into M splits, and each Map task handles one of them. The intermediate output of the Map function is divided into R pieces. The Reduce task will read the intermediate output of the Map function and generate the final output files. For a cluster of machines, we have only one master machine, and the others are worker machines. The master machine assigns tasks (Map or Reduce) to workers and tracks the state of each task. The worker machines communicate with the master when work is finished or communicate with other workers to read data to process. Fault tolerance is handled by periodical communication and re-computation when a machine fails. Because the computation is deterministic, recomputing the same task is okay. Many optimizations are applied in their implementation. Check the paper for details. EVALUATION # They used a cluster that consisted of around 1800 machines. Each machine has only 4GB of memory, and two 160GB IDE disks with a gigabit Ethernet link.\nThey tasted grep and sort workloads with 10^{10} 100-byte records (around 1TB). The results are shown in the terms of data throughput with the timeline.\nMAIN FINDING OF THE PAPER # It is useful to abstract a common pattern of certain computing tasks, and create an infrastructure to handle the common issues.\n","date":"May 16 2022","externalUrl":null,"permalink":"/papers/dean-2004-mapreducesimplifieddata/","section":"Papers","summary":"","title":"OSDI'04 - MapReduce: simplified data processing on large clusters","type":"papers"},{"content":"幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个 isolation level 下，会不会出现幻读，以及其解决方法。\n网上有许多关于幻读的文章，但是在读完之后发现，大多数的说明都浮于表面，好像作者们自己也并没有弄清楚幻读的本质。在本文中，我想利用数据库的一些高层抽象概念，来阐述幻读的本质。虽然不涉及任何的具体实现，但相信你在了解到这些概念之后，可以很快地理解幻读，以及各种幻读的处理方法。\n幻读以及其解决方法核心都可以浓缩在一句话中，即:\nThe phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.\n其中的关键词为: predicate read, insert or update, 和 nonserializable execution. 我们将以这些关键词为突破口，来理解幻读问题。\n并行可能性：Isolation 和 Conflict Serializability # ACID 是数据库事务的重要特性。其中的 I，即 isolation，指的是“多个事务同时运行的时候，它们之间是相互孤立的”。\n上面对 isolation 的标准定义我个人并不喜欢，因为它太过于抽象。依照我个人的理解，更具体一点儿说，isolation 的目标是保证多个事务同时运行的时候，不会因 race condtion 而使得数据库的一致性(consistency) 被破坏。\n那么如何实现 isolation 呢？其关键就是并行事务运行的 serializability.\nSchedules and Serial Schedules # 当多个事务同时运行的时候，因为任务调度是由操作系统在负责，这些事务所包含的操作会交错运行。这些操作的一个实际运行顺序被称为一个 schedule。可见，对于一组同时运行的事务，它们可能出现的 schedules 是非常多的 (有 n 个事务的话，会有 $n!$ 种可能)。\n依据我们对 race condition 的理解，对于同一个 data item，如果并行访问中有写操作参与，就会有 race condition 的出现。那么，对于一组并行事务来说，如果其中有两个或以上的事务对同一个 data item 进行访问，且其中包含写操作，我们就说这些访问操作之间是会有冲突的 (conflict)。我们需要对其进行运行顺序进行控制，否则，最终的结果是不可控的，可能会破坏数据库的一致性。\nConflict Serializability # 想要控制事务运行顺序，最简单的方式就是顺序运行。即，一个事务运行完成之后，才运行另一个事务。这种运行顺序，就是 serial schedules.\nSerial schedules 虽然保证了数据库的一致性，但事务的并行也随之消失了。为了提高系统的性能，我们还是想要事务在不破坏一致性的前提下并行工作。如果一些 schedules，它能够支持并行，且能保证其运行结果和 serial schedules 的结果相同，我们就叫它 serializable schedules。\n我们之前提到过，在一个 schedule 中分属于不同事务的两个连续操作，如果它们访问同一个 data item，且其中有一个是写操作，那么它们之间就是冲突的。即，在这个 scheule 中，它们两者的运行顺序是不能改变的。但，如果不冲突，这两个操作的顺序就可以交换。\n通过交换一个 schedule 中不冲突的连续操作，我们可以得到新的 schedule。如果一个 schedule，通过不断的交换各个事务间不冲突的操作，能得到一个 serial schedule，我们就说这个 schedule 具有 conflict serializability ，或者说它是 conflict serializable 的。\n无疑，conflict serializable 的事务运行顺序，就是我们想要的运行顺序，因为在允许事务并行运行的同时，它的运行结果和事务依次顺序运行时的结果一致。\n需要追加说明的是，serializability 不止 conflict serializability 一种。例如，还有 view serializability。但因为实现的难度等原因，几乎所有的数据库都是使用 conflict serializability。\n一致性弱化：Isolation Levels # Serializability 固然可以保证数据库在事务并行时的一致性，但因为它的限制较多，使得系统的整体并行性能受到了压制。\n有些时候，我们为了性能，根据实际的应用场景，可以牺牲一些对一致性的要求。这就是 isolation levels 的意义所在。SQL 标准中给出了四个不同级别的 isolation levels：\nSerializable: 最高级别，保证之前提到的 serializable execution. Repeatable read: 一个事务只能读取到其他事务 commit 后的值。而且，如果一个事务会对同一个 data item 读取多次，那么该事务完成最后一个读取之前，其他事务不能更改该 data item。 Read committed: 仅保证只能读取到其他事务 commit 后的值。 Read uncommitted: 未 commit 的数据也能读取。 四个等级的主要差距在 read 上，因为它们都保证没有 dirty write。即，如果一个 data item 已经被一个事务修改了，在该事务 commit 或 abort 之前，其他事务不能修改该 data item。\n一致性保障：Concurrency Control Protocols # 数据库的并行控制 (concurrency control) 子系统的作用，就是保证在多个事务运行的时候，可能出现的运行顺序 (schedules) 能够符合所指定的 isolation level 的要求。\nConcurrency control protocols 是一些并行控制规则，依据这些规则工作，我们就可以控制可能出现的并行事务运行顺序。常见的规则类型有 lock-based protocol, timestamp-based protocols, 以及 validation-based protocol。\nLock-based protocol 是通过给想要访问的 data item 上锁的方式来实现并行控制。这是一个比较通用的控制方式，在数据库以外的领域也被大量使用。 Timestamp-based protocol 则是通过记录每个 data item 的读、写 timestamp ，并以之与事务的 timestamp 进行比较的方式来进行并行控制。 Validation-based protocol 算是 timestamp-based protocol 的一个扩展。 在这里，我不对这些 protocol 的具体内容进行阐述。我们只需要知道一点，这些 protocol 的工作原理，不论是上锁，还是添加 timestamp，都有一个前提，就是目标 data item 是已存在的。这个隐性的前提看起来理所当然，但它就是幻读问题的根本所在。\n终于，幻读：Read repeatable level 下，会有幻读吗？ # 幻读 Phantom phenomenon # 幻读现象指的是，在一个事务中，多次运行同一个 query 所得到的结果不同。而这个结果，是其他事务添加或删除被读取的 tuple 而发生的。\n幻读在 read uncommitted，read committed，以及 read repeatable level 下都会出现。但是 read uncommitted 和 read committed 本身还有 dirty read 和 unrepeatable read 的现象。为了避免混淆，我们以 read repeatable (RR) 下的情况来做例子，这也是许多面试题的假定条件。\n要理解幻读出现的原因，先让我们再读一次文章开头的句子：\nThe phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.\n可见，幻读的出现是有具体的场景的。第一，是在做 predicate read 的时候；第二，对于前面读取操作的目标 data item，有与其冲突的 insert 或 update 发生。\n根据之前的内容我们已经知道，当多个并行事务对同一 data item 有相邻的读写访问时，这些读写访问操作之间是冲突的。这些冲突在 concurrency control protocols 的帮助下，是可以得到解决，使得数据库的一致性得到某种程度的保障。\n既然读写是冲突的，我们也有 concurrency control 的帮忙，为什么幻读还会出现呢？答案，就在前面提到的各个 concurrency control protocols 的工作方式上。 我们以 lock-based protocol 中的 two-phase lock 为例来看看到底是怎么一回事：\n当事务进行读取请求的时候，该事务会对将要访问的 tuple 加上共享锁，并维持到事务结束 (RR 条件下)。此时，如果有另外的事务想要修改被上锁的 tuple，需要获得该 tuple 的排他锁。所以，修改无法发生。没有问题。 但是，如果是不满足 predicate read 的限制范围的 tuple 被 update，因为该 tuple 没有被上锁，这个请求可以立即执行。假如 update 后的 tuple 满足了 predicate read 的限制条件。当我们再次做相同的读取请求时，就会有新的 tuple 被读取。 同理，insert 请求也可以直接运行。如果其他的事务新添加了满足限制范围的 tuple，那么再次运行相同的读取请求时，也会发现读取结果发生了变化。 可见，幻读出现的原因在于， 两个事务实际上有冲突，但因为冲突的对象是还不存在，concurrency control 无法对其访问进行限制，所以 unserializable schedules 得以出现。\n避免幻读的方法 # 幻读发生的原因是我们无法探知到发生在还未存在的数据上的请求冲突。那么，解决此问题的核心就是，将请求冲突转移到已存在的数据上。\n例如，可以将冲突转移到 table 的 metadata 上，或者转移到 index 上。即，将 table metadata 或 index 纳入 concurrency control 的管理范围。Serializable level 下不会有幻读问题，因为它同时只让一个事务访问 table，也就是将冲突转移到 table metadata。\n将冲突转移到 table metadata 的方式会大大降低并行度，我们大都不会采用。转移到 index 上是一个不错的选择， index-locking 就是其中著名的方式之一。\n根据 index concurrency control 的方式不同，我们可以有不同的方式来避免幻读。但其核心离不开“将请求冲突转移到已存在的数据上”，以保证只有 serializable schedules 能够出现。\n总结 # 数据库通过一定的 concurrency control protocols 来保证多个事务并行运行时结果的正确性。 幻读的发生，是因为基本的 concurrency control protocols 不能探知到发生在还未存在的数据对象上的访问冲突。 解决幻读的方法是，将对未存在的数据对象上的访问冲突，转移到已存在的数据上去。著名的方法有 index-locking，next-key locking 等。 ","date":"May 12 2022","externalUrl":null,"permalink":"/posts/isolation-levels-and-phtantom/","section":"","summary":"","title":"幻读，到底是怎么一回事儿","type":"posts"},{"content":"We are implementing a lock-based concurrency control scheme in this project. More specifically, the strict two phase locking protocol.\nThe concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.\nYou need to read the source code carefully. The instruction of this project is kind vague, or even wrong.\nTask 1 - Lock Manager # Read transaction.h, transaction_manager.h, and log_manager.h to learn the APIs first.\nThe log manager only communicates with transactions and the transaction manager. As the textbook says, the log manager tracks all the lock requests for different tuples; a transaction tracks all the locks it holds for different tuples.\nAll the works are around the management of the hash table in the log_manager and the two sets that track locks in transaction. We only need to implement the basic logic structure of each API in this task because we will modify them in the following tasks.\nTask 2 - Deadlock Prevention # We use wound-wait here, which means, when requesting a lock on a data item, the \u0026ldquo;younger\u0026rdquo; transactions wait for the \u0026ldquo;older\u0026rdquo; transactions, while the \u0026ldquo;older\u0026rdquo; transactions kill the \u0026ldquo;younger\u0026rdquo; transactions.\nBecause the log_manager and transaction track the lock requests for each tuple and each transaction, this part of work is\nTask 3 Concurrency Control # The instruction and code are inconsistent. The instruction asks us to maintain the write sets in transactions. However, the table write sets have already been handled by the APIs in table_heap.cpp. As a result, actually, we do not need to maintain the tuple write sets by ourself.\nEach transaction can execute several queries. We should consider this and modify our lock manager. For example, what should we do if a transaction inserts some tuples, then read them?\nTo achieve different isolation level with strict 2PL protocol, we need to use these locks properly in different executor. According to the lecture slides, we should:\nSerializable: Obtain all locks first; ;plus index locks, plus strict 2PL. Repeatable Reads: Same as above, but no index locks. Read Committed: Same as above, but share locks are released immediately. Read Uncommitted: Same as above but allows dirty reads (no share locks). Some exceptions that are thrown from the lock manager cannot be fetched by the test code. So, letting the caller of the lock manager to handle lock fails is a better choice. This costed me few hours to debug..\nResult # ","date":"May 9 2022","externalUrl":null,"permalink":"/posts/projects/cmu15445-project04/","section":"","summary":"","title":"CMU 15-445 2021 Fall Project#4","type":"posts"},{"content":"","date":"May 9 2022","externalUrl":null,"permalink":"/categories/projects/","section":"Categories","summary":"","title":"Projects","type":"categories"},{"content":"In this project, we will Implement executors for taking query plan nodes and executing them.\nWe are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling Next on its children to retrieve tuples and process them one-by-one.\nHow executor works # Before start coding, we need to learn a lot from the related source code first. The instruction does not show all the details and I believe they did this intentionally.\nAs discussed in the lecture, DBMSs will convert a SQL statement into a query plan, which is a tree consisted by operator nodes. The executors that we implement define how we process these operators.\nInside an operator, we get the expression of the operator. Thus, to implement an executor, we need to get the expression of that plan node, and evaluate these expression in the right way to get the result.\nIn this project, I recommend you to read the test code first before anything else. The test code tells us the structure of the code base and how they are combined together.\nThe plan node and execution engine are the most important components.\nExecutionEngine is defined in execution_engine.h and there is only one single API \u0026ndash; Execute(). In this function, it calls ExecuteFactory to create executor object and return a smart pointer to the caller. There is a SetUp() function in executor_test_util.h that does all the initializations for us. That\u0026rsquo;s why we can use GetExecutionEngine() and GetExecutorContext() in the test file directly. (I was very curious about this.) Different operators have their own plan node. The executor can fetch information it needs from the passed in plan node. Thus, do check all the members of the plan node. Sequential Scan # This task needs us to read a lot code to know how these components work. In short, what we need to do is read all the tuples from the TableHeap.\nOnce you know how to read a tuple from a given table, this task is almost done. However, do remember to use the output scheme to build the output tuple. Also, use the given predicate to filtrate out unsatisfied tuples.\nInsertion # In this part we need to do both:\nInsert new tuples into the table Insert new indexes for these new tuples There can have several tuples to be inserted in one query, and each table can have several indexes (based on different index keys).\nAll the components that we need to insert tuples and indexes can be find through the catalog of the table. More specifically:\nWe can get table information from the catalog. Through the table information, we can get the container of the table (TableHeap), which provides the APIs to modify the table. We can get index information from the catalog. Similarly, through the index information, we can get the container of the index (Index), which provides the APIs to modify the index. Because all the APIs are provided by the code base or ourself (i.e., the underlying buffer pool management and extendible hash index), there is no much code we need to write for insertion.\nUpdate # Update is very similar with insertion. As described in the project instruction, the APIs that create updated tuples has been provided, so the tuple update part is very simple.\nWe need to consider when and how to update the indexes. A hint for this is that the index of attributes in the table index is the same in the table, which means we can know if the index keys are updated.\nDelete # Delete itself is very straightforward. Just delete the tuple and related indexes.\nNested Loop Join # Need to read the test case code to learn how to build the joined tuples. More specifically, there are two Expressions that we need in this task, one is from the predicate, which is used for check if two tuples are matched; another is from the output schema columns, we need to use it to create the output tuples.\nHash Join # Hash join is more complicated than nested loop join. The good news (?) is that, we assume the hash table can fit in the memory, so the basic hash join algorithm is enough.\nThe basic hash join algorithm has two phases: build and probe. Before we check any tuple of the inner table, we need to build the hash table for the outer table first. This phase should be done at the beginning.\nThen, for each tuple of the inner table, we can use the hash table to find the matched tuples. We need to create a hash table by ourself. This is the most difficult part. As the instruction told us, we should check SimpleAggregationHashTable to learn how to create one for hash joining. You should also check aggregation_plan.h, which contains some components that SimpleAggregationHashTable uses. This part of work may need deeper knowledge about C++ than other tasks.\nWe also need to get the hash keys using the given expressions. (P.S. the instruction in the website may not be updated. There is no GetLeftJoinKey() and GetRightJoinKey() member functions in the code base that I am using.)\nAggregation # Since the hash table is given by the code base, we only need to use these expressions that the plan node gives to us. We only need to care about the GROUP BY and HAVING clauses. Aggregations are handled by the given code.\nDistinct # After we know how to build our own hash table through the exercise of hash join, this task becomes quite easy. We only need to create another hash table, and use it to get distinct tuples.\nResult # At the time of my submission, I was ranked first on the leaderboard. ^^\n","date":"April 30 2022","externalUrl":null,"permalink":"/posts/projects/cmu15445-project03/","section":"","summary":"","title":"CMU 15-445 2021 Fall Project#3","type":"posts"},{"content":"Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don\u0026rsquo;t think that would be very different from public the source code.\nTask #1 - Page Layouts # Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.\nWe have implemented the BufferPoolManager in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of Page is stored in the page frame? In this task, we need to create two kinds of Pages for our hash table:\nHash table directory page Hash table bucket page Since we are using the previous allocated memory space (we cast the data_ field of Page to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary.\nFor bitwise operations, because the bitmap is based on char arrays, we can only do bitwise operations char by char (at least, this is what I find).\nHash Table Directory Page # This kind of page stores metadata for the hash table. The most important part is the bucket address table.\nAt this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks.\nHash Bucket Page # Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs.\nThere are two bitmaps that we used to indicate if a slot contains valid KV:\nreadable_ occupied_ occupied_ actually has no specially meaning in extendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases.\nThe page layout itself is very straightforward. Only the bitwise operations are a little annoying.\nTask #2 - Hash Table Implementation # This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages.\nI recommend to implement the search at the very beginning then other functions, since other operations also need search to check if a specific KV pair is existed.\nSearch # For a given key, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key.\nBecause we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair.\nInsert # The core components of insertion part is split. Highly recommend to use pen and paper to figure how bucket split works before coding.\nThe insertion procedure is as follows:\nFind the right bucket If there is room in the bucket, insert the KV pair If there is no room -\u0026gt; split the bucket How to split one bucket? Assume we call the split target split bucket and the newly created bucket image bucket.\nIf $ global\\_depth == local\\_depth $:\nIncrease the global_depth by 1, so double the table size The following steps are same as situation $ global\\_depth \u0026gt; local\\_depth $ If $global\\_depth \u0026gt; local\\_depth$:\nAllocate a new page for the image bucket Adjust the entries in the bucket address table leave the half of the entries pointing to the split bucket set all the remaining entries to point to the image bucket also increase the local_depth by 1 because we need one more bit to separate them Rehash KV pairs in the split bucket Re-attemp the insertion Should use the Insert() function because we may need more splits Add your own test cases. The given test case is so small and cannot cover all situations.\nRemove # Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us.\nThe project description gives a fairly thorough instructions for merge. Follow the instruction is enough.\nShrinking the table is very straightforward since we use LSB to assign KV pairs to different buckets.\nTask #3 Concurrency Control # Try coarse-grained latch (lock) first, then reduce the latch range.\nNo special comments for this. You can do it!\nResult # ","date":"March 19 2022","externalUrl":null,"permalink":"/posts/projects/cmu15445_project2/","section":"","summary":"","title":"CMU 15-445 2021 Fall Project#2","type":"posts"},{"content":"CMU 15-445 是著名的数据库系统本科课程，针对数据库内核而不是数据库应用。我学习了 21 年度的课程，下面是课程左右的一些总结：\n","date":"March 19 2022","externalUrl":null,"permalink":"/posts/projects/","section":"","summary":"","title":"CMU 15-445 2021 Projects","type":"posts"},{"content":"Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don\u0026rsquo;t think that would be very different from public the source code.\nTask #1 - LRU Replacement Policy # BufferPoolManger contains all the frames. LRUReplacer is an implementation of the Replacer and it helps BufferPoolManger to manage these frames.\nThis LRU policy is not very \u0026ldquo;LRU\u0026rdquo; in my opinion. Refer the test cases we can see, if we Unpin the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation).\nYou may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how BufferPoolManager utlizes the LRUReplacer.\nActually, it is the BufferPoolMangerInstance managing the pages in the buffer. The LRUReplacer itself only contains page frames that we can use for storing new pages. In other words, the reference (pin) count of pages that existed in the frames that in the LRUReplacer is zero, and we can swap them out in anytime.\nSince we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the include/common/rwlatch.h to learn how Bustub (i.e., the DBMS that we are implementing) uses them.\nTask #2 - Buffer Pool Manager Instance # We use Page as the container to manage the pages of our DB storage engine. Page objects are pre-allocated for each frame in the buffer pool. We reuse existed Page objects instead of creating a new one for every newly read in pages.\nWe pin a page when we want to use it, and we unpin a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, pin and unpin are hints to tell the pool manager which page it can swap out if there is no free space.\nCaution, frame and page are refering to different concepts. page is a chunk of data that stored in our DBMS; frame is a slot in the page buffer that has the same size as the page. So, use frame_id_t and page_id_t at the right place.\nThe comments in the base code is not very clear. They use \u0026ldquo;page\u0026rdquo; to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding.\nBufferPoolManager uses four components to manage pages and frames:\npage_table_: a map that stores the mapping relationship between page_id and frame_id. free_list_: a linked-list that stores the free frames. replacer_: a LRUReplacer that stores used frames with zero pin count. pages_: stores pre-allocated Page objects. In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable.\nBufferPoolManager is the friend of Page, so we can access the private members of Page. (This is a good example about when to use friend \u0026ndash; when we need to change some member variables but we do not want give setters so that every one can change them.)\nIf we can do three things right, this task is not that difficult:\nMove page to/from LRU. Know when to flush a page. (Read points are very clear). Which page metadata we need to update. Critical hints:\nDo read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned false in a function, however, they assume we should return true in that case. Do not use your own judgement, just follow the description.) What will happen if we NewPage() then Unpin() the same page immediately? Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you. Task #3 - Parallel Buffer Pool Manager # Task 3 is very straightforward. If our BufferPoolManagerInstance is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance.\nSome people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us.\nResult # Passed all test cases with full grades.\n","date":"March 14 2022","externalUrl":null,"permalink":"/posts/projects/cmu15445_project1/","section":"","summary":"","title":"CMU 15-445 2021 Fall Project#1","type":"posts"},{"content":" Short Summary # This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore.\nThe main ideas of BlueStore are:\nAvoid using local file system to store and represent Ceph objects Use KV-store to provide transaction mechanism instead of build it by ourself What\u0026rsquo;s the problem # There is a software called storage backend in Ceph. The storage backend is responsible to accept requests from upper layer of Ceph and do the real I/O on storage devices.\nCeph used to use commonly used local file systems based storage backend (i.e., FileStore). FileStore works, but not that well. There are mainly three drawbacks in FileStore:\nHard to implement efficient transactions on top of existing file systems The local file system\u0026rsquo; metadata performance is not great (e.g., enumerating directories, ordering in the return result) Hard to adopt emerging storage hardware that abandon the venrable block interface (e.g., Zone divecies) Why the problem is interesting # Because storage backends do the real I/O job, the performance storage backends domains the performance of the whole Ceph system. For years, the developers of Ceph have had a lot of troubles when using local file systems to build storage backends The Core Ideas # The problems of FileStore # Transaction: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL\nleveraging file system\u0026rsquo;s transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces. User space WAL: has consistency problem (not atom operations, because it is a logical WAL, it use read/write system calls to write/read data to/from logging part). The cost for handle the problem is expensive. Also slow read-modify-write and double-write (logging all data) problems. Using KV-store: This is the cure. However, there is still some unnecessary file system overhead like journaling of journal problem Slow metadata operations: enumeration is slow. The read result from a object sets should in order, which file systems do not do. We need to do sorting after read. To reduce the sorting overhead, Ceph limits the number of files in a directory, which introduces directory splition. The dir splition has overhead.\nDoes not support new storage hardware: new storage devices may need modifications in the existing file systems. If Ceph uses local file systems, the Ceph team can only wait for the developers of the file systems to adopt the new storage.\nIn the paper, there are more details. In summary, the reasons of above problems are:\nFile system overhead Ceph uses file system metadata to represent Ceph object metadata. (i.e., object to file, object group to diectory) The file system metadata operations are not fast and also may have some consistent issues. BlueStore # Does not use local file system anymore. Instead, store Ceph objects into raw storage directly. This method avoids the overhead of file systems. Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata. This method provides much faster metadata operations and also avoid building transtion mechanism by Ceph developer. Because RocksDB runs on top of file systems. BlueStore has a very simply file system that only works for RocksDB called BlueFS. The BlueFS stores all the contents in logging space and cleans invalid data periodly. If you understand the reason of why FileStore performs not well, you can simply understand the choices they did when build BlueStore.\nBlueStore still has some issues. For example, because BlueStore do not use file systems, it cannot leverage the OS page cache and need to build the cache by itself. However, build a effective cache is hard.\n","date":"January 17 2021","externalUrl":null,"permalink":"/papers/1.aghayev-2019-filesystemunfit/","section":"Papers","summary":"","title":"SOSP'19 - File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","type":"papers"},{"content":" Short Summary # This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requested.\nThey reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive.\nThey also tried to increase the request size to exploit SSD internal parallelism.\nWhat Is the Problem # Search engines pose great challenges to storage systems:\nlow latency high data throughput high scalability The datasets become too large to fit into the RAM. Simply use RAM as a cache cannot achieve the goal.\nSSD and NVRAM can boost performance well. For example, flash-based SSDs provide much higher throughput and lower latency compared to HDD. However, since SSDs exhibit vastly different characteristic from HDDs, we need to evolve the software on top of the storage stack to exploit the full potential of SSDs.\nIn this paper, the authors rebuild a search engine to better utilize SSDs to achieve the necessary performance goals with main memory that is significantly smaller than the data set.\nWhy the Problem Is Interesting # There are already many studies that work on making SSD-friendly softwares within storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processing); and SFS, F2fS (file system).\nHowever, there is no optimization for full-text search engines. Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares.\nThe Idea # The key idea is: read as needed.\nThe reason behind of the idea is SSD can provide millisecond-level read latency, which is fast enough to avoid cache data into main memory.\nThere are three challenges:\nreduce read amplification hide I/O latency issue large requests to exploit SSD performance (This work is focus on the data structure, inverted index. This is a commonly used data structure in the information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.)\nCategory Techniques Reduce read amplification - cross-stage data grouping - two-way cost-aware bloom filters - trade disk space for I/O Hide I/O latency adaptive prefetching Issue large requests cross-stage data grouping Cross-stage data grouping # This technique is used to reduce read amplification and issue large requests.\nWiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query. Inverted index of WiSER places data of different stages in the order that it will be accessed.\nPrevious search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requests.\nTwo-way Cost-aware Filters # When doing phrase queries we need to use the position information to check the terms around a specific term to improve search precision.\nThe naive approach is to read the positions from all the terms in the phrase then iterate the position list. To reduce the unnecessary reading, WiSER employs a bitmap-based bloom filter. The reason to use bitmap is to reduce the size of bloom filter. There are many empty entries in the filter array, use bitmap can avoid the waste.\nCost-aware means comparing the size of position list with that of the bloom filters. If the size of position list is smaller than that of bloom filters, WiSER reads the position list directly.\nTwo-way filters shares the same idea. WiSER chooses to read the smaller bloom filter to reduce the read amplification.\nAdaptive Prefetching # Prefetching is one of the commonly used technique to hide the I/O latency. Even though, the read latency of SSD is small. Compare to DRAM, the read latency of SSD still much larger.\nPrevious search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e. Linux readahead) which increases the read amplification. WiSER defines an area called prefetch zone. A prefetch zone is further divided into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a query are larger than a threshold.\nTo enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calls madvise() with the MADV_SEQUENTIAL hint to readahead in the prefetch zone.\nTrade Disk Space for I/O # Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves.\nCompressing all data in the buffer together achieves better compression. However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries.\nIn the evaluation, WiSER uses 25% more storage than Elasticsearch. The authors argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER.\n","date":"December 26 2020","externalUrl":null,"permalink":"/papers/he-2020-readasneeded/","section":"Papers","summary":"","title":"FAST'20 - Read as Needed: Building WiSER, a Flash-Optimized Search Engine","type":"papers"},{"content":"","externalUrl":null,"permalink":"/archives/","section":"Gy's Blog","summary":"archives","title":"Archive","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/search/","section":"Gy's Blog","summary":"search","title":"Search","type":"page"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]