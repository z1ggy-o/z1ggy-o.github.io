<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Distributed-Systems on Gy's Blog</title><link>https://z1ggy-o.github.io/tags/distributed-systems/</link><description>Recent content in Distributed-Systems on Gy's Blog</description><generator>Hugo -- 0.140.2</generator><language>en-us</language><lastBuildDate>Sat, 28 Dec 2024 14:09:27 +0800</lastBuildDate><atom:link href="https://z1ggy-o.github.io/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml"/><item><title>TOCS'13 - Spanner: Google’s Globally Distributed Database</title><link>https://z1ggy-o.github.io/papers/2.c-2013-spanner/</link><pubDate>Sun, 31 Jul 2022 05:40:38 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/2.c-2013-spanner/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Some applications need relation data model and strong consistency which BigTable cannot gives.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Provide SQL-like interface, strong consistency, and high read performance.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Replication&lt;/p>
&lt;ul>
&lt;li>Use Paxos to replicate data to several nodes, which provides higher availability.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Local Transactions (within a paxos group)&lt;/p></description></item><item><title>SOSP'97 - Frangipani: a scalable distributed file system</title><link>https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/</link><pubDate>Fri, 01 Jul 2022 13:30:21 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>File system administration for a large, growing computer installation is a laborious task.&lt;/li>
&lt;li>A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>A real system that shows us how to do cache coherence and distributed transactions.&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.&lt;/li>
&lt;li>Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.&lt;/li>
&lt;li>All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.&lt;/li>
&lt;li>Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)&lt;/li>
&lt;li>Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.&lt;/li>
&lt;li>The underlying distributed storage and the lock server are both using Paxos to ensure availability.&lt;/li>
&lt;/ul>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;ul>
&lt;li>They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.&lt;/li>
&lt;li>They tested both the local performance and the scalability of the proposed system.&lt;/li>
&lt;/ul>
&lt;h2 id="the-main-finding-of-the-paper">The Main Finding of the Paper&lt;/h2>
&lt;ul>
&lt;li>Complex clients sharing simple storage can have better scalability.&lt;/li>
&lt;li>Distributed lock can be used to achieve cache coherence and distributed transaction.&lt;/li>
&lt;/ul></description></item><item><title>ATC'10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems</title><link>https://z1ggy-o.github.io/papers/hunt-2010-zookeeperwaitfreecoordination/</link><pubDate>Mon, 23 May 2022 09:00:39 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/hunt-2010-zookeeperwaitfreecoordination/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Large-scale distributed applications require different forms of coordination.&lt;/p>
&lt;p>Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.&lt;/p>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.&lt;/li>
&lt;li>Achieve high performance by relaxing consistency guarantees&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.&lt;/p></description></item><item><title>SOSP '97 - The design of a practical system for fault-tolerant virtual machines</title><link>https://z1ggy-o.github.io/papers/scales-2010-designpracticalsystem/</link><pubDate>Mon, 23 May 2022 09:00:39 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/scales-2010-designpracticalsystem/</guid><description>&lt;h2 id="motivation-1">MOTIVATION &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.&lt;/p>
&lt;h2 id="contribution">CONTRIBUTION&lt;/h2>
&lt;p>Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the &amp;ldquo;state-machine approach&amp;rdquo;, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.&lt;/p></description></item><item><title>SOSP'03 - The Google file system</title><link>https://z1ggy-o.github.io/papers/ghemawat-2003-googlefilesystem/</link><pubDate>Sun, 22 May 2022 11:11:56 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/ghemawat-2003-googlefilesystem/</guid><description>&lt;h2 id="motivation">MOTIVATION&lt;/h2>
&lt;p>Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).&lt;/p>
&lt;p>Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.&lt;/p>
&lt;h2 id="contribution">CONTRIBUTION&lt;/h2>
&lt;p>They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.&lt;/p></description></item><item><title>OSDI'04 - MapReduce: simplified data processing on large clusters</title><link>https://z1ggy-o.github.io/papers/dean-2004-mapreducesimplifieddata/</link><pubDate>Mon, 16 May 2022 09:44:18 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/dean-2004-mapreducesimplifieddata/</guid><description>&lt;h2 id="motivation">MOTIVATION&lt;/h2>
&lt;p>Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.&lt;/p>
&lt;p>To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.&lt;/p></description></item><item><title>2PC v.s. 3PC: 一句话的总结</title><link>https://z1ggy-o.github.io/posts/2pc-vs-3pc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/2pc-vs-3pc/</guid><description>&lt;p>最近在准备面试的过程中看到有这样一个问题，就是让比较一下 2PC 和 3PC。在网上找了一些文章来读，感觉都没有十分简洁地说明两者之间最基本的区别点。所以，在这里写一篇小文，表达一下自己对 2PC，3PC 最核心区别的理解。&lt;/p>
&lt;p>最重要的最先说，在我看来两者的核心区别在于：&lt;strong>参与者间是否对 transaction commit/abort 建立了共识&lt;/strong>。3PC 的参与者之间对 commit 的成立是具有共识的，2PC 则没有。&lt;/p>
&lt;h2 id="3pc-相比-2pc-带来了什么">3PC 相比 2PC 带来了什么？&lt;/h2>
&lt;p>大家都知道，3PC 相比于 2PC 来说多了两个东西：&lt;/p>
&lt;ol>
&lt;li>增加了 time out&lt;/li>
&lt;li>commit phase 被分割为了 prepare commit 和 do commit 两个部分&lt;/li>
&lt;/ol>
&lt;p>增加一个 prepare commit phase 带来了什么呢？是集群对 commit 这一决定的共识。&lt;/p>
&lt;p>在 2PC 协议中，协调者单方面向参与者发送一次 commit 消息。这个消息有两个含义：&lt;/p>
&lt;ol>
&lt;li>让参与者进行 commit&lt;/li>
&lt;li>所有参与者都认可此 commit&lt;/li>
&lt;/ol>
&lt;p>但需要注意的是，对于一个 transaction，是 commit 还是 abort 这个决定本身只有协调者知道。一旦协调者故障，这部分信息就消失了。所以我们说 2PC 的协调者是单点故障点。&lt;/p>
&lt;p>3PC 协议中，prepare commit 消息让一个 transaction 该 commit 还是 abort 这个决定本身被传导到了所有参与者处。如此一来，如果协调者故障，参与者们可以根据 prepare commit 的情况继续工作。&lt;/p></description></item></channel></rss>