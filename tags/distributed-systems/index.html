<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Distributed-Systems | Gy's Blog</title>
<meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://z1ggy-o.github.io/tags/distributed-systems/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://z1ggy-o.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://z1ggy-o.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://z1ggy-o.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://z1ggy-o.github.io/apple-touch-icon.png><link rel=mask-icon href=https://z1ggy-o.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://z1ggy-o.github.io/tags/distributed-systems/index.xml><link rel=alternate hreflang=en href=https://z1ggy-o.github.io/tags/distributed-systems/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://z1ggy-o.github.io/tags/distributed-systems/"><meta property="og:site_name" content="Gy's Blog"><meta property="og:title" content="Distributed-Systems"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Distributed-Systems"><meta name=twitter:description content></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://z1ggy-o.github.io/ accesskey=h title="Gy's Blog (Alt + H)">Gy's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://z1ggy-o.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://z1ggy-o.github.io/weekly/ title=Logs><span>Logs</span></a></li><li><a href=https://z1ggy-o.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://z1ggy-o.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://z1ggy-o.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://z1ggy-o.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://z1ggy-o.github.io/tags/>Tags</a></div><h1>Distributed-Systems</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>TOCS'13 - Spanner: Google’s Globally Distributed Database</h2></header><div class=entry-content><p>Motivation Some applications need relation data model and strong consistency which BigTable cannot gives.
So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.
Contribution Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.
Provide SQL-like interface, strong consistency, and high read performance.
Solution Replication
Use Paxos to replicate data to several nodes, which provides higher availability. Local Transactions (within a paxos group)
...</p></div><footer class=entry-footer><span title='2022-07-31 05:40:38 +0000 UTC'>July 31, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to TOCS'13 - Spanner: Google’s Globally Distributed Database" href=https://z1ggy-o.github.io/papers/2.c-2013-spanner/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>SOSP'97 - Frangipani: a scalable distributed file system</h2></header><div class=entry-content><p>Motivation File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution A real system that shows us how to do cache coherence and distributed transactions. Solution The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache. Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers. All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured. Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.) Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily. The underlying distributed storage and the lock server are both using Paxos to ensure availability. Evaluation They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c. They tested both the local performance and the scalability of the proposed system. The Main Finding of the Paper Complex clients sharing simple storage can have better scalability. Distributed lock can be used to achieve cache coherence and distributed transaction.</p></div><footer class=entry-footer><span title='2022-07-01 13:30:21 +0000 UTC'>July 1, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to SOSP'97 - Frangipani: a scalable distributed file system" href=https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>ATC'10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems</h2></header><div class=entry-content><p>Motivation Large-scale distributed applications require different forms of coordination.
Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.
Contribution Exposes APIs that enables application developers to implement their own primitives, without changes to the service core. Achieve high performance by relaxing consistency guarantees Solution ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.
...</p></div><footer class=entry-footer><span title='2022-05-23 09:00:39 +0000 UTC'>May 23, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to ATC'10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems" href=https://z1ggy-o.github.io/papers/hunt-2010-zookeeperwaitfreecoordination/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>SOSP '97 - The design of a practical system for fault-tolerant virtual machines</h2></header><div class=entry-content><p>MOTIVATION 1 A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.
CONTRIBUTION Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the “state-machine approach”, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.
...</p></div><footer class=entry-footer><span title='2022-05-23 09:00:39 +0000 UTC'>May 23, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to SOSP '97 - The design of a practical system for fault-tolerant virtual machines" href=https://z1ggy-o.github.io/papers/scales-2010-designpracticalsystem/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>SOSP'03 - The Google file system</h2></header><div class=entry-content><p>MOTIVATION Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).
Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.
CONTRIBUTION They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.
...</p></div><footer class=entry-footer><span title='2022-05-22 11:11:56 +0000 UTC'>May 22, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to SOSP'03 - The Google file system" href=https://z1ggy-o.github.io/papers/ghemawat-2003-googlefilesystem/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>OSDI'04 - MapReduce: simplified data processing on large clusters</h2></header><div class=entry-content><p>MOTIVATION Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.
To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.
...</p></div><footer class=entry-footer><span title='2022-05-16 09:44:18 +0000 UTC'>May 16, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</footer><a class=entry-link aria-label="post link to OSDI'04 - MapReduce: simplified data processing on large clusters" href=https://z1ggy-o.github.io/papers/dean-2004-mapreducesimplifieddata/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>2PC v.s. 3PC: 一句话的总结</h2></header><div class=entry-content><p>最近在准备面试的过程中看到有这样一个问题，就是让比较一下 2PC 和 3PC。在网上找了一些文章来读，感觉都没有十分简洁地说明两者之间最基本的区别点。所以，在这里写一篇小文，表达一下自己对 2PC，3PC 最核心区别的理解。
最重要的最先说，在我看来两者的核心区别在于：参与者间是否对 transaction commit/abort 建立了共识。3PC 的参与者之间对 commit 的成立是具有共识的，2PC 则没有。
3PC 相比 2PC 带来了什么？ 大家都知道，3PC 相比于 2PC 来说多了两个东西：
增加了 time out commit phase 被分割为了 prepare commit 和 do commit 两个部分 增加一个 prepare commit phase 带来了什么呢？是集群对 commit 这一决定的共识。
在 2PC 协议中，协调者单方面向参与者发送一次 commit 消息。这个消息有两个含义：
让参与者进行 commit 所有参与者都认可此 commit 但需要注意的是，对于一个 transaction，是 commit 还是 abort 这个决定本身只有协调者知道。一旦协调者故障，这部分信息就消失了。所以我们说 2PC 的协调者是单点故障点。
3PC 协议中，prepare commit 消息让一个 transaction 该 commit 还是 abort 这个决定本身被传导到了所有参与者处。如此一来，如果协调者故障，参与者们可以根据 prepare commit 的情况继续工作。
...</p></div><footer class=entry-footer>1 min&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 2PC v.s. 3PC: 一句话的总结" href=https://z1ggy-o.github.io/posts/2pc-vs-3pc/></a></article></main><footer class=footer><span>&copy; 2024 <a href=https://z1ggy-o.github.io/>Gy's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>