[{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don't think that would be very different from public the source code.\n Task #1 - LRU Replacement Policy BufferPoolManger contains all the frames. LRUReplacer is an implementation of the Replacer and it helps BufferPoolManger to manage these frames.\nThis LRU policy is not very \"LRU\" in my opinion. Refer the test cases we can see, if we Unpin the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation).\nYou may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how BufferPoolManager utlizes the LRUReplacer.\nActually, it is the BufferPoolMangerInstance managing the pages in the buffer. The LRUReplacer itself only contains page frames that we can use for storing new pages. In other words, the reference (pin) count of pages that existed in the frames that in the LRUReplacer is zero, and we can swap them out in anytime.\nSince we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the include/common/rwlatch.h to learn how Bustub (i.e., the DBMS that we are implementing) uses them.\nTask #2 - Buffer Pool Manager Instance We use Page as the container to manage the pages of our DB storage engine. Page objects are pre-allocated for each frame in the buffer pool. We reuse existed Page objects instead of creating a new one for every newly read in pages.\nWe pin a page when we want to use it, and we unpin a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, pin and unpin are hints to tell the pool manager which page it can swap out if there is no free space.\nCaution, frame and page are refering to different concepts. page is a chunk of data that stored in our DBMS; frame is a slot in the page buffer that has the same size as the page. So, use frame_id_t and page_id_t at the right place.\nThe comments in the base code is not very clear. They use \"page\" to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding.\nBufferPoolManager uses four components to manage pages and frames:\n page_table_: a map that stores the mapping relationship between page_id and frame_id. free_list_: a linked-list that stores the free frames. replacer_: a LRUReplacer that stores used frames with zero pin count. pages_: stores pre-allocated Page objects.  In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable.\nBufferPoolManager is the friend of Page, so we can access the private members of Page. (This is a good example about when to use friend -- when we need to change some member variables but we do not want give setters so that every one can change them.)\nIf we can do three things right, this task is not that difficult:\n Move page to/from LRU. Know when to flush a page. (Read points are very clear). Which page metadata we need to update.  Critical hints:\n Do read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned false in a function, however, they assume we should return true in that case. Do not use your own judgement, just follow the description.) What will happen if we NewPage() then Unpin() the same page immediately? Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you.  Task #3 - Parallel Buffer Pool Manager Task 3 is very straightforward. If our BufferPoolManagerInstance is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance.\nSome people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us.\nResult Passed all test cases with full grades.\n","description":"","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/posts/cmu15445_project1/"},{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don't think that would be very different from public the source code.\n Task #1 - Page Layouts Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.\nWe have implemented the BufferPoolManager in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of Page is stored in the page frame? In this task, we need to create two kinds of Pages for our hash table:\n Hash table directory page Hash table bucket page  Since we are using the previous allocated memory space (we cast the data_ field of Page to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary.\nFor bitwise operations, because the bitmap is based on char arrays, we can only do bitwise operations char by char (at least, this is what I find).\nHash Table Directory Page This kind of page stores metadata for the hash table. The most important part is the bucket address table.\nAt this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks.\nHash Bucket Page Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs.\nThere are two bitmaps that we used to indicate if a slot contains valid KV:\n readable_ occupied_  occupied_ actually has no specially meaning in extendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases.\nThe page layout itself is very straightforward. Only the bitwise operations are a little annoying.\nTask #2 - Hash Table Implementation This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages.\nI recommend to implement the search at the very beginning then other functions, since other operations also need search to check if a specific KV pair is existed.\nSearch For a given key, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key.\nBecause we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair.\nInsert The core components of insertion part is split. Highly recommend to use pen and paper to figure how bucket split works before coding.\nThe insertion procedure is as follows:\n Find the right bucket If there is room in the bucket, insert the KV pair If there is no room -\u003e split the bucket  How to split one bucket? Assume we call the split target split bucket and the newly created bucket image bucket.\n  If $ global\\_depth == local\\_depth $:\n Increase the global_depth by 1, so double the table size The following steps are same as situation $ global\\_depth \u003e local\\_depth $    If $global\\_depth \u003e local\\_depth$:\n Allocate a new page for the image bucket Adjust the entries in the bucket address table  leave the half of the entries pointing to the split bucket set all the remaining entries to point to the image bucket also increase the local_depth by 1 because we need one more bit to separate them   Rehash KV pairs in the split bucket Re-attemp the insertion  Should use the Insert() function because we may need more splits      Add your own test cases. The given test case is so small and cannot cover all situations.\nRemove Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us.\nThe project description gives a fairly thorough instructions for merge. Follow the instruction is enough.\nShrinking the table is very straightforward since we use LSB to assign KV pairs to different buckets.\nTask #3 Concurrency Control Try coarse-grained latch (lock) first, then reduce the latch range.\nNo special comments for this. You can do it!\nResult ","description":"","tags":["Database"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/posts/cmu15445_project2/"},{"categories":["paper notes"],"content":"Short Summary This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore.\nThe main ideas of BlueStore are:\n Avoid using local file system to store and represent Ceph objects Use KV-store to provide transaction mechanism instead of build it by ourself  What's the problem There is a software called storage backend in Ceph. The storage backend is responsible to accept requests from upper layer of Ceph and do the real I/O on storage devices.\nCeph used to use commonly used local file systems based storage backend (i.e., FileStore). FileStore works, but not that well. There are mainly three drawbacks in FileStore:\n Hard to implement efficient transactions on top of existing file systems The local file system' metadata performance is not great (e.g., enumerating directories, ordering in the return result) Hard to adopt emerging storage hardware that abandon the venrable block interface (e.g., Zone divecies)  Why the problem is interesting  Because storage backends do the real I/O job, the performance storage backends domains the performance of the whole Ceph system. For years, the developers of Ceph have had a lot of troubles when using local file systems to build storage backends  The Core Ideas The problems of FileStore   Transaction: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL\n leveraging file system's transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces. User space WAL: has consistency problem (not atom operations, because it is a logical WAL, it use read/write system calls to write/read data to/from logging part). The cost for handle the problem is expensive. Also slow read-modify-write and double-write (logging all data) problems. Using KV-store: This is the cure. However, there is still some unnecessary file system overhead like journaling of journal problem    Slow metadata operations: enumeration is slow. The read result from a object sets should in order, which file systems do not do. We need to do sorting after read. To reduce the sorting overhead, Ceph limits the number of files in a directory, which introduces directory splition. The dir splition has overhead.\n  Does not support new storage hardware: new storage devices may need modifications in the existing file systems. If Ceph uses local file systems, the Ceph team can only wait for the developers of the file systems to adopt the new storage.\n  In the paper, there are more details. In summary, the reasons of above problems are:\n File system overhead Ceph uses file system metadata to represent Ceph object metadata. (i.e., object to file, object group to diectory) The file system metadata operations are not fast and also may have some consistent issues.  BlueStore  Does not use local file system anymore. Instead, store Ceph objects into raw storage directly. This method avoids the overhead of file systems. Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata. This method provides much faster metadata operations and also avoid building transtion mechanism by Ceph developer. Because RocksDB runs on top of file systems. BlueStore has a very simply file system that only works for RocksDB called BlueFS. The BlueFS stores all the contents in logging space and cleans invalid data periodly.  If you understand the reason of why FileStore performs not well, you can simply understand the choices they did when build BlueStore.\nBlueStore still has some issues. For example, because BlueStore do not use file systems, it cannot leverage the OS page cache and need to build the cache by itself. However, build a effective cache is hard.\n","description":"","tags":null,"title":"File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/posts/aghayev-2019-filesystemunfit/"},{"categories":["paper notes"],"content":"Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.\nThe authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer.\nThis strategy can reduce SSD pressure which prolongs SSD life and still provide relative good performance.\nWhat is the problem In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.\nMore specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer requests first, so consumer requests can have shorter latency.\nHowever, in a real system, SSDs handle most of the write requests and HDDs are idle in more than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation. Also, deep queue depth makes requests suffering long latency even when we using SSDs.\nWhy the problem is interesting (important)? The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer. The request latency of HDD can be classified as three categories: fast, middle, and slow. Write requests data is put to the buffer first, then to the disk. When the buffer is full, HDD will block the coming requests until it flushes all the data in the buffer into disk. When there are free space in the buffer, request latency is in fast or middle range, otherwise in slow range.\nThe fast and middle latency is in $μ s$-level which similar with the performance of SSD. If we can control the buffer in disk to handle requests which their size is in the buffer size range, then we can get SSD-level performance when using HDD to handle small write requests.\nThe idea Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also provides reasonable performance.\nTo achieve the goal, there are two key components in this paper:\n Make sure requests to HDD are in the fast and middle latency range Determining which write requests should be dispatch to HDD  To handle the first challenge, the authors provided a prediction model. The model itself is simply comparing the current request size with pre-defined threshold. We cannot know the write buffer size of HDD directly. However, we can get an approximate value of the buffer size through profiling. The threshold are the cumulative amount of written data for the fast/mid/slow stages.\nSince we only want to use the fast and middle stages, we need to skip the slow stage. There are two methods to do this. First, sync system call from host can enforce the buffer flush; second, HDD controller will flush the buffer when the buffer is full. sync is a expensive operation, so the authors choose to use padding data to full fill the buffer, which can let controller to flush the data in the buffer.\nThe second reason of why we need padding data is we want to make sure the prediction model working well. That means the prediction model needs a sequential continuous write requests. When HDD is idle, the controller will empty the buffer even when the buffer is not full, which break the prediction. Read requests also break the prediction. Using padding data can help the system to maintain and adjust the prediction. More specifically, when HDD is idle, the system use small size padding data to avoid disk control flush the buffer; when read requests finished, since we cannot know if the disk controller flushes the buffer, the system use large size padding data to quickly full fill the buffer, which can help recorrect the prediction model. These padding data will be remove during the GC procedure.\nSteering requests to HDDs is much easier to understand. The latency of request is related to the I/O queue depth. We do profiling to find the relation between SSD's queue depth and the request request latency. In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs fast stage. We use the queue depth value as the threshold. When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.\nDrawbacks and personal questions about the study  Only works for small size of write requests The consistency is not guaranteed The disk cannot be managed as RAID (can we?) GC is still a problem  ","description":"","tags":null,"title":"BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/posts/wang-2020-bcw/"},{"categories":["paper notes"],"content":"Short Summary This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requested.\nThey reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive.\nThey also tried to increase the request size to exploit SSD internal parallelism.\nWhat Is the Problem Search engines pose great challenges to storage systems:\n low latency high data throughput high scalability  The datasets become too large to fit into the RAM. Simply use RAM as a cache cannot achieve the goal.\nSSD and NVRAM can boost performance well. For example, flash-based SSDs provide much higher throughput and lower latency compared to HDD. However, since SSDs exhibit vastly different characteristic from HDDs, we need to evolve the software on top of the storage stack to exploit the full potential of SSDs.\nIn this paper, the authors rebuild a search engine to better utilize SSDs to achieve the necessary performance goals with main memory that is significantly smaller than the data set.\nWhy the Problem Is Interesting There are already many studies that work on making SSD-friendly softwares within storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processing); and SFS, F2fS (file system).\nHowever, there is no optimization for full-text search engines. Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares.\nThe Idea The key idea is: read as needed.\nThe reason behind of the idea is SSD can provide millisecond-level read latency, which is fast enough to avoid cache data into main memory.\nThere are three challenges:\n reduce read amplification hide I/O latency issue large requests to exploit SSD performance  (This work is focus on the data structure, inverted index. This is a commonly used data structure in the information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.)\n   Category Techniques     Reduce read amplification - cross-stage data grouping    - two-way cost-aware bloom filters    - trade disk space for I/O   Hide I/O latency adaptive prefetching   Issue large requests cross-stage data grouping    Cross-stage data grouping This technique is used to reduce read amplification and issue large requests.\nWiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query. Inverted index of WiSER places data of different stages in the order that it will be accessed.\nPrevious search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requests.\nTwo-way Cost-aware Filters When doing phrase queries we need to use the position information to check the terms around a specific term to improve search precision.\nThe naive approach is to read the positions from all the terms in the phrase then iterate the position list. To reduce the unnecessary reading, WiSER employs a bitmap-based bloom filter. The reason to use bitmap is to reduce the size of bloom filter. There are many empty entries in the filter array, use bitmap can avoid the waste.\nCost-aware means comparing the size of position list with that of the bloom filters. If the size of position list is smaller than that of bloom filters, WiSER reads the position list directly.\nTwo-way filters shares the same idea. WiSER chooses to read the smaller bloom filter to reduce the read amplification.\nAdaptive Prefetching Prefetching is one of the commonly used technique to hide the I/O latency. Even though, the read latency of SSD is small. Compare to DRAM, the read latency of SSD still much larger.\nPrevious search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e. Linux readahead) which increases the read amplification. WiSER defines an area called prefetch zone. A prefetch zone is further divided into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a query are larger than a threshold.\nTo enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calls madvise() with the MADV_SEQUENTIAL hint to readahead in the prefetch zone.\nTrade Disk Space for I/O Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves.\nCompressing all data in the buffer together achieves better compression. However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries.\nIn the evaluation, WiSER uses 25% more storage than Elasticsearch. The authors argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER.\n","description":"","tags":null,"title":"Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/posts/he-2020-readasneeded/"},{"categories":null,"content":"I rebuilt my personal blog by using hugo.\nI have cleaned all my old blogs (not so much actually).\nI will write something related to my research reading or personal learning. Hope I can meet you soon.\n","description":"","tags":null,"title":"My First Post","uri":"/posts/my-first-post/"}]
