[{"categories":["paper notes"],"content":"MOTIVATION 1 A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.\nCONTRIBUTION Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the \"state-machine approach\", which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.\nThere are three challenges:\n correctly capturing all the input and non-determinism correctly applying the inputs and non-determinism to the backup low performance degradation  To handle these challenges, they implemented a fault-tolerant virtual machines in VMware vSphere 4.0. This system reduces the performance of real applications by less than 10%, and needs less than 20 Mb/s network bandwidth.\nSOLUTION  There is one primary VM and one backup VM on different hosts. All input goes to the primary. And the input is sent to the backup VM via logging channel (network). A VM has a broad set of inputs and some additional information is needed for non-deterministic operations. They use the VMware deterministic replay to records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file. They design a protocol to ensure the failover (i.e., switching to the backup) is transparent to the clients. The core idea is that before send an output to the external world, we must need to make sure the backup VM has received the log entry that produces the output. The failure detection is handled by UDP heartbeating messages and logging traffic. To avoid \"split-brain\" situation, they store a flag in the shared storage, so VMs can know if there is any other running primary.  LIMITATION The limitation is that the implementation only works for uni-processor machines because recording and replaying the execution of a multi-processor VM can lead to significant performance issues (accessing to shared memory is non-deterministic operation).\nMAIN TAKEAWAY It is helpful to distinguish between internal and external and internal events of the system. For an infrastructure, only external events can really affect other applications.\n  D. J. Scales, M. Nelson, and G. Venkitachalam, “The design of a practical system for fault-tolerant virtual machines,” SIGOPS Oper. Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010, doi: 10.1145/1899928.1899932. ↩︎\n   ","description":"","tags":["distributed-systems"],"title":"Note: The design of a practical system for fault-tolerant virtual machines","uri":"/posts/paper-notes/scales-2010-designpracticalsystem/"},{"categories":["paper notes"],"content":"MOTIVATION Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).\nBecause Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.\nCONTRIBUTION They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.\nThis system is developed only for Google’s own programs. As a result, GFS does not provide POSIX APIs. Programs are designed and implemented based on GFS, which simplifies the design of GFS.\nSOLUTION  GFS uses a single master multiple chunkservers architecture. The master maintains all file system metadata and the chunkservers handle the file data. The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state. Considering the characteristics of the workloads in Google (append only, sequential read), they decide to divide files into fixed-size chunks (64MB). Each chunk has a globally unique chunk handle assigned by the master, that’s how we can find a chunk of a specific file. The client gives file name and in file offset to the master. Then, the master will send back the corresponding chunk handle and the chunkservers that have that chunk. After that, clients will communicate with chunkservers directly. This approach avoids the single master becoming the bottleneck. To ensure high availability and also improve parallelism, each file chunk is replicated on multiple chunservers on different racks. The metadata in master is protected by the operation log, also this log is replicated on multiple machines. When write happens, the data mutation propagates along the chunkservers incrementally. As a result, the write becomes faster, but clients can read stale data.  EVALUATION  Micro-benchmarks on a small cluster with 16 chunkserver. Tested the read, write, and record append performance. Two real world clusters in Google. One for production, another one for research and development.  LIMITATION With the increasing volume of data, the single master design can no longer cope with the demands.\nMAIN TAKEAWAY There are times when we can discard generalization and design a dedicated system for a specific scenario, which leads to a more simply design.\n","description":"","tags":["distributed-systems"],"title":"Note: The Google file system","uri":"/posts/paper-notes/ghemawat-2003-googlefilesystem/"},{"categories":["projects"],"content":"In this lab, we implement a simplified MapReduce framework.\nThere are one coordinator process and multiple worker processes. The coordinator manages tasks and the worker pass the input data to the given user map and reduce function.\nThere is no pipeline, which means we will finish all map tasks first then start working on reduce tasks.\nThis lab is simply, if you are unfamiliar with Go as I am, then most of your time will spend on learning Go, not implementing MR.\nCoordinator Information to maintain:\n Idle tasks The state of each running task (pending tasks)  Things need to do:\n The coordinator is actually only a server that provides RPC methods for worker processes As a result, the coordinator is actually a \"remote\" shared resource. We only check task states when workers call the cooresponding methods  There are basically only two methods we need to provide:\n A query method:  Give a task to the worker (either map or reduce) Record the state of the task, include starting time (or deadline).   A finish method:  A worker use this method to ack the end of a task. Change process phases can also be handled here. If all tasks are finished, we can exit.    Worker Workers are very straightforward. They keep asking for a task from the coordinator. When the task is finished, worker send a ack to the coordinator.\nIf we do not care graceful exit, the worker process can exit if the call() function return false.\nMap Task  Get filename with a unique id Read the file and call mapf() Separate KV pairs into different groups (buckets) and store them into different files 3.1 Need get nReduce from the coordinator 3.2 Filename is created by task id and bucket id, e.g., \"im-taskid-bucketid\" or \"mr-X-Y\" as the instruction hints told us  Reduce Task  Read intermediate data from files that created by map tasks Sort the read in KV pairs (we can use map directly in this lab because data is fit in the memory) Call reduce for each distinct key Write the result out filename \"mr-out-n\", n is the task unique id  ","description":"","tags":["distributed-systems"],"title":"MIT 6.824 Lab1: MapReduce","uri":"/posts/projects/mit6.824-lab1/"},{"categories":["paper notes"],"content":"MOTIVATION Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.\nTo parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.\nCONTRIBUTION They implement a library that can provide a simple interface that enables automatic parallelization and distribution of large-scale computations. In addition, the library handles machine failures without interaction with programmers.\nSOLUTION  Inspired by the map and reduce primitives present in functional languages, they provide a restricted programming model that parallelizes the computation automatically (the computation must be deterministic). Both Map and Reduce functions are written by the user. The input data will be partitioned into M splits, and each Map task handles one of them. The intermediate output of the Map function is divided into R pieces. The Reduce task will read the intermediate output of the Map function and generate the final output files. For a cluster of machines, we have only one master machine, and the others are worker machines. The master machine assigns tasks (Map or Reduce) to workers and tracks the state of each task. The worker machines communicate with the master when work is finished or communicate with other workers to read data to process. Fault tolerance is handled by periodical communication and re-computation when a machine fails. Because the computation is deterministic, recomputing the same task is okay. Many optimizations are applied in their implementation. Check the paper for details.  EVALUATION They used a cluster that consisted of around 1800 machines. Each machine has only 4GB of memory, and two 160GB IDE disks with a gigabit Ethernet link.\nThey tasted grep and sort workloads with 10^{10} 100-byte records (around 1TB). The results are shown in the terms of data throughput with the timeline.\nMAIN FINDING OF THE PAPER It is useful to abstract a common pattern of certain computing tasks, and create an infrastructure to handle the common issues.\n","description":"","tags":["distributed-systems"],"title":"Note: MapReduce: simplified data processing on large clusters","uri":"/posts/paper-notes/dean-2004-mapreducesimplifieddata/"},{"categories":["articles"],"content":"幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个 isolation level 下，会不会出现幻读，以及其解决方法。\n网上有许多关于幻读的文章，但是在读完之后发现，大多数的说明都浮于表面，好像作者们自己也并没有弄清楚幻读的本质。在本文中，我想利用数据库的一些高层抽象概念，来阐述幻读的本质。虽然不涉及任何的具体实现，但相信你在了解到这些概念之后，可以很快地理解幻读，以及各种幻读的处理方法。\n幻读以及其解决方法核心都可以浓缩在一句话中，即:\n The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.\n 其中的关键词为: predicate read, insert or update, 和 nonserializable execution. 我们将以这些关键词为突破口，来理解幻读问题。\n并行可能性：Isolation 和 Conflict Serializability ACID 是数据库事务的重要特性。其中的 I，即 isolation，指的是“多个事务同时运行的时候，它们之间是相互孤立的”。\n上面对 isolation 的标准定义我个人并不喜欢，因为它太过于抽象。依照我个人的理解，更具体一点儿说，isolation 的目标是保证多个事务同时运行的时候，不会因 race condtion 而使得数据库的一致性(consistency) 被破坏。\n那么如何实现 isolation 呢？其关键就是并行事务运行的 serializability.\nSchedules and Serial Schedules 当多个事务同时运行的时候，因为任务调度是由操作系统在负责，这些事务所包含的操作会交错运行。这些操作的一个实际运行顺序被称为一个 schedule。可见，对于一组同时运行的事务，它们可能出现的 schedules 是非常多的 (有 n 个事务的话，会有 $n!$ 种可能)。\n依据我们对 race condition 的理解，对于同一个 data item，如果并行访问中有写操作参与，就会有 race condition 的出现。那么，对于一组并行事务来说，如果其中有两个或以上的事务对同一个 data item 进行访问，且其中包含写操作，我们就说这些访问操作之间是会有冲突的 (conflict)。我们需要对其进行运行顺序进行控制，否则，最终的结果是不可控的，可能会破坏数据库的一致性。\nConflict Serializability 想要控制事务运行顺序，最简单的方式就是顺序运行。即，一个事务运行完成之后，才运行另一个事务。这种运行顺序，就是 serial schedules.\nSerial schedules 虽然保证了数据库的一致性，但事务的并行也随之消失了。为了提高系统的性能，我们还是想要事务在不破坏一致性的前提下并行工作。如果一些 schedules，它能够支持并行，且能保证其运行结果和 serial schedules 的结果相同，我们就叫它 serializable schedules。\n我们之前提到过，在一个 schedule 中分属于不同事务的两个连续操作，如果它们访问同一个 data item，且其中有一个是写操作，那么它们之间就是冲突的。即，在这个 scheule 中，它们两者的运行顺序是不能改变的。但，如果不冲突，这两个操作的顺序就可以交换。\n通过交换一个 schedule 中不冲突的连续操作，我们可以得到新的 schedule。如果一个 schedule，通过不断的交换各个事务间不冲突的操作，能得到一个 serial schedule，我们就说这个 schedule 具有 conflict serializability ，或者说它是 conflict serializable 的。\n无疑，conflict serializable 的事务运行顺序，就是我们想要的运行顺序，因为在允许事务并行运行的同时，它的运行结果和事务依次顺序运行时的结果一致。\n需要追加说明的是，serializability 不止 conflict serializability 一种。例如，还有 view serializability。但因为实现的难度等原因，几乎所有的数据库都是使用 conflict serializability。\n一致性弱化：Isolation Levels Serializability 固然可以保证数据库在事务并行时的一致性，但因为它的限制较多，使得系统的整体并行性能受到了压制。\n有些时候，我们为了性能，根据实际的应用场景，可以牺牲一些对一致性的要求。这就是 isolation levels 的意义所在。SQL 标准中给出了四个不同级别的 isolation levels：\n Serializable: 最高级别，保证之前提到的 serializable execution. Repeatable read: 一个事务只能读取到其他事务 commit 后的值。而且，如果一个事务会对同一个 data item 读取多次，那么该事务完成最后一个读取之前，其他事务不能更改该 data item。 Read committed: 仅保证只能读取到其他事务 commit 后的值。 Read uncommitted: 未 commit 的数据也能读取。  四个等级的主要差距在 read 上，因为它们都保证没有 dirty write。即，如果一个 data item 已经被一个事务修改了，在该事务 commit 或 abort 之前，其他事务不能修改该 data item。\n一致性保障：Concurrency Control Protocols 数据库的并行控制 (concurrency control) 子系统的作用，就是保证在多个事务运行的时候，可能出现的运行顺序 (schedules) 能够符合所指定的 isolation level 的要求。\nConcurrency control protocols 是一些并行控制规则，依据这些规则工作，我们就可以控制可能出现的并行事务运行顺序。常见的规则类型有 lock-based protocol, timestamp-based protocols, 以及 validation-based protocol。\n Lock-based protocol 是通过给想要访问的 data item 上锁的方式来实现并行控制。这是一个比较通用的控制方式，在数据库以外的领域也被大量使用。 Timestamp-based protocol 则是通过记录每个 data item 的读、写 timestamp ，并以之与事务的 timestamp 进行比较的方式来进行并行控制。 Validation-based protocol 算是 timestamp-based protocol 的一个扩展。  在这里，我不对这些 protocol 的具体内容进行阐述。我们只需要知道一点，这些 protocol 的工作原理，不论是上锁，还是添加 timestamp，都有一个前提，就是目标 data item 是已存在的。这个隐性的前提看起来理所当然，但它就是幻读问题的根本所在。\n终于，幻读：Read repeatable level 下，会有幻读吗？ 幻读 Phantom phenomenon 幻读现象指的是，在一个事务中，多次运行同一个 query 所得到的结果不同。而这个结果，是其他事务添加或删除被读取的 tuple 而发生的。\n幻读在 read uncommitted，read committed，以及 read repeatable level 下都会出现。但是 read uncommitted 和 read committed 本身还有 dirty read 和 unrepeatable read 的现象。为了避免混淆，我们以 read repeatable (RR) 下的情况来做例子，这也是许多面试题的假定条件。\n要理解幻读出现的原因，先让我们再读一次文章开头的句子：\n The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.\n 可见，幻读的出现是有具体的场景的。第一，是在做 predicate read 的时候；第二，对于前面读取操作的目标 data item，有与其冲突的 insert 或 update 发生。\n根据之前的内容我们已经知道，当多个并行事务对同一 data item 有相邻的读写访问时，这些读写访问操作之间是冲突的。这些冲突在 concurrency control protocols 的帮助下，是可以得到解决，使得数据库的一致性得到某种程度的保障。\n既然读写是冲突的，我们也有 concurrency control 的帮忙，为什么幻读还会出现呢？答案，就在前面提到的各个 concurrency control protocols 的工作方式上。 我们以 lock-based protocol 中的 two-phase lock 为例来看看到底是怎么一回事：\n 当事务进行读取请求的时候，该事务会对将要访问的 tuple 加上共享锁，并维持到事务结束 (RR 条件下)。此时，如果有另外的事务想要修改被上锁的 tuple，需要获得该 tuple 的排他锁。所以，修改无法发生。没有问题。 但是，如果是不满足 predicate read 的限制范围的 tuple 被 update，因为该 tuple 没有被上锁，这个请求可以立即执行。假如 update 后的 tuple 满足了 predicate read 的限制条件。当我们再次做相同的读取请求时，就会有新的 tuple 被读取。 同理，insert 请求也可以直接运行。如果其他的事务新添加了满足限制范围的 tuple，那么再次运行相同的读取请求时，也会发现读取结果发生了变化。  可见，幻读出现的原因在于， 两个事务实际上有冲突，但因为冲突的对象是还不存在，concurrency control 无法对其访问进行限制，所以 unserializable schedules 得以出现。\n避免幻读的方法 幻读发生的原因是我们无法探知到发生在还未存在的数据上的请求冲突。那么，解决此问题的核心就是，将请求冲突转移到已存在的数据上。\n例如，可以将冲突转移到 table 的 metadata 上，或者转移到 index 上。即，将 table metadata 或 index 纳入 concurrency control 的管理范围。Serializable level 下不会有幻读问题，因为它同时只让一个事务访问 table，也就是将冲突转移到 table metadata。\n将冲突转移到 table metadata 的方式会大大降低并行度，我们大都不会采用。转移到 index 上是一个不错的选择， index-locking 就是其中著名的方式之一。\n根据 index concurrency control 的方式不同，我们可以有不同的方式来避免幻读。但其核心离不开“将请求冲突转移到已存在的数据上”，以保证只有 serializable schedules 能够出现。\n总结  数据库通过一定的 concurrency control protocols 来保证多个事务并行运行时结果的正确性。 幻读的发生，是因为基本的 concurrency control protocols 不能探知到发生在还未存在的数据对象上的访问冲突。 解决幻读的方法是，将对未存在的数据对象上的访问冲突，转移到已存在的数据上去。著名的方法有 index-locking，next-key locking 等。  ","description":"","tags":["DBMS"],"title":"幻读，到底是怎么一回事儿","uri":"/posts/articles/isolation-levels-and-phtantom/"},{"categories":["projects"],"content":"We are implementing a lock-based concurrency control scheme in this project. More specifically, the strict two phase locking protocol.\nThe concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.\nYou need to read the source code carefully. The instruction of this project is kind vague, or even wrong.\nTask 1 - Lock Manager Read transaction.h, transaction_manager.h, and log_manager.h to learn the APIs first.\nThe log manager only communicates with transactions and the transaction manager. As the textbook says, the log manager tracks all the lock requests for different tuples; a transaction tracks all the locks it holds for different tuples.\nAll the works are around the management of the hash table in the log_manager and the two sets that track locks in transaction. We only need to implement the basic logic structure of each API in this task because we will modify them in the following tasks.\nTask 2 - Deadlock Prevention We use wound-wait here, which means, when requesting a lock on a data item, the \"younger\" transactions wait for the \"older\" transactions, while the \"older\" transactions kill the \"younger\" transactions.\nBecause the log_manager and transaction track the lock requests for each tuple and each transaction, this part of work is\nTask 3 Concurrency Control The instruction and code are inconsistent. The instruction asks us to maintain the write sets in transactions. However, the table write sets have already been handled by the APIs in table_heap.cpp. As a result, actually, we do not need to maintain the tuple write sets by ourself.\nEach transaction can execute several queries. We should consider this and modify our lock manager. For example, what should we do if a transaction inserts some tuples, then read them?\nTo achieve different isolation level with strict 2PL protocol, we need to use these locks properly in different executor. According to the lecture slides, we should:\n Serializable: Obtain all locks first; ;plus index locks, plus strict 2PL. Repeatable Reads: Same as above, but no index locks. Read Committed: Same as above, but share locks are released immediately. Read Uncommitted: Same as above but allows dirty reads (no share locks).  Some exceptions that are thrown from the lock manager cannot be fetched by the test code. So, letting the caller of the lock manager to handle lock fails is a better choice. This costed me few hours to debug..\nResult ","description":"","tags":["DBMS"],"title":"CMU 15-445 2021 Fall Project#4","uri":"/posts/projects/cmu15445-project04/"},{"categories":["projects"],"content":"In this project, we will Implement executors for taking query plan nodes and executing them.\nWe are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling Next on its children to retrieve tuples and process them one-by-one.\nHow executor works Before start coding, we need to learn a lot from the related source code first. The instruction does not show all the details and I believe they did this intentionally.\nAs discussed in the lecture, DBMSs will convert a SQL statement into a query plan, which is a tree consisted by operator nodes. The executors that we implement define how we process these operators.\nInside an operator, we get the expression of the operator. Thus, to implement an executor, we need to get the expression of that plan node, and evaluate these expression in the right way to get the result.\nIn this project, I recommend you to read the test code first before anything else. The test code tells us the structure of the code base and how they are combined together.\nThe plan node and execution engine are the most important components.\n ExecutionEngine is defined in execution_engine.h and there is only one single API -- Execute(). In this function, it calls ExecuteFactory to create executor object and return a smart pointer to the caller. There is a SetUp() function in executor_test_util.h that does all the initializations for us. That's why we can use GetExecutionEngine() and GetExecutorContext() in the test file directly. (I was very curious about this.) Different operators have their own plan node. The executor can fetch information it needs from the passed in plan node. Thus, do check all the members of the plan node.  Sequential Scan This task needs us to read a lot code to know how these components work. In short, what we need to do is read all the tuples from the TableHeap.\nOnce you know how to read a tuple from a given table, this task is almost done. However, do remember to use the output scheme to build the output tuple. Also, use the given predicate to filtrate out unsatisfied tuples.\nInsertion In this part we need to do both:\n Insert new tuples into the table Insert new indexes for these new tuples  There can have several tuples to be inserted in one query, and each table can have several indexes (based on different index keys).\nAll the components that we need to insert tuples and indexes can be find through the catalog of the table. More specifically:\n We can get table information from the catalog. Through the table information, we can get the container of the table (TableHeap), which provides the APIs to modify the table. We can get index information from the catalog. Similarly, through the index information, we can get the container of the index (Index), which provides the APIs to modify the index.  Because all the APIs are provided by the code base or ourself (i.e., the underlying buffer pool management and extendible hash index), there is no much code we need to write for insertion.\nUpdate Update is very similar with insertion. As described in the project instruction, the APIs that create updated tuples has been provided, so the tuple update part is very simple.\nWe need to consider when and how to update the indexes. A hint for this is that the index of attributes in the table index is the same in the table, which means we can know if the index keys are updated.\nDelete Delete itself is very straightforward. Just delete the tuple and related indexes.\nNested Loop Join Need to read the test case code to learn how to build the joined tuples. More specifically, there are two Expressions that we need in this task, one is from the predicate, which is used for check if two tuples are matched; another is from the output schema columns, we need to use it to create the output tuples.\nHash Join Hash join is more complicated than nested loop join. The good news (?) is that, we assume the hash table can fit in the memory, so the basic hash join algorithm is enough.\nThe basic hash join algorithm has two phases: build and probe. Before we check any tuple of the inner table, we need to build the hash table for the outer table first. This phase should be done at the beginning.\nThen, for each tuple of the inner table, we can use the hash table to find the matched tuples. We need to create a hash table by ourself. This is the most difficult part. As the instruction told us, we should check SimpleAggregationHashTable to learn how to create one for hash joining. You should also check aggregation_plan.h, which contains some components that SimpleAggregationHashTable uses. This part of work may need deeper knowledge about C++ than other tasks.\nWe also need to get the hash keys using the given expressions. (P.S. the instruction in the website may not be updated. There is no GetLeftJoinKey() and GetRightJoinKey() member functions in the code base that I am using.)\nAggregation Since the hash table is given by the code base, we only need to use these expressions that the plan node gives to us. We only need to care about the GROUP BY and HAVING clauses. Aggregations are handled by the given code.\nDistinct After we know how to build our own hash table through the exercise of hash join, this task becomes quite easy. We only need to create another hash table, and use it to get distinct tuples.\nResult At the time of my submission, I was ranked first on the leaderboard. ^^\n","description":"","tags":["DBMS"],"title":"CMU 15-445 2021 Fall Project#3","uri":"/posts/projects/cmu15445-project03/"},{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don't think that would be very different from public the source code.\n Task #1 - Page Layouts Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.\nWe have implemented the BufferPoolManager in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of Page is stored in the page frame? In this task, we need to create two kinds of Pages for our hash table:\n Hash table directory page Hash table bucket page  Since we are using the previous allocated memory space (we cast the data_ field of Page to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary.\nFor bitwise operations, because the bitmap is based on char arrays, we can only do bitwise operations char by char (at least, this is what I find).\nHash Table Directory Page This kind of page stores metadata for the hash table. The most important part is the bucket address table.\nAt this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks.\nHash Bucket Page Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs.\nThere are two bitmaps that we used to indicate if a slot contains valid KV:\n readable_ occupied_  occupied_ actually has no specially meaning in extendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases.\nThe page layout itself is very straightforward. Only the bitwise operations are a little annoying.\nTask #2 - Hash Table Implementation This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages.\nI recommend to implement the search at the very beginning then other functions, since other operations also need search to check if a specific KV pair is existed.\nSearch For a given key, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key.\nBecause we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair.\nInsert The core components of insertion part is split. Highly recommend to use pen and paper to figure how bucket split works before coding.\nThe insertion procedure is as follows:\n Find the right bucket If there is room in the bucket, insert the KV pair If there is no room -\u003e split the bucket  How to split one bucket? Assume we call the split target split bucket and the newly created bucket image bucket.\n  If $ global\\_depth == local\\_depth $:\n Increase the global_depth by 1, so double the table size The following steps are same as situation $ global\\_depth \u003e local\\_depth $    If $global\\_depth \u003e local\\_depth$:\n Allocate a new page for the image bucket Adjust the entries in the bucket address table  leave the half of the entries pointing to the split bucket set all the remaining entries to point to the image bucket also increase the local_depth by 1 because we need one more bit to separate them   Rehash KV pairs in the split bucket Re-attemp the insertion  Should use the Insert() function because we may need more splits      Add your own test cases. The given test case is so small and cannot cover all situations.\nRemove Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us.\nThe project description gives a fairly thorough instructions for merge. Follow the instruction is enough.\nShrinking the table is very straightforward since we use LSB to assign KV pairs to different buckets.\nTask #3 Concurrency Control Try coarse-grained latch (lock) first, then reduce the latch range.\nNo special comments for this. You can do it!\nResult ","description":"","tags":["DBMS"],"title":"CMU 15-445 2021 Fall Project#2","uri":"/posts/projects/cmu15445_project2/"},{"categories":["projects"],"content":" Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don't think that would be very different from public the source code.\n Task #1 - LRU Replacement Policy BufferPoolManger contains all the frames. LRUReplacer is an implementation of the Replacer and it helps BufferPoolManger to manage these frames.\nThis LRU policy is not very \"LRU\" in my opinion. Refer the test cases we can see, if we Unpin the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation).\nYou may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how BufferPoolManager utlizes the LRUReplacer.\nActually, it is the BufferPoolMangerInstance managing the pages in the buffer. The LRUReplacer itself only contains page frames that we can use for storing new pages. In other words, the reference (pin) count of pages that existed in the frames that in the LRUReplacer is zero, and we can swap them out in anytime.\nSince we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the include/common/rwlatch.h to learn how Bustub (i.e., the DBMS that we are implementing) uses them.\nTask #2 - Buffer Pool Manager Instance We use Page as the container to manage the pages of our DB storage engine. Page objects are pre-allocated for each frame in the buffer pool. We reuse existed Page objects instead of creating a new one for every newly read in pages.\nWe pin a page when we want to use it, and we unpin a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, pin and unpin are hints to tell the pool manager which page it can swap out if there is no free space.\nCaution, frame and page are refering to different concepts. page is a chunk of data that stored in our DBMS; frame is a slot in the page buffer that has the same size as the page. So, use frame_id_t and page_id_t at the right place.\nThe comments in the base code is not very clear. They use \"page\" to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding.\nBufferPoolManager uses four components to manage pages and frames:\n page_table_: a map that stores the mapping relationship between page_id and frame_id. free_list_: a linked-list that stores the free frames. replacer_: a LRUReplacer that stores used frames with zero pin count. pages_: stores pre-allocated Page objects.  In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable.\nBufferPoolManager is the friend of Page, so we can access the private members of Page. (This is a good example about when to use friend -- when we need to change some member variables but we do not want give setters so that every one can change them.)\nIf we can do three things right, this task is not that difficult:\n Move page to/from LRU. Know when to flush a page. (Read points are very clear). Which page metadata we need to update.  Critical hints:\n Do read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned false in a function, however, they assume we should return true in that case. Do not use your own judgement, just follow the description.) What will happen if we NewPage() then Unpin() the same page immediately? Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you.  Task #3 - Parallel Buffer Pool Manager Task 3 is very straightforward. If our BufferPoolManagerInstance is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance.\nSome people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us.\nResult Passed all test cases with full grades.\n","description":"","tags":["DBMS"],"title":"CMU 15-445 2021 Fall Project#1","uri":"/posts/projects/cmu15445_project1/"},{"categories":["paper notes"],"content":"Short Summary This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore.\nThe main ideas of BlueStore are:\n Avoid using local file system to store and represent Ceph objects Use KV-store to provide transaction mechanism instead of build it by ourself  What's the problem There is a software called storage backend in Ceph. The storage backend is responsible to accept requests from upper layer of Ceph and do the real I/O on storage devices.\nCeph used to use commonly used local file systems based storage backend (i.e., FileStore). FileStore works, but not that well. There are mainly three drawbacks in FileStore:\n Hard to implement efficient transactions on top of existing file systems The local file system' metadata performance is not great (e.g., enumerating directories, ordering in the return result) Hard to adopt emerging storage hardware that abandon the venrable block interface (e.g., Zone divecies)  Why the problem is interesting  Because storage backends do the real I/O job, the performance storage backends domains the performance of the whole Ceph system. For years, the developers of Ceph have had a lot of troubles when using local file systems to build storage backends  The Core Ideas The problems of FileStore   Transaction: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL\n leveraging file system's transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces. User space WAL: has consistency problem (not atom operations, because it is a logical WAL, it use read/write system calls to write/read data to/from logging part). The cost for handle the problem is expensive. Also slow read-modify-write and double-write (logging all data) problems. Using KV-store: This is the cure. However, there is still some unnecessary file system overhead like journaling of journal problem    Slow metadata operations: enumeration is slow. The read result from a object sets should in order, which file systems do not do. We need to do sorting after read. To reduce the sorting overhead, Ceph limits the number of files in a directory, which introduces directory splition. The dir splition has overhead.\n  Does not support new storage hardware: new storage devices may need modifications in the existing file systems. If Ceph uses local file systems, the Ceph team can only wait for the developers of the file systems to adopt the new storage.\n  In the paper, there are more details. In summary, the reasons of above problems are:\n File system overhead Ceph uses file system metadata to represent Ceph object metadata. (i.e., object to file, object group to diectory) The file system metadata operations are not fast and also may have some consistent issues.  BlueStore  Does not use local file system anymore. Instead, store Ceph objects into raw storage directly. This method avoids the overhead of file systems. Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata. This method provides much faster metadata operations and also avoid building transtion mechanism by Ceph developer. Because RocksDB runs on top of file systems. BlueStore has a very simply file system that only works for RocksDB called BlueFS. The BlueFS stores all the contents in logging space and cleans invalid data periodly.  If you understand the reason of why FileStore performs not well, you can simply understand the choices they did when build BlueStore.\nBlueStore still has some issues. For example, because BlueStore do not use file systems, it cannot leverage the OS page cache and need to build the cache by itself. However, build a effective cache is hard.\n","description":"","tags":null,"title":"Note: File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution","uri":"/posts/paper-notes/aghayev-2019-filesystemunfit/"},{"categories":["paper notes"],"content":"Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.\nThe authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer.\nThis strategy can reduce SSD pressure which prolongs SSD life and still provide relative good performance.\nWhat is the problem In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.\nMore specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer requests first, so consumer requests can have shorter latency.\nHowever, in a real system, SSDs handle most of the write requests and HDDs are idle in more than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation. Also, deep queue depth makes requests suffering long latency even when we using SSDs.\nWhy the problem is interesting (important)? The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer. The request latency of HDD can be classified as three categories: fast, middle, and slow. Write requests data is put to the buffer first, then to the disk. When the buffer is full, HDD will block the coming requests until it flushes all the data in the buffer into disk. When there are free space in the buffer, request latency is in fast or middle range, otherwise in slow range.\nThe fast and middle latency is in $μ s$-level which similar with the performance of SSD. If we can control the buffer in disk to handle requests which their size is in the buffer size range, then we can get SSD-level performance when using HDD to handle small write requests.\nThe idea Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also provides reasonable performance.\nTo achieve the goal, there are two key components in this paper:\n Make sure requests to HDD are in the fast and middle latency range Determining which write requests should be dispatch to HDD  To handle the first challenge, the authors provided a prediction model. The model itself is simply comparing the current request size with pre-defined threshold. We cannot know the write buffer size of HDD directly. However, we can get an approximate value of the buffer size through profiling. The threshold are the cumulative amount of written data for the fast/mid/slow stages.\nSince we only want to use the fast and middle stages, we need to skip the slow stage. There are two methods to do this. First, sync system call from host can enforce the buffer flush; second, HDD controller will flush the buffer when the buffer is full. sync is a expensive operation, so the authors choose to use padding data to full fill the buffer, which can let controller to flush the data in the buffer.\nThe second reason of why we need padding data is we want to make sure the prediction model working well. That means the prediction model needs a sequential continuous write requests. When HDD is idle, the controller will empty the buffer even when the buffer is not full, which break the prediction. Read requests also break the prediction. Using padding data can help the system to maintain and adjust the prediction. More specifically, when HDD is idle, the system use small size padding data to avoid disk control flush the buffer; when read requests finished, since we cannot know if the disk controller flushes the buffer, the system use large size padding data to quickly full fill the buffer, which can help recorrect the prediction model. These padding data will be remove during the GC procedure.\nSteering requests to HDDs is much easier to understand. The latency of request is related to the I/O queue depth. We do profiling to find the relation between SSD's queue depth and the request request latency. In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs fast stage. We use the queue depth value as the threshold. When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.\nDrawbacks and personal questions about the study  Only works for small size of write requests The consistency is not guaranteed The disk cannot be managed as RAID (can we?) GC is still a problem  ","description":"","tags":null,"title":"Note: BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","uri":"/posts/paper-notes/wang-2020-bcw/"},{"categories":["paper notes"],"content":"Short Summary This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requested.\nThey reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive.\nThey also tried to increase the request size to exploit SSD internal parallelism.\nWhat Is the Problem Search engines pose great challenges to storage systems:\n low latency high data throughput high scalability  The datasets become too large to fit into the RAM. Simply use RAM as a cache cannot achieve the goal.\nSSD and NVRAM can boost performance well. For example, flash-based SSDs provide much higher throughput and lower latency compared to HDD. However, since SSDs exhibit vastly different characteristic from HDDs, we need to evolve the software on top of the storage stack to exploit the full potential of SSDs.\nIn this paper, the authors rebuild a search engine to better utilize SSDs to achieve the necessary performance goals with main memory that is significantly smaller than the data set.\nWhy the Problem Is Interesting There are already many studies that work on making SSD-friendly softwares within storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processing); and SFS, F2fS (file system).\nHowever, there is no optimization for full-text search engines. Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares.\nThe Idea The key idea is: read as needed.\nThe reason behind of the idea is SSD can provide millisecond-level read latency, which is fast enough to avoid cache data into main memory.\nThere are three challenges:\n reduce read amplification hide I/O latency issue large requests to exploit SSD performance  (This work is focus on the data structure, inverted index. This is a commonly used data structure in the information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.)\n   Category Techniques     Reduce read amplification - cross-stage data grouping    - two-way cost-aware bloom filters    - trade disk space for I/O   Hide I/O latency adaptive prefetching   Issue large requests cross-stage data grouping    Cross-stage data grouping This technique is used to reduce read amplification and issue large requests.\nWiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query. Inverted index of WiSER places data of different stages in the order that it will be accessed.\nPrevious search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requests.\nTwo-way Cost-aware Filters When doing phrase queries we need to use the position information to check the terms around a specific term to improve search precision.\nThe naive approach is to read the positions from all the terms in the phrase then iterate the position list. To reduce the unnecessary reading, WiSER employs a bitmap-based bloom filter. The reason to use bitmap is to reduce the size of bloom filter. There are many empty entries in the filter array, use bitmap can avoid the waste.\nCost-aware means comparing the size of position list with that of the bloom filters. If the size of position list is smaller than that of bloom filters, WiSER reads the position list directly.\nTwo-way filters shares the same idea. WiSER chooses to read the smaller bloom filter to reduce the read amplification.\nAdaptive Prefetching Prefetching is one of the commonly used technique to hide the I/O latency. Even though, the read latency of SSD is small. Compare to DRAM, the read latency of SSD still much larger.\nPrevious search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e. Linux readahead) which increases the read amplification. WiSER defines an area called prefetch zone. A prefetch zone is further divided into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a query are larger than a threshold.\nTo enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calls madvise() with the MADV_SEQUENTIAL hint to readahead in the prefetch zone.\nTrade Disk Space for I/O Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves.\nCompressing all data in the buffer together achieves better compression. However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries.\nIn the evaluation, WiSER uses 25% more storage than Elasticsearch. The authors argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER.\n","description":"","tags":null,"title":"Note: Read as Needed: Building WiSER, a Flash-Optimized Search Engine","uri":"/posts/paper-notes/he-2020-readasneeded/"},{"categories":null,"content":"I rebuilt my personal blog by using hugo.\nI have cleaned all my old blogs (not so much actually).\nI will write something related to my research reading or personal learning. Hope I can meet you soon.\n","description":"","tags":null,"title":"My First Post","uri":"/posts/articles/my-first-post/"}]
