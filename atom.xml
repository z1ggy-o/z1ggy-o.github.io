<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US">
    <title type="text">Ziggy&#39;s Blog</title>
    <subtitle type="html">This is the personal blog of Ziggy, which mostly focuses on CS and personal life.</subtitle>
    <updated>2022-07-01T20:34:20&#43;08:00</updated>
    <id>https://z1ggy-o.github.io/</id>
    <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/" />
    <link rel="self" type="application/atom&#43;xml" href="https://z1ggy-o.github.io/atom.xml" />
    <author>
            <name>Ziggy</name>
            <uri>https://z1ggy-o.github.io/</uri>
            
                <email>gy.zhu29@outlook.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    <generator uri="https://gohugo.io/" version="0.98.0">Hugo</generator>
        <entry>
            <title type="text">VLDB&#39;14 - Staring into the abyss: An evaluation of concurrency control with one thousand cores</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/yu-2014-staringabyssevaluation/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/yu-2014-staringabyssevaluation/</id>
            <updated>2022-07-01T20:33:27&#43;08:00</updated>
            <published>2022-07-01T20:21:38&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.
To understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.&lt;/p&gt;
&lt;p&gt;To understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.&lt;/p&gt;
&lt;p&gt;This paper focus on concurrency control schemes.
(spoiler:  the [following research]([[@An empirical evaluation of in-memory multi-version concurrency control]]) of [[Andrew Pavlo]] found out concurrency control schemes are not the most important component that affect the scalability of main memory DBMS on many-core environment )&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of the scalability of seven commonly used concurrency control schemes (OLTP, in-memory DBMS)&lt;/li&gt;
&lt;li&gt;Evaluation is processed on a simulated machine with 1000 cores (actually is a cluster of 22 real machines)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is evaluation paper, thus there is no solutions but evaluation analyses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All concurrency control schemes fail to scale to a large number of cores. The bottlenecks are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;lock thrashing&lt;/li&gt;
&lt;li&gt;preemptive aborts&lt;/li&gt;
&lt;li&gt;deadlocks&lt;/li&gt;
&lt;li&gt;timestamp allocation&lt;/li&gt;
&lt;li&gt;memory-to-memory copying&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lock thrashing happens in any waiting-based algorithm. Using the &amp;quot;non-waiting&amp;quot; can alleviate this problem, however, we will have more aborting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In high contention workloads, a non-waiting deadlock prevention scheme is better than deadlock detection. (Restart is fast in main memory DBMS)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory allocation (includes timestamp allocation) is usually managed by a centric data structure, which becomes the bottleneck. Avoiding shared, centric data structure is important to achieve higher scalability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add new hardware to offload some tasks (e.g., memory copying) from CPU is a feasible way to improve the scalability&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The evaluation platform is based on a CPU simulator, Graphite (from MIT).&lt;/li&gt;
&lt;li&gt;The authors implemented a lightweight main memory DBMS only for this testing.&lt;/li&gt;
&lt;li&gt;Workloads are YCSB and TPC-C&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;Main finding of the paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Like other systems, to improve the scalability, how to avoid shared data is the key.&lt;/li&gt;
&lt;li&gt;&amp;quot;Measure, Then Build&amp;quot;, and a thorough measurement is always needed. This paper is good enough on analyzing concurrency control schemes, however, the following research revels that this is not the most important part...&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">ATC&#39;10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/hunt-2010-zookeeperwaitfreecoordination/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/hunt-2010-zookeeperwaitfreecoordination/</id>
            <updated>2022-07-01T20:33:27&#43;08:00</updated>
            <published>2022-05-23T17:00:39&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation Large-scale distributed applications require different forms of coordination.
Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.
Contribution  Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Large-scale distributed applications require different forms of coordination.&lt;/p&gt;
&lt;p&gt;Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.&lt;/li&gt;
&lt;li&gt;Achieve high performance by relaxing consistency guarantees&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;p&gt;ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.&lt;/p&gt;
&lt;p&gt;ZooKeeper provides a set of simplified APIs to access these data nodes. Application develops can use these APIs to implement different forms of coordination that they want.&lt;/p&gt;
&lt;p&gt;ZooKeeper servers use an atomic broadcast protocol, Zab (similar to Raft) to sync state between each other.&lt;/p&gt;
&lt;p&gt;It is vague about how to implement ZooKeeper. However, since it is an open source project, the source code is always available.&lt;/p&gt;
&lt;p&gt;In short, ZooKeeper guarantees correct coordination with high performance through:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using wait-free data objects instead of blocking.&lt;/li&gt;
&lt;li&gt;FIFO client ordering of all operations.&lt;/li&gt;
&lt;li&gt;Linearizing all writes to the leader, then propagate to other servers (they call this A-linearizability).&lt;/li&gt;
&lt;li&gt;Read locally. Can have stale data, however, much faster.&lt;/li&gt;
&lt;li&gt;Asynchronous enables batching to reduce networking and storage I/O overhead.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;Main finding of the paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Abstraction of basic building components can help create more general systems.&lt;/li&gt;
&lt;li&gt;ZooKeeper is one of the best examples that shows us how to weak consistency in favor of higher performance.&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: The design of a practical system for fault-tolerant virtual machines</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-23T17:00:39&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION 1 A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation-1&#34;&gt;MOTIVATION &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the &amp;quot;state-machine approach&amp;quot;, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.&lt;/p&gt;
&lt;p&gt;There are three challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;correctly capturing all the input and non-determinism&lt;/li&gt;
&lt;li&gt;correctly applying the inputs and non-determinism to the backup&lt;/li&gt;
&lt;li&gt;low performance degradation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To handle these challenges, they implemented a fault-tolerant virtual machines in VMware vSphere 4.0. This system reduces the performance of real applications by less than 10%, and needs less than 20 Mb/s network bandwidth.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There is one primary VM and one backup VM on different hosts. All input goes to the primary. And the input is sent to the backup VM via logging channel (network).&lt;/li&gt;
&lt;li&gt;A VM has a broad set of inputs and some additional information is needed for non-deterministic operations. They use the &lt;em&gt;VMware deterministic replay&lt;/em&gt; to records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file.&lt;/li&gt;
&lt;li&gt;They design a protocol to ensure the failover (i.e., switching to the backup) is transparent to the clients. The core idea is that before send an output to the external world, we must need to make sure the backup VM has received the log entry that produces the output.&lt;/li&gt;
&lt;li&gt;The failure detection is handled by UDP heartbeating messages and  logging traffic.&lt;/li&gt;
&lt;li&gt;To avoid &amp;quot;split-brain&amp;quot; situation, they store a flag in the shared storage, so VMs can know if there is any other running primary.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitation&#34;&gt;LIMITATION&lt;/h2&gt;
&lt;p&gt;The limitation is that the implementation only works for uni-processor machines because recording and replaying the execution of a multi-processor VM can lead to significant performance issues (accessing to shared memory is non-deterministic operation).&lt;/p&gt;
&lt;h2 id=&#34;main-takeaway&#34;&gt;MAIN TAKEAWAY&lt;/h2&gt;
&lt;p&gt;It is helpful to distinguish between internal and external and internal events of the system. For an infrastructure, only external events can really affect other applications.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;D. J. Scales, M. Nelson, and G. Venkitachalam, “The design of a practical system for fault-tolerant virtual machines,” SIGOPS Oper. Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010, doi: 10.1145/1899928.1899932.&amp;#160;&lt;a href=&#34;https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: The Google file system</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/ghemawat-2003-googlefilesystem/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/ghemawat-2003-googlefilesystem/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-22T19:11:56&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).
Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;MOTIVATION&lt;/h2&gt;
&lt;p&gt;Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).&lt;/p&gt;
&lt;p&gt;Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.&lt;/p&gt;
&lt;p&gt;This system is developed only for Google’s own programs. As a result, GFS does not provide POSIX APIs. Programs are designed and implemented based on GFS, which simplifies the design of GFS.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GFS uses a single master multiple chunkservers architecture. The master maintains all file system metadata and the chunkservers handle the file data. The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.&lt;/li&gt;
&lt;li&gt;Considering the characteristics of the workloads in Google (append only, sequential read), they decide to divide files into fixed-size chunks (64MB). Each chunk has a globally unique chunk handle assigned by the master, that’s how we can find a chunk of a specific file.&lt;/li&gt;
&lt;li&gt;The client gives file name and in file offset to the master. Then, the master will send back the corresponding chunk handle and the chunkservers that have that chunk. After that, clients will communicate with chunkservers directly. This approach avoids the single master becoming the bottleneck.&lt;/li&gt;
&lt;li&gt;To ensure high availability and also improve parallelism, each file chunk is replicated on multiple chunservers on different racks. The metadata in master is protected by the operation log, also this log is replicated on multiple machines.&lt;/li&gt;
&lt;li&gt;When write happens, the data mutation propagates along the chunkservers incrementally. As a result, the write becomes faster, but clients can read stale data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;EVALUATION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Micro-benchmarks on a small cluster with 16 chunkserver. Tested the read, write, and record append performance.&lt;/li&gt;
&lt;li&gt;Two real world clusters in Google. One for production, another one for research and development.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitation&#34;&gt;LIMITATION&lt;/h2&gt;
&lt;p&gt;With the increasing volume of data, the single master design can no longer cope with the demands.&lt;/p&gt;
&lt;h2 id=&#34;main-takeaway&#34;&gt;MAIN TAKEAWAY&lt;/h2&gt;
&lt;p&gt;There are times when we can discard generalization and design a dedicated system for a specific scenario, which leads to a more simply design.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">MIT 6.824 Lab1: MapReduce</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/mit6.824-lab1/" />
            <id>https://z1ggy-o.github.io/posts/projects/mit6.824-lab1/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-21T13:37:58&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">In this lab, we implement a simplified MapReduce framework.
There are one coordinator process and multiple worker processes. The coordinator manages tasks and the worker pass the input data to the given user map and reduce function.
There is no pipeline, which means we will finish all map tasks first then start working on reduce tasks.</summary>
            
                <content type="html">&lt;p&gt;In this lab, we implement a simplified MapReduce framework.&lt;/p&gt;
&lt;p&gt;There are one coordinator process and multiple worker processes. The coordinator manages tasks and the worker pass the input data to the given user map and reduce function.&lt;/p&gt;
&lt;p&gt;There is no pipeline, which means we will finish all map tasks first then start working on reduce tasks.&lt;/p&gt;
&lt;p&gt;This lab is simply, if you are unfamiliar with Go as I am, then most of your time will spend on learning Go, not implementing MR.&lt;/p&gt;
&lt;h2 id=&#34;coordinator&#34;&gt;Coordinator&lt;/h2&gt;
&lt;p&gt;Information to maintain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Idle tasks&lt;/li&gt;
&lt;li&gt;The state of each running task (pending tasks)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Things need to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coordinator is actually only a server that provides RPC methods for worker processes&lt;/li&gt;
&lt;li&gt;As a result, the coordinator is actually a &amp;quot;remote&amp;quot; shared resource. We only check task states when workers call the cooresponding methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are basically only two methods we need to provide:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A query method:
&lt;ul&gt;
&lt;li&gt;Give a task to the worker (either map or reduce)&lt;/li&gt;
&lt;li&gt;Record the state of the task, include starting time (or deadline).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A finish method:
&lt;ul&gt;
&lt;li&gt;A worker use this method to ack the end of a task.&lt;/li&gt;
&lt;li&gt;Change process phases can also be handled here.&lt;/li&gt;
&lt;li&gt;If all tasks are finished, we can exit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;worker&#34;&gt;Worker&lt;/h2&gt;
&lt;p&gt;Workers are very straightforward. They keep asking for a task from the coordinator. When the task is finished, worker send a ack to the coordinator.&lt;/p&gt;
&lt;p&gt;If we do not care graceful exit, the worker process can exit if the &lt;code&gt;call()&lt;/code&gt; function return false.&lt;/p&gt;
&lt;h3 id=&#34;map-task&#34;&gt;Map Task&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Get filename with a unique id&lt;/li&gt;
&lt;li&gt;Read the file and call &lt;code&gt;mapf()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Separate KV pairs into different groups (buckets) and store them into different files
3.1 Need get nReduce from the coordinator
3.2 Filename is created by task id and bucket id, e.g., &amp;quot;im-taskid-bucketid&amp;quot; or &amp;quot;mr-X-Y&amp;quot; as the instruction hints told us&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;reduce-task&#34;&gt;Reduce Task&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Read intermediate data from files that created by map tasks&lt;/li&gt;
&lt;li&gt;Sort the read in KV pairs (we can use map directly in this lab because data is fit in the memory)&lt;/li&gt;
&lt;li&gt;Call reduce for each distinct key&lt;/li&gt;
&lt;li&gt;Write the result out filename &amp;quot;mr-out-n&amp;quot;, n is the task unique id&lt;/li&gt;
&lt;/ol&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: MapReduce: simplified data processing on large clusters</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/dean-2004-mapreducesimplifieddata/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/dean-2004-mapreducesimplifieddata/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-16T17:44:18&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.
To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;MOTIVATION&lt;/h2&gt;
&lt;p&gt;Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;They implement a library that can provide a simple interface that enables automatic parallelization and distribution of large-scale computations. In addition, the library handles machine failures without interaction with programmers.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; primitives present in functional languages, they provide a restricted programming model that parallelizes the computation automatically (the computation must be deterministic).&lt;/li&gt;
&lt;li&gt;Both Map and Reduce functions are written by the user. The input data will be partitioned into M splits, and each Map task handles one of them. The intermediate output of the Map function is divided into R pieces. The Reduce task will read the intermediate output of the Map function and generate the final output files.&lt;/li&gt;
&lt;li&gt;For a cluster of machines, we have only one master machine, and the others are worker machines. The master machine assigns tasks (Map or Reduce) to workers and tracks the state of each task. The worker machines communicate with the master when work is finished or communicate with other workers to read data to process.&lt;/li&gt;
&lt;li&gt;Fault tolerance is handled by periodical communication and re-computation when a machine fails. Because the computation is deterministic, recomputing the same task is okay.&lt;/li&gt;
&lt;li&gt;Many optimizations are applied in their implementation. Check the paper for details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;EVALUATION&lt;/h2&gt;
&lt;p&gt;They used a cluster that consisted of around 1800 machines. Each machine has only 4GB of memory, and two 160GB IDE disks with a gigabit Ethernet link.&lt;/p&gt;
&lt;p&gt;They tasted grep and sort workloads with 10^{10} 100-byte records (around 1TB). The results are shown in the terms of data throughput with the timeline.&lt;/p&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;MAIN FINDING OF THE PAPER&lt;/h2&gt;
&lt;p&gt;It is useful to abstract a common pattern of certain computing tasks, and create an infrastructure to handle the common issues.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">幻读，到底是怎么一回事儿</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/isolation-levels-and-phtantom/" />
            <id>https://z1ggy-o.github.io/posts/articles/isolation-levels-and-phtantom/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-12T15:11:54&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个…</summary>
            
                <content type="html">&lt;p&gt;幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个 isolation level 下，会不会出现幻读，以及其解决方法。&lt;/p&gt;
&lt;p&gt;网上有许多关于幻读的文章，但是在读完之后发现，大多数的说明都浮于表面，好像作者们自己也并没有弄清楚幻读的本质。在本文中，我想利用数据库的一些高层抽象概念，来阐述幻读的本质。虽然不涉及任何的具体实现，但相信你在了解到这些概念之后，可以很快地理解幻读，以及各种幻读的处理方法。&lt;/p&gt;
&lt;p&gt;幻读以及其解决方法核心都可以浓缩在一句话中，即:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中的关键词为: &lt;em&gt;predicate read&lt;/em&gt;, &lt;em&gt;insert or update&lt;/em&gt;, 和 &lt;em&gt;nonserializable execution&lt;/em&gt;. 我们将以这些关键词为突破口，来理解幻读问题。&lt;/p&gt;
&lt;h2 id=&#34;并行可能性isolation-和-conflict-serializability&#34;&gt;并行可能性：Isolation 和 Conflict Serializability&lt;/h2&gt;
&lt;p&gt;ACID 是数据库事务的重要特性。其中的 I，即 isolation，指的是“多个事务同时运行的时候，它们之间是相互孤立的”。&lt;/p&gt;
&lt;p&gt;上面对 isolation 的标准定义我个人并不喜欢，因为它太过于抽象。依照我个人的理解，更具体一点儿说，isolation 的目标是保证多个事务同时运行的时候，不会因 race condtion 而使得数据库的一致性(consistency) 被破坏。&lt;/p&gt;
&lt;p&gt;那么如何实现 isolation 呢？其关键就是并行事务运行的 serializability.&lt;/p&gt;
&lt;h3 id=&#34;schedules-and-serial-schedules&#34;&gt;Schedules and Serial Schedules&lt;/h3&gt;
&lt;p&gt;当多个事务同时运行的时候，因为任务调度是由操作系统在负责，这些事务所包含的操作会交错运行。这些操作的一个实际运行顺序被称为一个 &lt;em&gt;schedule&lt;/em&gt;。可见，对于一组同时运行的事务，它们可能出现的 schedules 是非常多的 (有 n 个事务的话，会有 $n!$ 种可能)。&lt;/p&gt;
&lt;p&gt;依据我们对 race condition 的理解，对于同一个 data item，如果并行访问中有写操作参与，就会有 race condition 的出现。那么，对于一组并行事务来说，如果其中有两个或以上的事务对同一个 data item 进行访问，且其中包含写操作，我们就说这些访问操作之间是会有冲突的 (conflict)。我们需要对其进行运行顺序进行控制，否则，最终的结果是不可控的，可能会破坏数据库的一致性。&lt;/p&gt;
&lt;h3 id=&#34;conflict-serializability&#34;&gt;Conflict Serializability&lt;/h3&gt;
&lt;p&gt;想要控制事务运行顺序，最简单的方式就是顺序运行。即，一个事务运行完成之后，才运行另一个事务。这种运行顺序，就是 &lt;em&gt;serial schedules&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Serial schedules 虽然保证了数据库的一致性，但事务的并行也随之消失了。为了提高系统的性能，我们还是想要事务在不破坏一致性的前提下并行工作。如果一些 schedules，它能够支持并行，且能保证其运行结果和 serial schedules 的结果相同，我们就叫它 &lt;em&gt;serializable schedules&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;我们之前提到过，在一个 schedule 中分属于不同事务的两个连续操作，如果它们访问同一个 data item，且其中有一个是写操作，那么它们之间就是冲突的。即，在这个 scheule 中，它们两者的运行顺序是不能改变的。但，如果不冲突，这两个操作的顺序就可以交换。&lt;/p&gt;
&lt;p&gt;通过交换一个 schedule 中不冲突的连续操作，我们可以得到新的 schedule。如果一个 schedule，通过不断的交换各个事务间不冲突的操作，能得到一个 serial schedule，我们就说这个 schedule 具有 &lt;em&gt;conflict serializability&lt;/em&gt; ，或者说它是 &lt;em&gt;conflict serializable&lt;/em&gt; 的。&lt;/p&gt;
&lt;p&gt;无疑，conflict serializable 的事务运行顺序，就是我们想要的运行顺序，因为在允许事务并行运行的同时，它的运行结果和事务依次顺序运行时的结果一致。&lt;/p&gt;
&lt;p&gt;需要追加说明的是，serializability 不止 conflict serializability 一种。例如，还有 view serializability。但因为实现的难度等原因，几乎所有的数据库都是使用 conflict serializability。&lt;/p&gt;
&lt;h2 id=&#34;一致性弱化isolation-levels&#34;&gt;一致性弱化：Isolation Levels&lt;/h2&gt;
&lt;p&gt;Serializability 固然可以保证数据库在事务并行时的一致性，但因为它的限制较多，使得系统的整体并行性能受到了压制。&lt;/p&gt;
&lt;p&gt;有些时候，我们为了性能，根据实际的应用场景，可以牺牲一些对一致性的要求。这就是 isolation levels 的意义所在。SQL 标准中给出了四个不同级别的 isolation levels：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Serializable&lt;/strong&gt;: 最高级别，保证之前提到的 serializable execution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeatable read&lt;/strong&gt;: 一个事务只能读取到其他事务 commit 后的值。而且，如果一个事务会对同一个 data item 读取多次，那么该事务完成最后一个读取之前，其他事务不能更改该 data item。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read committed&lt;/strong&gt;: 仅保证只能读取到其他事务 commit 后的值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read uncommitted&lt;/strong&gt;: 未 commit 的数据也能读取。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;四个等级的主要差距在 read 上，因为它们都保证没有 dirty write。即，如果一个 data item 已经被一个事务修改了，在该事务 commit 或 abort 之前，其他事务不能修改该 data item。&lt;/p&gt;
&lt;h2 id=&#34;一致性保障concurrency-control-protocols&#34;&gt;一致性保障：Concurrency Control Protocols&lt;/h2&gt;
&lt;p&gt;数据库的并行控制 (concurrency control) 子系统的作用，就是保证在多个事务运行的时候，可能出现的运行顺序 (schedules) 能够符合所指定的 isolation level 的要求。&lt;/p&gt;
&lt;p&gt;Concurrency control protocols 是一些并行控制规则，依据这些规则工作，我们就可以控制可能出现的并行事务运行顺序。常见的规则类型有 &lt;em&gt;lock-based protocol&lt;/em&gt;, &lt;em&gt;timestamp-based protocols&lt;/em&gt;, 以及 &lt;em&gt;validation-based protocol&lt;/em&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lock-based protocol 是通过给想要访问的 data item 上锁的方式来实现并行控制。这是一个比较通用的控制方式，在数据库以外的领域也被大量使用。&lt;/li&gt;
&lt;li&gt;Timestamp-based protocol 则是通过记录每个 data item 的读、写 timestamp ，并以之与事务的 timestamp 进行比较的方式来进行并行控制。&lt;/li&gt;
&lt;li&gt;Validation-based protocol 算是 timestamp-based protocol 的一个扩展。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这里，我不对这些 protocol 的具体内容进行阐述。我们只需要知道一点，这些 protocol 的工作原理，不论是上锁，还是添加 timestamp，都有一个前提，就是&lt;strong&gt;目标 data item 是已存在的&lt;/strong&gt;。这个隐性的前提看起来理所当然，但它就是幻读问题的根本所在。&lt;/p&gt;
&lt;h2 id=&#34;终于幻读read-repeatable-level-下会有幻读吗&#34;&gt;终于，幻读：Read repeatable level 下，会有幻读吗？&lt;/h2&gt;
&lt;h3 id=&#34;幻读-phantom-phenomenon&#34;&gt;幻读 Phantom phenomenon&lt;/h3&gt;
&lt;p&gt;幻读现象指的是，在一个事务中，多次运行同一个 query 所得到的结果不同。而这个结果，是其他事务添加或删除被读取的 tuple 而发生的。&lt;/p&gt;
&lt;p&gt;幻读在 read uncommitted，read committed，以及 read repeatable level 下都会出现。但是 read uncommitted 和 read committed 本身还有 dirty read 和 unrepeatable read 的现象。为了避免混淆，我们以 read repeatable (RR) 下的情况来做例子，这也是许多面试题的假定条件。&lt;/p&gt;
&lt;p&gt;要理解幻读出现的原因，先让我们再读一次文章开头的句子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可见，幻读的出现是有具体的场景的。第一，是在做 &lt;strong&gt;predicate read&lt;/strong&gt; 的时候；第二，对于前面读取操作的目标 data item，有与其冲突的 &lt;strong&gt;insert&lt;/strong&gt; 或 &lt;strong&gt;update&lt;/strong&gt; 发生。&lt;/p&gt;
&lt;p&gt;根据之前的内容我们已经知道，当多个并行事务对同一 data item 有相邻的读写访问时，这些读写访问操作之间是冲突的。这些冲突在 concurrency control protocols 的帮助下，是可以得到解决，使得数据库的一致性得到某种程度的保障。&lt;/p&gt;
&lt;p&gt;既然读写是冲突的，我们也有 concurrency control 的帮忙，为什么幻读还会出现呢？答案，就在前面提到的各个 concurrency control protocols 的工作方式上。
我们以 lock-based protocol 中的 two-phase lock 为例来看看到底是怎么一回事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当事务进行读取请求的时候，该事务会对将要访问的 tuple 加上共享锁，并维持到事务结束 (RR 条件下)。此时，如果有另外的事务想要修改被上锁的 tuple，需要获得该 tuple 的排他锁。所以，修改无法发生。没有问题。&lt;/li&gt;
&lt;li&gt;但是，如果是不满足 predicate read 的限制范围的 tuple 被 update，因为该 tuple 没有被上锁，这个请求可以立即执行。假如 update 后的 tuple 满足了 predicate read 的限制条件。当我们再次做相同的读取请求时，就会有新的 tuple 被读取。&lt;/li&gt;
&lt;li&gt;同理，insert 请求也可以直接运行。如果其他的事务新添加了满足限制范围的 tuple，那么再次运行相同的读取请求时，也会发现读取结果发生了变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可见，幻读出现的原因在于， 两个事务实际上有冲突，但&lt;strong&gt;因为冲突的对象是还不存在，concurrency control 无法对其访问进行限制&lt;/strong&gt;，所以 unserializable schedules 得以出现。&lt;/p&gt;
&lt;h3 id=&#34;避免幻读的方法&#34;&gt;避免幻读的方法&lt;/h3&gt;
&lt;p&gt;幻读发生的原因是我们无法探知到发生在还未存在的数据上的请求冲突。那么，解决此问题的核心就是，&lt;strong&gt;将请求冲突转移到已存在的数据上&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;例如，可以将冲突转移到 table 的 metadata 上，或者转移到 index 上。即，将 table metadata 或 index 纳入 concurrency control 的管理范围。Serializable level 下不会有幻读问题，因为它同时只让一个事务访问 table，也就是将冲突转移到 table metadata。&lt;/p&gt;
&lt;p&gt;将冲突转移到 table metadata 的方式会大大降低并行度，我们大都不会采用。转移到 index 上是一个不错的选择， &lt;em&gt;index-locking&lt;/em&gt; 就是其中著名的方式之一。&lt;/p&gt;
&lt;p&gt;根据 index concurrency control 的方式不同，我们可以有不同的方式来避免幻读。但其核心离不开“将请求冲突转移到已存在的数据上”，以保证只有 serializable schedules 能够出现。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据库通过一定的 concurrency control protocols 来保证多个事务并行运行时结果的正确性。&lt;/li&gt;
&lt;li&gt;幻读的发生，是因为基本的 concurrency control protocols 不能探知到发生在还未存在的数据对象上的访问冲突。&lt;/li&gt;
&lt;li&gt;解决幻读的方法是，将对未存在的数据对象上的访问冲突，转移到已存在的数据上去。著名的方法有 index-locking，next-key locking 等。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#4</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445-project04/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445-project04/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-09T22:54:38&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">We are implementing a lock-based concurrency control scheme in this project. More specifically, the strict two phase locking protocol.
The concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.</summary>
            
                <content type="html">&lt;p&gt;We are implementing a lock-based concurrency control scheme in this project. More specifically, the &lt;strong&gt;strict two phase locking protocol&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.&lt;/p&gt;
&lt;p&gt;You need to read the source code carefully. The instruction of this project is kind vague, or even wrong.&lt;/p&gt;
&lt;h2 id=&#34;task-1---lock-manager&#34;&gt;Task 1 - Lock Manager&lt;/h2&gt;
&lt;p&gt;Read &lt;code&gt;transaction.h&lt;/code&gt;, &lt;code&gt;transaction_manager.h&lt;/code&gt;, and &lt;code&gt;log_manager.h&lt;/code&gt; to learn the APIs first.&lt;/p&gt;
&lt;p&gt;The log manager only communicates with transactions and the transaction manager. As the textbook says, the log manager tracks all the lock requests for different tuples; a transaction tracks all the locks it holds for different tuples.&lt;/p&gt;
&lt;p&gt;All the works are around the management of the hash table in the &lt;code&gt;log_manager&lt;/code&gt; and the two sets that track locks in &lt;code&gt;transaction&lt;/code&gt;. We only need to implement the basic logic structure of each API in this task because we will modify them in the following tasks.&lt;/p&gt;
&lt;h2 id=&#34;task-2---deadlock-prevention&#34;&gt;Task 2 - Deadlock Prevention&lt;/h2&gt;
&lt;p&gt;We use &lt;strong&gt;wound-wait&lt;/strong&gt; here, which means, when requesting a lock on a data item, the &amp;quot;younger&amp;quot; transactions wait for the &amp;quot;older&amp;quot; transactions, while the &amp;quot;older&amp;quot; transactions kill the &amp;quot;younger&amp;quot; transactions.&lt;/p&gt;
&lt;p&gt;Because the &lt;code&gt;log_manager&lt;/code&gt; and &lt;code&gt;transaction&lt;/code&gt; track the lock requests for each tuple and each transaction, this part of work is&lt;/p&gt;
&lt;h2 id=&#34;task-3-concurrency-control&#34;&gt;Task 3 Concurrency Control&lt;/h2&gt;
&lt;p&gt;The instruction and code are inconsistent. The instruction asks us to maintain the write sets in transactions. However, the table write sets have already been handled by the APIs in &lt;code&gt;table_heap.cpp&lt;/code&gt;.  As a result, actually, we do not need to maintain the tuple write sets by ourself.&lt;/p&gt;
&lt;p&gt;Each transaction can execute several queries. We should consider this and modify our lock manager. For example, what should we do if a transaction inserts some tuples, then read them?&lt;/p&gt;
&lt;p&gt;To achieve different isolation level with strict 2PL protocol, we need to use these locks properly in different executor. According to the lecture slides, we should:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Serializable&lt;/strong&gt;: Obtain all locks first; ;plus index locks, plus strict 2PL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeatable Reads&lt;/strong&gt;: Same as above, but no index locks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Committed&lt;/strong&gt;: Same as above, but share locks are released immediately.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Uncommitted&lt;/strong&gt;: Same as above but allows dirty reads (no share locks).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some exceptions that are thrown from the lock manager cannot be fetched by the test code. So, letting the caller of the lock manager to handle lock fails is a better choice. This costed me few hours to debug..&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project03-grades.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#3</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445-project03/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445-project03/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-04-30T20:37:56&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">In this project, we will Implement executors for taking query plan nodes and executing them.
We are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling Next on its children to retrieve tuples and process them one-by-one.</summary>
            
                <content type="html">&lt;p&gt;In this project, we will Implement &lt;strong&gt;executors&lt;/strong&gt; for taking query plan nodes and executing them.&lt;/p&gt;
&lt;p&gt;We are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling &lt;code&gt;Next&lt;/code&gt; on its children to retrieve tuples and process them one-by-one.&lt;/p&gt;
&lt;h2 id=&#34;how-executor-works&#34;&gt;How &lt;code&gt;executor&lt;/code&gt; works&lt;/h2&gt;
&lt;p&gt;Before start coding, we need to learn a lot from the related source code first. The instruction does not show all the details and I believe they did this intentionally.&lt;/p&gt;
&lt;p&gt;As discussed in the lecture, DBMSs will convert a SQL statement into a query plan, which is a tree consisted by operator nodes. The executors that we implement define how we process these operators.&lt;/p&gt;
&lt;p&gt;Inside an operator, we get the expression of the operator. Thus, to implement an executor, we need to get the expression of that plan node, and evaluate these expression in the right way to get the result.&lt;/p&gt;
&lt;p&gt;In this project, I recommend you to read the test code first before anything else. The test code tells us the structure of the code base and how they are combined together.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;plan node&lt;/em&gt; and &lt;em&gt;execution engine&lt;/em&gt; are the most important components.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ExecutionEngine&lt;/code&gt; is defined in &lt;code&gt;execution_engine.h&lt;/code&gt; and there is only one single API -- &lt;code&gt;Execute()&lt;/code&gt;. In this function, it calls &lt;code&gt;ExecuteFactory&lt;/code&gt; to create executor object and return a smart pointer to the caller.&lt;/li&gt;
&lt;li&gt;There is a &lt;code&gt;SetUp()&lt;/code&gt; function in &lt;code&gt;executor_test_util.h&lt;/code&gt; that does all the initializations for us. That&#39;s why we can use &lt;code&gt;GetExecutionEngine()&lt;/code&gt; and &lt;code&gt;GetExecutorContext()&lt;/code&gt; in the test file directly. (I was very curious about this.)&lt;/li&gt;
&lt;li&gt;Different operators have their own plan node. The executor can fetch information it needs from the passed in plan node. Thus, do check all the members of the plan node.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sequential-scan&#34;&gt;Sequential Scan&lt;/h2&gt;
&lt;p&gt;This task needs us to read a lot code to know how these components work. In short, what we need to do is read all the tuples from the &lt;code&gt;TableHeap&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you know how to read a tuple from a given table, this task is almost done. However, do remember to use the output scheme to build the output tuple. Also, use the given predicate to filtrate out unsatisfied tuples.&lt;/p&gt;
&lt;h2 id=&#34;insertion&#34;&gt;Insertion&lt;/h2&gt;
&lt;p&gt;In this part we need to do both:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insert new tuples into the table&lt;/li&gt;
&lt;li&gt;Insert new indexes for these new tuples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There can have several tuples to be inserted in one query, and each table can have several indexes (based on different index keys).&lt;/p&gt;
&lt;p&gt;All the components that we need to insert tuples and indexes can be find through the &lt;code&gt;catalog&lt;/code&gt; of the table. More specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can get table information from the catalog. Through the table information, we can get the container of the table (&lt;code&gt;TableHeap&lt;/code&gt;), which provides the APIs to modify the table.&lt;/li&gt;
&lt;li&gt;We can get index information from the catalog. Similarly, through the index information, we can get the container of the index (&lt;code&gt;Index&lt;/code&gt;), which provides the APIs to modify the index.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because all the APIs are provided by the code base or ourself (i.e., the underlying buffer pool management and extendible hash index), there is no much code we need to write for insertion.&lt;/p&gt;
&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;
&lt;p&gt;Update is very similar with insertion. As described in the project instruction, the APIs that create updated tuples has been provided, so the tuple update part is very simple.&lt;/p&gt;
&lt;p&gt;We need to consider when and how to update the indexes. A hint for this is that the index of attributes in the table index is the same in the table, which means we can know if the index keys are updated.&lt;/p&gt;
&lt;h2 id=&#34;delete&#34;&gt;Delete&lt;/h2&gt;
&lt;p&gt;Delete itself is very straightforward. Just delete the tuple and related indexes.&lt;/p&gt;
&lt;h2 id=&#34;nested-loop-join&#34;&gt;Nested Loop Join&lt;/h2&gt;
&lt;p&gt;Need to read the test case code to learn how to build the joined tuples. More specifically, there are two &lt;code&gt;Expression&lt;/code&gt;s that we need in this task, one is from the predicate, which is used for check if two tuples are matched; another is from the output schema columns, we need to use it to create the output tuples.&lt;/p&gt;
&lt;h2 id=&#34;hash-join&#34;&gt;Hash Join&lt;/h2&gt;
&lt;p&gt;Hash join is more complicated than nested loop join. The good news (?) is that, we assume the hash table can fit in the memory, so the basic hash join algorithm is enough.&lt;/p&gt;
&lt;p&gt;The basic hash join algorithm has two phases: build and probe.
Before we check any tuple of the inner table, we need to build the hash table for the outer table first. This phase should be done at the beginning.&lt;/p&gt;
&lt;p&gt;Then, for each tuple of the inner table, we can use the hash table to find the matched tuples.
We need to create a hash table by ourself. This is the most difficult part. As the instruction told us, we should check &lt;code&gt;SimpleAggregationHashTable&lt;/code&gt; to learn how to create one for hash joining. You should also check &lt;code&gt;aggregation_plan.h&lt;/code&gt;, which contains some components that &lt;code&gt;SimpleAggregationHashTable&lt;/code&gt; uses. This part of work may need deeper knowledge about C++ than other tasks.&lt;/p&gt;
&lt;p&gt;We also need to get the hash keys using the given expressions. (P.S. the instruction in the website may not be updated. There is no &lt;code&gt;GetLeftJoinKey()&lt;/code&gt; and &lt;code&gt;GetRightJoinKey()&lt;/code&gt; member functions in the code base that I am using.)&lt;/p&gt;
&lt;h2 id=&#34;aggregation&#34;&gt;Aggregation&lt;/h2&gt;
&lt;p&gt;Since the hash table is given by the code base, we only need to use these expressions that the plan node gives to us.
We only need to care about the &lt;code&gt;GROUP BY&lt;/code&gt; and &lt;code&gt;HAVING&lt;/code&gt; clauses. Aggregations are handled by the given code.&lt;/p&gt;
&lt;h2 id=&#34;distinct&#34;&gt;Distinct&lt;/h2&gt;
&lt;p&gt;After we know how to build our own hash table through the exercise of hash join, this task becomes quite easy. We only need to create another hash table, and use it to get distinct tuples.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project03-grades.png&#34; alt=&#34;image.png&#34;&gt;
At the time of my submission, I was ranked first on the leaderboard. ^^&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#2</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445_project2/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445_project2/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-03-19T15:58:46&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.</summary>
            
                <content type="html">&lt;blockquote&gt;
&lt;p&gt;Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;task-1---page-layouts&#34;&gt;Task #1 - Page Layouts&lt;/h2&gt;
&lt;p&gt;Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.&lt;/p&gt;
&lt;p&gt;We have implemented the &lt;code&gt;BufferPoolManager&lt;/code&gt; in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of &lt;code&gt;Page&lt;/code&gt; is stored in the page frame? In this task, we need to create two kinds of &lt;code&gt;Page&lt;/code&gt;s for our hash table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hash table directory page&lt;/li&gt;
&lt;li&gt;Hash table bucket page&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we are using the previous allocated memory space (we cast the &lt;code&gt;data_&lt;/code&gt; field of &lt;code&gt;Page&lt;/code&gt; to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary.&lt;/p&gt;
&lt;p&gt;For bitwise operations, because the bitmap is based on char arrays, we can only do bitwise operations char by char (at least, this is what I find).&lt;/p&gt;
&lt;h3 id=&#34;hash-table-directory-page&#34;&gt;Hash Table Directory Page&lt;/h3&gt;
&lt;p&gt;This kind of page stores metadata for the hash table. The most important part is the &lt;strong&gt;bucket address table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks.&lt;/p&gt;
&lt;h3 id=&#34;hash-bucket-page&#34;&gt;Hash Bucket Page&lt;/h3&gt;
&lt;p&gt;Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs.&lt;/p&gt;
&lt;p&gt;There are two bitmaps that we used to indicate if a slot contains valid KV:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;readable_&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;occupied_&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;occupied_&lt;/code&gt; actually has no specially meaning in extendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases.&lt;/p&gt;
&lt;p&gt;The page layout itself is very straightforward. Only the bitwise operations are a little annoying.&lt;/p&gt;
&lt;h2 id=&#34;task-2---hash-table-implementation&#34;&gt;Task #2 - Hash Table Implementation&lt;/h2&gt;
&lt;p&gt;This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages.&lt;/p&gt;
&lt;p&gt;I recommend to implement the search at the very beginning then other functions, since other operations also need search to check if a specific KV pair is existed.&lt;/p&gt;
&lt;h3 id=&#34;search&#34;&gt;Search&lt;/h3&gt;
&lt;p&gt;For a given &lt;code&gt;key&lt;/code&gt;, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key.&lt;/p&gt;
&lt;p&gt;Because we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair.&lt;/p&gt;
&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;The core components of insertion part is split. Highly recommend to use pen and paper to figure how bucket split works before coding.&lt;/p&gt;
&lt;p&gt;The insertion procedure is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find the right bucket&lt;/li&gt;
&lt;li&gt;If there is room in the bucket, insert the KV pair&lt;/li&gt;
&lt;li&gt;If there is no room -&amp;gt; split the bucket&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How to split one bucket? Assume we call the split target &lt;em&gt;split bucket&lt;/em&gt; and the newly created bucket &lt;em&gt;image bucket&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $ global\_depth == local\_depth $:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increase the &lt;code&gt;global_depth&lt;/code&gt; by 1, so double the table size&lt;/li&gt;
&lt;li&gt;The following steps are same as situation $ global\_depth &amp;gt; local\_depth $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $global\_depth &amp;gt; local\_depth$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allocate a new page for the image bucket&lt;/li&gt;
&lt;li&gt;Adjust the entries in the bucket address table
&lt;ul&gt;
&lt;li&gt;leave the half of the entries pointing to the split bucket&lt;/li&gt;
&lt;li&gt;set all the remaining entries to point to the image bucket&lt;/li&gt;
&lt;li&gt;also increase the &lt;code&gt;local_depth&lt;/code&gt; by 1 because we need one more bit to separate them&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rehash KV pairs in the split bucket&lt;/li&gt;
&lt;li&gt;Re-attemp the insertion
&lt;ul&gt;
&lt;li&gt;Should use the &lt;code&gt;Insert()&lt;/code&gt; function because we may need more splits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Add your own test cases. The given test case is so small and cannot cover all situations.&lt;/p&gt;
&lt;h3 id=&#34;remove&#34;&gt;Remove&lt;/h3&gt;
&lt;p&gt;Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us.&lt;/p&gt;
&lt;p&gt;The project description gives a fairly thorough instructions for merge. Follow the instruction is enough.&lt;/p&gt;
&lt;p&gt;Shrinking the table is very straightforward since we use LSB to assign KV pairs to different buckets.&lt;/p&gt;
&lt;h2 id=&#34;task-3-concurrency-control&#34;&gt;Task #3 Concurrency Control&lt;/h2&gt;
&lt;p&gt;Try coarse-grained latch (lock) first, then reduce the latch range.&lt;/p&gt;
&lt;p&gt;No special comments for this. You can do it!&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project02-grades.png&#34; alt=&#34;grades&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#1</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445_project1/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445_project1/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-03-14T21:23:00&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.</summary>
            
                <content type="html">&lt;blockquote&gt;
&lt;p&gt;Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;task-1---lru-replacement-policy&#34;&gt;Task #1 - LRU Replacement Policy&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManger&lt;/code&gt; contains all the frames.
&lt;code&gt;LRUReplacer&lt;/code&gt; is an implementation of the &lt;code&gt;Replacer&lt;/code&gt; and it helps &lt;code&gt;BufferPoolManger&lt;/code&gt; to manage these frames.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;LRU&lt;/code&gt; policy is not very &amp;quot;LRU&amp;quot; in my opinion. Refer the test cases we can see, if we &lt;code&gt;Unpin&lt;/code&gt; the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation).&lt;/p&gt;
&lt;p&gt;You may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how &lt;code&gt;BufferPoolManager&lt;/code&gt; utlizes the &lt;code&gt;LRUReplacer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Actually, it is the &lt;code&gt;BufferPoolMangerInstance&lt;/code&gt; managing the pages in the buffer. The &lt;code&gt;LRUReplacer&lt;/code&gt; itself only contains page frames that we can use for storing new pages.
In other words, the reference (pin) count of pages that existed in the frames that in the &lt;code&gt;LRUReplacer&lt;/code&gt; is zero, and we can swap them out in anytime.&lt;/p&gt;
&lt;p&gt;Since we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the &lt;code&gt;include/common/rwlatch.h&lt;/code&gt; to learn how Bustub (i.e., the DBMS that we are implementing) uses them.&lt;/p&gt;
&lt;h2 id=&#34;task-2---buffer-pool-manager-instance&#34;&gt;Task #2 - Buffer Pool Manager Instance&lt;/h2&gt;
&lt;p&gt;We use &lt;code&gt;Page&lt;/code&gt; as the container to manage the pages of our DB storage engine. &lt;code&gt;Page&lt;/code&gt; objects are pre-allocated for each frame in the buffer pool. We reuse existed &lt;code&gt;Page&lt;/code&gt; objects instead of creating a new one for every newly read in pages.&lt;/p&gt;
&lt;p&gt;We &lt;span class=&#34;underline&#34;&gt;pin&lt;/span&gt; a page when we want to use it, and we &lt;span class=&#34;underline&#34;&gt;unpin&lt;/span&gt; a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, &lt;span class=&#34;underline&#34;&gt;pin&lt;/span&gt; and &lt;span class=&#34;underline&#34;&gt;unpin&lt;/span&gt; are hints to tell the pool manager which page it can swap out if there is no free space.&lt;/p&gt;
&lt;p&gt;Caution, &lt;code&gt;frame&lt;/code&gt; and &lt;code&gt;page&lt;/code&gt; are refering to different concepts. &lt;code&gt;page&lt;/code&gt; is a chunk of data that stored in our DBMS; &lt;code&gt;frame&lt;/code&gt; is a slot in the page buffer that has the same size as the &lt;code&gt;page&lt;/code&gt;. So, use &lt;code&gt;frame_id_t&lt;/code&gt; and &lt;code&gt;page_id_t&lt;/code&gt; at the right place.&lt;/p&gt;
&lt;p&gt;The comments in the base code is not very clear. They use &amp;quot;page&amp;quot; to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManager&lt;/code&gt; uses four components to manage pages and frames:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;page_table_&lt;/code&gt;: a map that stores the mapping relationship between &lt;code&gt;page_id&lt;/code&gt; and &lt;code&gt;frame_id&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;free_list_&lt;/code&gt;: a linked-list that stores the free frames.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;replacer_&lt;/code&gt;: a &lt;code&gt;LRUReplacer&lt;/code&gt; that stores used frames with zero pin count.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pages_&lt;/code&gt;: stores pre-allocated &lt;code&gt;Page&lt;/code&gt; objects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManager&lt;/code&gt; is the &lt;code&gt;friend&lt;/code&gt; of &lt;code&gt;Page&lt;/code&gt;, so we can access the &lt;code&gt;private&lt;/code&gt; members of &lt;code&gt;Page&lt;/code&gt;. (This is a good example about when to use &lt;code&gt;friend&lt;/code&gt; -- when we need to change some member variables but we do not want give setters so that every one can change them.)&lt;/p&gt;
&lt;p&gt;If we can do three things right, this task is not that difficult:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move page to/from LRU.&lt;/li&gt;
&lt;li&gt;Know when to flush a page. (Read points are very clear).&lt;/li&gt;
&lt;li&gt;Which page metadata we need to update.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Critical hints:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned &lt;code&gt;false&lt;/code&gt; in a function, however, they assume we should return &lt;code&gt;true&lt;/code&gt;  in that case. Do not use your own judgement, just follow the description.)&lt;/li&gt;
&lt;li&gt;What will happen if we &lt;code&gt;NewPage()&lt;/code&gt; then &lt;code&gt;Unpin()&lt;/code&gt; the same page immediately?&lt;/li&gt;
&lt;li&gt;Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;task-3---parallel-buffer-pool-manager&#34;&gt;Task #3 - Parallel Buffer Pool Manager&lt;/h2&gt;
&lt;p&gt;Task 3 is very straightforward. If our &lt;code&gt;BufferPoolManagerInstance&lt;/code&gt; is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance.&lt;/p&gt;
&lt;p&gt;Some people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;Passed all test cases with full grades.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static%5Fresources/main/img/202203142113296.png&#34; alt=&#34;Project#1 grades&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/aghayev-2019-filesystemunfit/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/aghayev-2019-filesystemunfit/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2021-01-17T00:29:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore.
The main ideas of BlueStore are:</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;This paper mostly consists of two parts. The first part tells us why the &lt;code&gt;FileStore&lt;/code&gt; has performance issues.
And the second part tells us how Ceph team build &lt;code&gt;BlueStore&lt;/code&gt; based on the
lessons that they learnt from &lt;code&gt;FileStore&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The main ideas of &lt;code&gt;BlueStore&lt;/code&gt; are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Avoid using local file system to store and represent Ceph objects&lt;/li&gt;
&lt;li&gt;Use KV-store to provide transaction mechanism instead of build it by ourself&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-s-the-problem&#34;&gt;What&#39;s the problem&lt;/h2&gt;
&lt;p&gt;There is a software called &lt;code&gt;storage backend&lt;/code&gt; in Ceph. The &lt;code&gt;storage backend&lt;/code&gt; is
responsible to accept requests from upper layer of Ceph and do the real I/O on
storage devices.&lt;/p&gt;
&lt;p&gt;Ceph used to use commonly used local file systems based storage backend (i.e.,
FileStore). FileStore works, but not that well. There are mainly three
drawbacks in FileStore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hard to implement efficient transactions on top of existing file systems&lt;/li&gt;
&lt;li&gt;The local file system&#39; metadata performance is not great (e.g., enumerating
directories, ordering in the return result)&lt;/li&gt;
&lt;li&gt;Hard to adopt emerging storage hardware that abandon the venrable block
interface (e.g., Zone divecies)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting&#34;&gt;Why the problem is interesting&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Because storage backends do the real I/O job, the performance storage
backends domains the performance of the whole Ceph system.&lt;/li&gt;
&lt;li&gt;For years, the developers of Ceph have had a lot of troubles when using local
file systems to build storage backends&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-core-ideas&#34;&gt;The Core Ideas&lt;/h2&gt;
&lt;h3 id=&#34;the-problems-of-filestore&#34;&gt;The problems of FileStore&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transaction&lt;/strong&gt;: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;leveraging file system&#39;s transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces.&lt;/li&gt;
&lt;li&gt;User space WAL: has consistency problem (not atom operations, because it is
a logical WAL, it use read/write system calls to write/read data to/from
logging part). The cost for handle the problem is expensive. Also slow
read-modify-write and double-write (logging all data) problems.&lt;/li&gt;
&lt;li&gt;Using KV-store: This is the cure. However, there is still some unnecessary
file system overhead like &lt;em&gt;journaling of journal&lt;/em&gt; problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Slow metadata operations&lt;/strong&gt;: enumeration is slow. The read result from a object
sets should in order, which file systems do not do. We need to do sorting
after read. To reduce the sorting overhead, Ceph limits the number of files in
a directory, which introduces &lt;em&gt;directory splition&lt;/em&gt;. The dir splition has
overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Does not support new storage hardware&lt;/strong&gt;: new storage devices may need
modifications in the existing file systems. If Ceph uses local file systems,
the Ceph team can only wait for the developers of the file systems to adopt
the new storage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the paper, there are more details. In summary, the reasons of above problems are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;File system overhead&lt;/li&gt;
&lt;li&gt;Ceph uses file system metadata to represent Ceph object metadata. (i.e.,
object to file, object group to diectory) The file system metadata operations
are not fast and also may have some consistent issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bluestore&#34;&gt;BlueStore&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Does not use local file system anymore. Instead, store Ceph objects into raw
storage directly. This method avoids the overhead of file systems.&lt;/li&gt;
&lt;li&gt;Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata.
This method provides much faster metadata operations and also avoid building
transtion mechanism by Ceph developer.&lt;/li&gt;
&lt;li&gt;Because RocksDB runs on top of file systems. BlueStore has a very simply file
system that only works for RocksDB called BlueFS. The BlueFS stores all the
contents in logging space and cleans invalid data periodly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you understand the reason of why FileStore performs not well, you can simply
understand the choices they did when build BlueStore.&lt;/p&gt;
&lt;p&gt;BlueStore still has some issues. For example, because BlueStore do not use file
systems, it cannot leverage the OS page cache and need to build the cache by
itself. However, build a effective cache is hard.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/wang-2020-bcw/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/wang-2020-bcw/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2020-12-31T22:37:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.
The authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full.</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.&lt;/p&gt;
&lt;p&gt;The authors of this paper find that the write requests can have $μ$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer.&lt;/p&gt;
&lt;p&gt;This strategy can reduce SSD pressure which prolongs SSD life and still provide relative good
performance.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-problem&#34;&gt;What is the problem&lt;/h2&gt;
&lt;p&gt;In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.&lt;/p&gt;
&lt;p&gt;More specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is
moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer
requests first, so consumer requests can have shorter latency.&lt;/p&gt;
&lt;p&gt;However, in a real system, SSDs handle most of the write requests and HDDs are idle in more
than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation.
Also, deep queue depth makes requests suffering long latency even when we using SSDs.&lt;/p&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting--important&#34;&gt;Why the problem is interesting (important)?&lt;/h2&gt;
&lt;p&gt;The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer.
The request latency of HDD can be classified as three categories: &lt;em&gt;fast, middle&lt;/em&gt;, and &lt;em&gt;slow&lt;/em&gt;.
Write requests data is put to the buffer first, then to the disk. When the buffer is full,
HDD will block the coming requests until it flushes all the data in the buffer into disk.
When there are free space in the buffer, request latency is in fast or middle range, otherwise
in slow range.&lt;/p&gt;
&lt;p&gt;The fast and middle latency is in $μ s$-level which similar with the performance of SSD.
If we can control the buffer in disk to handle requests which their size is in the buffer
size range, then we can get SSD-level performance when using HDD to handle small write
requests.&lt;/p&gt;
&lt;h2 id=&#34;the-idea&#34;&gt;The idea&lt;/h2&gt;
&lt;p&gt;Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also
provides reasonable performance.&lt;/p&gt;
&lt;p&gt;To achieve the goal, there are two key components in this paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure requests to HDD are in the fast and middle latency range&lt;/li&gt;
&lt;li&gt;Determining which write requests should be dispatch to HDD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To handle the first challenge, the authors provided a prediction model. The model itself
is simply comparing the current request size with pre-defined threshold.
We cannot know the write buffer size of HDD directly. However, we can get an approximate
value of the buffer size through profiling. The threshold are the cumulative amount of written data for the
fast/mid/slow stages.&lt;/p&gt;
&lt;p&gt;Since we only want to use the fast and middle stages, we need to skip the slow stage.
There are two methods to do this. First, &lt;code&gt;sync&lt;/code&gt; system call from host can enforce the
buffer flush; second, HDD controller will flush the buffer when the buffer is full.
&lt;code&gt;sync&lt;/code&gt; is a expensive operation, so the authors choose to use &lt;em&gt;padding data&lt;/em&gt; to full fill
the buffer, which can let controller to flush the data in the buffer.&lt;/p&gt;
&lt;p&gt;The second reason of why we need padding data is we want to make sure the prediction model
working well. That means the prediction model needs a sequential continuous write requests.
When HDD is idle, the controller will empty the buffer even when the buffer is not full,
which break the prediction. Read requests also break the prediction.
Using padding data can help the system to maintain and adjust the prediction.
More specifically, when HDD is idle, the system use small size padding data to avoid disk
control flush the buffer; when read requests finished, since we cannot know if the disk
controller flushes the buffer, the system use large size padding data to quickly full fill
the buffer, which can help recorrect the prediction model.
These padding data will be remove during the GC procedure.&lt;/p&gt;
&lt;p&gt;Steering requests to HDDs is much easier to understand. The latency of request is related
to the I/O queue depth.
We do profiling to find the relation between SSD&#39;s queue depth and the request request latency.
In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs
fast stage. We use the queue depth value as the threshold.
When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.&lt;/p&gt;
&lt;h2 id=&#34;drawbacks-and-personal-questions-about-the-study&#34;&gt;Drawbacks and personal questions about the study&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Only works for small size of write requests&lt;/li&gt;
&lt;li&gt;The consistency is not guaranteed&lt;/li&gt;
&lt;li&gt;The disk cannot be managed as RAID (can we?)&lt;/li&gt;
&lt;li&gt;GC is still a problem&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: Read as Needed: Building WiSER, a Flash-Optimized Search Engine</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/he-2020-readasneeded/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/he-2020-readasneeded/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2020-12-26T20:15:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requested.
They reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive.</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;This paper proposed a NAND-flash SSD-friendly full text engine. This engine can
achieve better performance than existing engines with much less memory requested.&lt;/p&gt;
&lt;p&gt;They reduce the unnecessary I/O (both the number of I/O and the volume). The
engine does not cache data into memory, instead, read data every time when query
arrive.&lt;/p&gt;
&lt;p&gt;They also tried to increase the request size to exploit SSD internal parallelism.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-problem&#34;&gt;What Is the Problem&lt;/h2&gt;
&lt;p&gt;Search engines pose great challenges to storage systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;low latency&lt;/li&gt;
&lt;li&gt;high data throughput&lt;/li&gt;
&lt;li&gt;high scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The datasets become too large to fit into the RAM. Simply use RAM as a cache
cannot achieve the goal.&lt;/p&gt;
&lt;p&gt;SSD and NVRAM can boost performance well. For example, flash-based SSDs provide
much higher throughput and lower latency compared to HDD.
However, since SSDs exhibit vastly different characteristic from HDDs, we need
to evolve the software on top of the storage stack to exploit the full potential
of SSDs.&lt;/p&gt;
&lt;p&gt;In this paper, the authors rebuild a search engine to better utilize SSDs to
achieve the necessary performance goals with main memory that is significantly
smaller than the data set.&lt;/p&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting&#34;&gt;Why the Problem Is Interesting&lt;/h2&gt;
&lt;p&gt;There are already many studies that work on making SSD-friendly softwares within storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processing); and SFS, F2fS (file system).&lt;/p&gt;
&lt;p&gt;However, there is no optimization for full-text search engines.
Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares.&lt;/p&gt;
&lt;h2 id=&#34;the-idea&#34;&gt;The Idea&lt;/h2&gt;
&lt;p&gt;The key idea is: &lt;strong&gt;read as needed&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The reason behind of the idea is SSD can provide millisecond-level read latency,
which is fast enough to avoid cache data into main memory.&lt;/p&gt;
&lt;p&gt;There are three challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;reduce read amplification&lt;/li&gt;
&lt;li&gt;hide I/O latency&lt;/li&gt;
&lt;li&gt;issue large requests to exploit SSD performance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(This work is focus on the data structure, inverted index. This is a commonly used data structure in the information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Techniques&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Reduce read amplification&lt;/td&gt;
&lt;td&gt;- cross-stage data grouping&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;- two-way cost-aware bloom filters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;- trade disk space for I/O&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hide I/O latency&lt;/td&gt;
&lt;td&gt;adaptive prefetching&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Issue large requests&lt;/td&gt;
&lt;td&gt;cross-stage data grouping&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;cross-stage-data-grouping&#34;&gt;Cross-stage data grouping&lt;/h3&gt;
&lt;p&gt;This technique is used to reduce read amplification and issue large requests.&lt;/p&gt;
&lt;p&gt;WiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query.
Inverted index of WiSER places data of different stages in the order that it will be accessed.&lt;/p&gt;
&lt;p&gt;Previous search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requests.&lt;/p&gt;
&lt;h3 id=&#34;two-way-cost-aware-filters&#34;&gt;Two-way Cost-aware Filters&lt;/h3&gt;
&lt;p&gt;When doing phrase queries we need to use the position information to check the terms around a specific term to improve search precision.&lt;/p&gt;
&lt;p&gt;The naive approach is to read the positions from all the terms in the phrase
then iterate the position list.
To reduce the unnecessary reading, WiSER employs a bitmap-based bloom filter.
The reason to use bitmap is to reduce the size of bloom filter. There are many
empty entries in the filter array, use bitmap can avoid the waste.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cost-aware&lt;/em&gt; means comparing the size of position list with that of the bloom
filters. If the size of position list is smaller than that of bloom filters,
WiSER reads the position list directly.&lt;/p&gt;
&lt;p&gt;Two-way filters shares the same idea. WiSER chooses to read the smaller bloom
filter to reduce the read amplification.&lt;/p&gt;
&lt;h3 id=&#34;adaptive-prefetching&#34;&gt;Adaptive Prefetching&lt;/h3&gt;
&lt;p&gt;Prefetching is one of the commonly used technique to hide the I/O latency.
Even though, the read latency of SSD is small. Compare to DRAM, the read latency
of SSD still much larger.&lt;/p&gt;
&lt;p&gt;Previous search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e.
Linux readahead) which increases the read amplification.
WiSER defines an area called &lt;em&gt;prefetch zone&lt;/em&gt;. A prefetch zone is further divided
into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a query are larger than a threshold.&lt;/p&gt;
&lt;p&gt;To enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calls &lt;code&gt;madvise()&lt;/code&gt; with the &lt;code&gt;MADV_SEQUENTIAL&lt;/code&gt; hint to readahead in the prefetch zone.&lt;/p&gt;
&lt;h3 id=&#34;trade-disk-space-for-i-o&#34;&gt;Trade Disk Space for I/O&lt;/h3&gt;
&lt;p&gt;Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves.&lt;/p&gt;
&lt;p&gt;Compressing all data in the buffer together achieves better compression.
However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries.&lt;/p&gt;
&lt;p&gt;In the evaluation, WiSER uses 25% more storage than Elasticsearch. The authors argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">My First Post</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/my-first-post/" />
            <id>https://z1ggy-o.github.io/posts/articles/my-first-post/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2020-11-08T09:48:14&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">I rebuilt my personal blog by using hugo.
I have cleaned all my old blogs (not so much actually).
I will write something related to my research reading or personal learning. Hope I can meet you soon.</summary>
            
                <content type="html">&lt;p&gt;I rebuilt my personal blog by using hugo.&lt;/p&gt;
&lt;p&gt;I have cleaned all my old blogs (not so much actually).&lt;/p&gt;
&lt;p&gt;I will write something related to my research reading or personal learning.
Hope I can meet you soon.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>
