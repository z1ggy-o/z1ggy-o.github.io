<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US">
    <title type="text">Ziggy&#39;s Blog</title>
    <subtitle type="html">This is the personal blog of Ziggy, which mostly focuses on CS and personal life.</subtitle>
    <updated>2022-07-31T21:28:46&#43;08:00</updated>
    <id>https://z1ggy-o.github.io/</id>
    <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/" />
    <link rel="self" type="application/atom&#43;xml" href="https://z1ggy-o.github.io/atom.xml" />
    <author>
            <name>Ziggy</name>
            <uri>https://z1ggy-o.github.io/</uri>
            
                <email>gy.zhu29@outlook.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    <generator uri="https://gohugo.io/" version="0.101.0">Hugo</generator>
        <entry>
            <title type="text">2PC v.s. 3PC: 一句话的总结</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/2pc-vs-3pc/" />
            <id>https://z1ggy-o.github.io/posts/articles/2pc-vs-3pc/</id>
            <updated>2022-07-31T21:28:21&#43;08:00</updated>
            <published>2022-07-31T20:15:08&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">最近在准备面试的过程中看到有这样一个问题，就是让比较一下 2PC 和 3PC。在网上找了一些文…</summary>
            
                <content type="html">&lt;p&gt;最近在准备面试的过程中看到有这样一个问题，就是让比较一下 2PC 和 3PC。在网上找了一些文章来读，感觉都没有十分简洁地说明两者之间最基本的区别点。所以，在这里写一篇小文，表达一下自己对 2PC，3PC 最核心区别的理解。&lt;/p&gt;
&lt;p&gt;最重要的最先说，在我看来两者的核心区别在于：&lt;strong&gt;参与者间是否对 transaction commit/abort 建立了共识&lt;/strong&gt;。3PC 的参与者之间对 commit 的成立是具有共识的，2PC 则没有。&lt;/p&gt;
&lt;h2 id=&#34;3pc-相比-2pc-带来了什么&#34;&gt;3PC 相比 2PC 带来了什么？&lt;/h2&gt;
&lt;p&gt;大家都知道，3PC 相比于 2PC 来说多了两个东西：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加了 time out&lt;/li&gt;
&lt;li&gt;commit phase 被分割为了 prepare commit 和 do commit 两个部分&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;增加一个 prepare commit phase 带来了什么呢？是集群对 commit 这一决定的共识。&lt;/p&gt;
&lt;p&gt;在 2PC 协议中，协调者单方面向参与者发送一次 commit 消息。这个消息有两个含义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;让参与者进行 commit&lt;/li&gt;
&lt;li&gt;所有参与者都认可此 commit&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但需要注意的是，对于一个 transaction，是 commit 还是 abort 这个决定本身只有协调者知道。一旦协调者故障，这部分信息就消失了。所以我们说 2PC 的协调者是单点故障点。&lt;/p&gt;
&lt;p&gt;3PC 协议中，prepare commit 消息让一个 transaction 该 commit 还是 abort 这个决定本身被传导到了所有参与者处。如此一来，如果协调者故障，参与者们可以根据 prepare commit 的情况继续工作。&lt;/p&gt;
&lt;p&gt;举例来说，time out 之后，我们从参与者中选出一个新的协调者。该协调者向其他参与者询问对于当前未完成 transaction 的信息。如果所有参与者都表示已经收到了 prepare commit，则证明我们可以对此 transaction 进行 commit；如果有部分参与者没有收到 prepare commit，我们可以选择 abort 或者再次尝试 commit 流程；如果谁都没有收到，则 rollback。&lt;/p&gt;
&lt;h2 id=&#34;2pc-的参与者为什么不能接替工作&#34;&gt;2PC 的参与者为什么不能接替工作？&lt;/h2&gt;
&lt;p&gt;读完上面的叙述中，大家可能会有疑问。明明 2PC 中的 commit 信息已经携带了所有参与者都同意 commit 的隐藏信息，为什么不能像 3PC 一样让参与者成为新的协调者继续工作呢？为了解决这个疑问，我们需要考虑一个极端情况：协调者在发送部分 commit 信息后故障，收到 commit 信息的参与者在 commit 后也发生故障。&lt;/p&gt;
&lt;p&gt;在这个情况下，如果剩余的参与者之一成为新的协调者，因为所有参与者都没有接收到 commit 消息，我们只能认定这个 transaction 需要被 abort。要是在此之后，之前已经 commit 的故障参与者恢复了，灾难就到来了 -- 集群内各个服务器的数据不一致。&lt;/p&gt;
&lt;p&gt;基于以上原因，2PC 在协调者发生故障后，只能阻塞等待。因为参与者自己不能判断能否对进行着的 transaction 进行 commit 或 abort。&lt;/p&gt;
&lt;p&gt;3PC 则没有这个问题。同样的情况，因为只有在确认所有的参与者都收到 prepare commit 之后才会实际进行 commit。即，在可能出现 abort 决定情况下，系统中谁都还没有对此 transaction 进行 commit。所以，当故障服务器恢复后，不会有 2PC 例子中的数据不一致问题。&lt;/p&gt;
&lt;h2 id=&#34;3pc-的问题&#34;&gt;3PC 的问题&lt;/h2&gt;
&lt;p&gt;敏锐的人可能发现了，上面对 3PC 参与者可以安全接替工作的表述并不是在所有的情况下都成立的。这是因为 3PC 是基于 reliable network 环境设计的。&lt;/p&gt;
&lt;p&gt;具体来说，如果发生了 network partition，又恰好把接收到和未接收到 prepare commit 消息的服务器们分别划到了不同的 partition 中。在 time out 后，一边会选择 commit，而另一边会选择 abort。一旦 network 重新恢复，我们也同样面对数据不一致问题。&lt;/p&gt;
&lt;p&gt;因为在实际的应用中，我们的网络都是不稳定的，加之 3PC 增加了一轮通信，所以很少在工程中使用。&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">TOCS&#39;13 - Spanner: Google’s Globally Distributed Database</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/c-2013-spanner/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/c-2013-spanner/</id>
            <updated>2022-07-31T13:44:03&#43;08:00</updated>
            <published>2022-07-31T13:40:38&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation Some applications need relation data model and strong consistency which BigTable cannot gives.
So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.
Contribution Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Some applications need relation data model and strong consistency which BigTable cannot gives.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide SQL-like interface, strong consistency, and high read performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Replication&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use Paxos to replicate data to several nodes, which provides higher availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Local Transactions (within a paxos group)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Each Paxos group leader has a lock table to implement concurrency control.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spanner chooses to use 2PL as the protocol because it is designed for long-lived txns, which performs poor under optimistic protocols.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distributed Transactions (across paxos groups)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Each Paxos group leader has a transaction manager to support distributed txns.&lt;br&gt;
The transaction manager is used as a participant leader.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spanner uses 2PC to support distributed txns. Paxos group leaders become the coordinator and participants.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Snapshot Isolation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For better read performance, Spanner uses MVCC to enable read without locking&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TrueTime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MVCC needs timestamp. Since Spanner is a globally distributed system, we cannot use logical timestamp (otherwise, the centric timestamp manager can be the bottleneck), we also cannot use physical time (not accurate).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To resolve this problem, Spanner introduces TrueTime, which is a time range based on physical time instead of a single time point.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This time range can cover the error of physical clocks from different datacenters. We use GPS and atomic clocks to sync timestamp on different datacenters. So we can ensure the txn order without a centric timestamp manager.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Micro benchmark that shows the performance of replication, transactions and availability on a setup that the network distance between each other is less than 1ms. (A common layout, most apps do not need to distribute all data worldwide)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Analysis of a real world workload, F1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-main-takeaways&#34;&gt;The main takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A real cool system that shows how to combine techniques from different fields together.&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note - x86 汇编语言（4）：分页机制</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/" />
            <id>https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/</id>
            <updated>2022-07-10T16:59:39&#43;08:00</updated>
            <published>2022-07-10T16:39:25&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">到了该系列笔记的最后一个章节了。之前的几个章节中，我们讨论了最直白的 8086 分段访问模型，…</summary>
            
                <content type="html">&lt;hr&gt;
&lt;p&gt;到了该系列笔记的最后一个章节了。之前的几个章节中，我们讨论了最直白的 8086 分段访问模型，又讲到了保护模式下的分段访问模式，现在我们来讲一讲实际生活中的默认内存管理方式--分页。&lt;/p&gt;
&lt;p&gt;本文中我们只谈寻址部分的内容，关于分页所提供的保护机能不作涉及。&lt;/p&gt;
&lt;p&gt;其余章节如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86&#34;&gt;第一部分: 计算机基础和实模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86&#34;&gt;第二部分：保护模式下的分段寻址和权限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86&#34;&gt;第三部分：多任务支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86&#34;&gt;第四部分：分页机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;为什么分页&#34;&gt;为什么分页？&lt;/h2&gt;
&lt;p&gt;聊到分页，我们经常将其和虚拟内存联系在一起。而虚拟内存又是多任务的好帮手。但是虚拟内存的实现并不需要分页。分页是为了更高效的进行内存管理。&lt;/p&gt;
&lt;p&gt;在多任务的系统中，我们之前已经通过 LDT 隔离了各个任务的内存空间。而段描述符中的 &lt;code&gt;P&lt;/code&gt; 位标志着该段是否存在于内存中，使得我们可以利用它将一个段交换到二级存储器中，为内存腾出空间。这似乎和广泛了解的分页虚拟内存没什么区别，但实际上有两个显著的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;段是不定长的。操作系统想要在内存里找一个合适的位置不容易。会出现“外部碎片化”的问题，即，有些内存空闲空间因为太小，不能被使用而浪费。&lt;/li&gt;
&lt;li&gt;段只能以整体进行交换。因为只有段描述符有一个 &lt;code&gt;P&lt;/code&gt; 位，我们只能选择将整个段换出，或导入。如果想要载入一个大段，可能要换出多个现有的段。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;综上两个原因，基于段的内存管理模式较为复杂，效率也较低。还有，一个段的大小不能大于物理内存的空间，否则无法被载入。&lt;/p&gt;
&lt;p&gt;分页机制则是以固定的大小对内存进行分割，每个单位就是所谓的“页”。对于每个页，我们都有相应的数据结构对其进行描述和管理。&lt;/p&gt;
&lt;p&gt;对固定大小的页进行管理有两个好处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们的管理模式变得更加简单。定长总是比不定长好处理。&lt;/li&gt;
&lt;li&gt;因为一般页的单位较小（例如 4KB）我们能够对内存进行更细颗粒度的管理，也突破了段大于物理内存则无法被载入运行的限制。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用分页机制也会有碎片化的问题，相对的，是“内部碎片化”，即一个页没有被完全使用。不过因为页的单位大小一般不大，相对外部碎片化来说，内部碎片化带来的内存浪费要少许多。&lt;/p&gt;
&lt;h2 id=&#34;分页机制的工作原理&#34;&gt;分页机制的工作原理&lt;/h2&gt;
&lt;p&gt;在 Intel 的设计中，分页机制是基于分段机制之上的。无论是否使用分页，分段机制都必须被开启。其结果就是，我们之前提到的寻址和保护机制等在分页模型下继续存在，我们是在之前的基础上追加分页。即，将段分成更小的页。&lt;/p&gt;
&lt;p&gt;在之前的分段模型中，不论是实模式还是保护模式下，都是基于“段基地址 + 段内偏移”的方式获得的。处理器中有专门对其进行计算的模块，我们叫它&lt;em&gt;段部件&lt;/em&gt;。我们把由段部件获得的地址称为&lt;em&gt;线性地址&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;在不开启分页机制时，线性地址就是内存物理地址。它被直接用到地址线上，帮助我们进行寻址。在使用分页机制之后，段被进一步拆分为更小的页。操作系统按照页为单位进行内存的分配和回收。其结果就是，一个段在逻辑上是连续的，但是，被分割为多个页之后，各个页可能被分配在不同的位置。这使得段和页之间有了一层映射关系。&lt;/p&gt;
&lt;p&gt;因为段的分割，以及其对应页的随机位置分配，之前由段部件提供的线性地址不再能被直接使用于内存寻址，而是要经过&lt;em&gt;页部件&lt;/em&gt;的处理，才能得到最终可用的内存物理地址。所以，在开启分页之后，段部件生成的线性地址，又被叫做&lt;em&gt;虚拟地址&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;图示大致如下：
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/virtualMem2phyMem.png&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;
&lt;h2 id=&#34;如何通过分页机制进行寻址&#34;&gt;如何通过分页机制进行寻址&lt;/h2&gt;
&lt;p&gt;上面我们提到，在使用分页后，之前我们熟悉的线性地址不能再被直接使用，而是需要再被进行一次转换。&lt;/p&gt;
&lt;p&gt;这个转换倒也不复杂。假设我们有一个段，它被分成了 3 个页。因为页的大小是固定的，我们可以根据具体的线性地址，通过计算，知道对应的页是谁。基于那个页的基地址，再加上偏移量，我们就从线性地址转换到了内存的物理地址。&lt;/p&gt;
&lt;p&gt;从上面的过程可以发现，要把通过分段获得的线性地址（虚拟地址）转换为分页后的物理地址，我们需要两个信息：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;段和页的映射信息。&lt;/li&gt;
&lt;li&gt;一个页的相关信息。最基本的，这个页具体的物理地址。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体的，我们把内存空间按页的大小进行分割之后，使用&lt;em&gt;页表&lt;/em&gt;来存放各个页的相关信息。因为页的大小固定，通过简单的除法就能从线性地址得知页在页表中的 index；而对应的&lt;em&gt;页表项&lt;/em&gt;中存放了该页的信息。&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/pageTable.png&#34; width=&#34;400&#34;/&gt;
&lt;h3 id=&#34;页表分级&#34;&gt;页表分级&lt;/h3&gt;
&lt;p&gt;上面其实已经体现了分页的工作核心。但是，实际应用起来，我们会发现一个问题，就是页表太大了。&lt;/p&gt;
&lt;p&gt;以 32 位系统为例。系统的寻址空间是 4GB，每个页表项大小为 4 bytes (32 位地址），假设以最常见的 4KB 为一个页的大小，则页表需要占用 4MB 的空间。&lt;/p&gt;
&lt;p&gt;这个空间占用看似不大，但是，接下来我们会提到，为了加强任务间的隔离，各个任务会使用自己的一份页表。这样一来，当同时运行的任务数量变多时，光页表自己就已然成为了内存消耗大户。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，人们选择使用页表分级的方式来节约内存。以二级表为例，除了上述的页表之外，再添加一个&lt;em&gt;页目录表&lt;/em&gt;。我们把页表里的表项分组，将上述的一个大的页表，分成多个小的页表。然后，页目录表中的表项记录页表们的信息。如此一来，对应页所在的页表就可以在需要的时候才创建，在不被使用的时候被交换到二级存储器中去，节约内存空间。&lt;/p&gt;
&lt;p&gt;有些朋友可能会问，只用一层级的页表，我们按需扩容不行吗？没用到的内存地址，就不为它生成对应的页表区域，需要的时候再扩展，那不也行吗？可惜的是，我们的线性地址和页表项的映射关系计算是静态的，如果一开始就需要用到高位的内存空间，又只有一级页表，那页表必须一开始就覆盖整个地址空间。碰巧的是，我们还真的需要在一开始就使用到高位和低位的地址空间，所以答案是否定的。为什么会用到，卖个关子，稍后再说。&lt;/p&gt;
&lt;p&gt;页目录表，页表，以及对应内存的关系大致如下图所示：
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/pageTableLevels.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;页目录表以及页表各自的表项内容如下。除了基地址外，还有一些域，它们和页的使用情况记录有关。感兴趣的朋友可以另外查询手册了解。
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/pageTableEntry.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;地址变换过程&#34;&gt;地址变换过程&lt;/h3&gt;
&lt;p&gt;有了地址变换所需的数据结构，现在我们就来看一看具体的地址变换过程。我们假设一个页的大小是 4KB，页表的页表项为 4 bytes，页目录的表项也为 4 bytes，每 1024 个页表项被分为一组，登记在页目录中。因为映射方式是静态的，过程其实很直接：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;页部件将段部件给出的线性地址分为三段：高 10 位，中 10 位，低 12 位。&lt;/li&gt;
&lt;li&gt;高 10 位作为页目录表的 index，帮助我们找到对应页表区段的起始地址。&lt;/li&gt;
&lt;li&gt;中 10 位作为页表的 index，帮我们从页表中找到对应页的页表项。&lt;/li&gt;
&lt;li&gt;页表项中有着该页的基地址，线性地址的低 12 位作为页内偏移量，和基地址相加，最终获得物理地址。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;看图理解会来得更直接一些。
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/LAtoPA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;分页以及虚拟地址空间&#34;&gt;分页以及虚拟地址空间&lt;/h2&gt;
&lt;p&gt;文章开头我们提到，保护模式下的分段机制已经可以支持虚拟内存。不过，在分段机制下，所有的任务共享者一个地址空间。虽然每个任务有自己的 LDT，但是各个 LDT 中的段描述符内的地址肯定是不同的，除非我们故意要指向相同的区域。&lt;/p&gt;
&lt;p&gt;在分页机制下，因为通过页表，我们引入了另一层的间接寻址访问。基于这个间接访问层，我们可以通过使用不同页表的方式，创建多个虚拟内存空间。这也是处理器设计者推荐的，所以之前我们说，一般我们会为每个任务创建它自己的页表。&lt;/p&gt;
&lt;p&gt;通过使用不同的页表，就算两个任务使用相同的虚拟地址进行寻址，因为页表内最终的映射的位置不同，它们会访问到不同的内存地址。这提供了更好的任务间隔离。&lt;/p&gt;
&lt;h3 id=&#34;硬件支持&#34;&gt;硬件支持&lt;/h3&gt;
&lt;p&gt;有多个页表，就自然有了去哪找到页表的问题。和之前 LDT 相似，处理器为记录页表的基地址，提供了一个单独的寄存器，这个寄存器就是 CR3。它存放着当前任务的第一级页表（在我们的例子中，页目录表）的起始物理地址。每个任务的 TSS 中也有相应的域记录第一级页表的起始地址，所以任务切换的时候，处理器会自动的帮我们进行切换页表。&lt;/p&gt;
&lt;h3 id=&#34;全局空间和局部空间的映射&#34;&gt;全局空间和局部空间的映射&lt;/h3&gt;
&lt;p&gt;每个任务有了自己的页表，这使得每个任务都访问自己的私有页面。但是除了各个用户任务私有的内容之外，它们还要访问一些公共资源，比如操作系统内核提供的公共函数。&lt;/p&gt;
&lt;p&gt;在之前的分段模型中，我们通过各种门来进行内核公共函数的调用。因为分页是基于分段之上的，所以之前的内容并没有改变。但，因为现在任务完全使用自己的虚拟空间，为了让它们能够找到那些公共的资源，我们需要相应的为它们设置页表来映射到对应的位置。使得每个任务可以通过自己的虚拟空间地址和页表找到这些资源。&lt;/p&gt;
&lt;p&gt;解决方法倒也不复杂。我们将整个虚拟内存空间分为&lt;em&gt;局部空间&lt;/em&gt;和&lt;em&gt;全局空间&lt;/em&gt;两个部分。全局空间的地址范围内，我们将各个任务的页表都映射到共享的资源去；对于每个任务自己的内存需求，则限制在局部空间的地址范围内。&lt;/p&gt;
&lt;p&gt;一般来说，我们把地址空间的高地址区作为全局空间，低地址区作为局部空间。这就是之前页表分级部分中提到的，我们要同时使用高低地址区域的原因。&lt;/p&gt;
&lt;h2 id=&#34;开启分页功能的小技巧&#34;&gt;开启分页功能的小技巧&lt;/h2&gt;
&lt;h3 id=&#34;平坦模型&#34;&gt;平坦模型&lt;/h3&gt;
&lt;p&gt;32 位以上架构中，不需要分段也可以拥有完全的寻址能力。加上分页也提供了任务间的隔离，使得分段不再具有太大的意义。&lt;/p&gt;
&lt;p&gt;在实际实现中，人们会仅仅声明一个拥有整个空间寻址能力的段，基于这个段之上利用分页进行内存管理。这就是所谓的分段模型。如此一来，一个程序中的所有数据都按照统一的基地址来安排，省去一些切换段寄存器的麻烦。&lt;/p&gt;
&lt;h3 id=&#34;解决页表的蛋鸡问题&#34;&gt;解决页表的“蛋鸡问题”&lt;/h3&gt;
&lt;p&gt;敏锐的朋友可能发现了，上述的分页访问模式中，在修改页目录表的时候，有一个“鸡生蛋，蛋生鸡”的问题。它发生在访问页目录表的时候。因为开启分页后，一切的访问都需要通过页目录表，页表，以及页部件的配合，来将虚拟地址转换为物理地址。&lt;/p&gt;
&lt;p&gt;我们可以通过 CR3 获得页目录表的物理地址，但没有用。就算它是我们想要的结果，我们现在却不能用它进行直接的寻址。想要修改页目录表，我们需要通过它自己先来找到它的物理地址。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，我们需要在开启分页前，在页目录表中添加指向自己的页表信息。一个巧妙的办法是将页目录表中一个表项指向目录表自己，如此一来，它既是页目录表，也是页表。书中给的做法是在页目录表最后的一个表项内添上目录表的物理地址，便于构造相应的虚拟地址。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;因为分段机制不能很好的应对多任务情况下的内存管理，我们引入了分页机制。&lt;/li&gt;
&lt;li&gt;Intel 的架构中，分页机制工作在分段机制之上。分页以固定长度为单位对段进行分割，并进行管理。&lt;/li&gt;
&lt;li&gt;为了记录段和页之间的映射关系，我们添加了页表这样一个数据信息。相应的，为了适应多任务切换，处理中有 CR3 寄存器存储页表基地址，TSS 中也有相应的域来存储该地址。&lt;/li&gt;
&lt;li&gt;分页可以提供多个虚拟空间，使得任务间的隔离更加完全。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/computer-organization/" term="Computer Organization" label="Computer Organization" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note - x86 汇编语言（3）：多任务支持</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/" />
            <id>https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/</id>
            <updated>2022-07-10T17:00:05&#43;08:00</updated>
            <published>2022-07-06T16:44:20&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">这是系列笔记的第三篇。在上一篇中，我们介绍了保护模式基于段选择子的寻址方式，这一篇中…</summary>
            
                <content type="html">&lt;hr&gt;
&lt;p&gt;这是系列笔记的第三篇。在上一篇中，我们介绍了保护模式基于段选择子的寻址方式，这一篇中我们来讲一讲该架构的另一个特点，多任务支持。&lt;/p&gt;
&lt;p&gt;其余章节如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86&#34;&gt;第一部分: 计算机基础和实模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86&#34;&gt;第二部分：保护模式下的分段寻址和权限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86&#34;&gt;第三部分：多任务支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86&#34;&gt;第四部分：分页机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;任务运行程序的表达&#34;&gt;任务：运行程序的表达&lt;/h2&gt;
&lt;p&gt;在之前的篇章中我们尽量避免了任务概念的出现。在 80286 之前，计算机主要以单任务（single-tasking）的方式运行。即，先集中于完成一个程序的运行，再运行下一个程序。有些类似于最初 mainframe 的 batching 的感觉。80286 首次在处理器硬件上提供了对多任务的支持，80386 时期多任务得到了广泛的应用。&lt;/p&gt;
&lt;p&gt;所谓任务，就是我们所熟知的进程。它是一个运行中的程序的抽象表达。我们知道，一个处理器只能在一个时间内运行一个指令。所以，多个程序的“同时”运行是一个假象。它的本质是处理器为各个程序分别运行一段时间，然后快速地在各个程序之间切换。&lt;/p&gt;
&lt;p&gt;一个程序在运行的时候，它会使用到计算机的各个部件，或者叫资源。要在多个程序之间快速切换，不仅要切走，还要能切回来。即，我们在载入其他程序之前，需要把当前程序所用到的资源保存起来，这样以后才能被重新载入，继续运行。这些与一个运行的程序相关的资源，可以叫做 context。所以，我们也会把任务切换叫做 context switching。&lt;/p&gt;
&lt;p&gt;这些与一个任务相关的信息被存放在一个叫做 Task State Segment (TSS) 的内存区域中，它的格式和保存的内容如下：&lt;/p&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/TSS.png&#34; width=&#34;300&#34;/&gt;
&lt;h2 id=&#34;任务间隔离&#34;&gt;任务间隔离&lt;/h2&gt;
&lt;p&gt;在上一篇中我们提到，保护模式下，我们使用段选择子从段描述符表中获取段描述符的方式来进行寻址。我们也提到了一个系统全局的描述符表，GDT。&lt;/p&gt;
&lt;p&gt;但是，如果仅有一个 GDT，基于段的保护措施是无法实现真正的保护的。因为每个任务都需要访问段描述符表来进行寻址，那么 GDT 所在的内存区间需要对所有的任何开放。其结果就是，一个任务可以访问同级别或低级别其他任务的段，只需出给相应段的段选择子就好了。&lt;/p&gt;
&lt;p&gt;为了杜绝任务间的非法访问，我们不能让每个程序都通过 GDT 来进行寻址，而是让每个任务有自己的一个描述符表，这就是 Local Descriptor Table (LDT)。&lt;/p&gt;
&lt;p&gt;LDT 在结构上和 GDT 没有不同，只是每个任务都有自己单独的 LDT，而且进行内存访问的时候只能使用自己的 LDT 来进行。因为 LDT 内只有和该任务自己相关的描述符存在，上述问题就得到了解决。&lt;/p&gt;
&lt;h2 id=&#34;处理器对支持多任务所做的设计&#34;&gt;处理器对支持多任务所做的设计&lt;/h2&gt;
&lt;p&gt;为了应对多任务，我们引入了 TSS 和 LDT。那么为了便捷地找到 TSS 和 LDT 我们的处理器就增加了两个相应的寄存器：TR 和 LDTR。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/TR-LDTR.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;TR 寄存器中我们存放当前任务 TSS 的地址，LDTR 中我们存放当前任务 LDT 的地址。段选择子中有一个 &lt;code&gt;TI&lt;/code&gt; 位，指明我们需要使用 GDT 还是 LDT 进行寻址。&lt;/p&gt;
&lt;p&gt;在实际的运行过程中，任务间的切换是相当频繁的。为了提高切换的速度，处理器会协助我们进行任务切换。具体的来说，只要我们给出想要切换的目标任务的 TSS，处理器会自动帮我们存储当前任务的信息，并且载入目标任务的信息。&lt;/p&gt;
&lt;h2 id=&#34;如何管理任务&#34;&gt;如何管理任务&lt;/h2&gt;
&lt;p&gt;任务的切换由处理器来协助，但是什么时候进行切换，切换到哪一个任务是由操作系统说了算的。&lt;/p&gt;
&lt;p&gt;为了进行任务调度，操作系统需要维护各个任务相关的信息，当前其中必须要有 TSS 和 LDT 的位置信息。所以，操作系统需要有一个内存区域来存放关于任务的信息，这个区域就叫做 Task Control Block (TCB)。&lt;/p&gt;
&lt;p&gt;TCB 并不是处理器的工作要求，而是存粹的操作系统软件内容，所以，每个操作系统都可以设计自己管理 TCB 的方式，以及存放的信息。最最简单的模型就是，TCB 中只有 TSS 和 LDT 域，然后每个 TCB 用链表连接起来。当然，实际生活中的操作系统会用更加成熟高效的管理方式。&lt;/p&gt;
&lt;h2 id=&#34;如何进行任务切换&#34;&gt;如何进行任务切换&lt;/h2&gt;
&lt;p&gt;任务切换有两种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协同式：即当前工作的任务主动放弃执行权；或者在其通过调用门请求操作系统系统服务的时候，由操作系统趁机把控制转换到另外的任务。&lt;/li&gt;
&lt;li&gt;抢占式：一般情况下，是利用定时器中断，并在中断服务程序中实施任务切换。因为硬件中断信号总是会定时出现，任务切换总能够进行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用控制转移指令，我们可以进行协同式的切换；利用中断，我们可以进行抢占式的中断。&lt;/p&gt;
&lt;p&gt;这里追加一个信息。前一章中我们提到，可以使用调用门来进行特权转换。实际上，除了调用门之外，我们还有三类与中断相关的门，它们被存放在中断描述符表（IDT）中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中断门&lt;/li&gt;
&lt;li&gt;陷阱门&lt;/li&gt;
&lt;li&gt;任务门&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;中断门和陷阱门在一般的中断处理中使用，指向的是处理函数的代码段选择子和段内偏移；而任务门则是用来进行任务切换的，存放着任务的 TSS 选择子。&lt;/p&gt;
&lt;h3 id=&#34;利用控制转移指令进行任务切换&#34;&gt;利用控制转移指令进行任务切换&lt;/h3&gt;
&lt;p&gt;我们可以利用 &lt;code&gt;call&lt;/code&gt; 和 &lt;code&gt;jmp&lt;/code&gt; 指令并给出一个 GDT 中的 TSS 描述符。处理器转移过程中，发现对象是 TSS 描述符，就会相应的进行任务切换。&lt;/p&gt;
&lt;p&gt;相似的，我们也可以利用 &lt;code&gt;call&lt;/code&gt; 和 &lt;code&gt;jmp&lt;/code&gt; 指令并给出一个任务门描述符。处理器判断出描述符指向任务门后，进行相应的任务切换。&lt;/p&gt;
&lt;h3 id=&#34;利用异常和中断进行切换&#34;&gt;利用异常和中断进行切换&lt;/h3&gt;
&lt;p&gt;如果异常或中断发生的时候，中断号指向的是中断描述符表中的任务门，处理器会进行相应的切换。&lt;/p&gt;
&lt;p&gt;使用中断的时候有一个要点，即，不论是中断门，陷阱门，还是任务门，它们都是中断相关，所以最后都是用 &lt;code&gt;iret&lt;/code&gt; 指令返回。前者是返回同一个任务的不同代码段，后者是返回到之前被中断暂停的任务。那么问题是，处理器如何知道自己该怎么做呢？&lt;/p&gt;
&lt;p&gt;答案在 EFLAGS 寄存器的 &lt;code&gt;NT&lt;/code&gt; 位上。NT 即 Nested，如果 &lt;code&gt;NT&lt;/code&gt; 位被置为 1，则说明当前的任务是嵌套于其他任务，或者说，是从其他任务切换而来的。&lt;/p&gt;
&lt;p&gt;处理器每次执行 &lt;code&gt;iret&lt;/code&gt; 指令的时候都会检测当前 EFLAGS 寄存器的 &lt;code&gt;NT&lt;/code&gt; 位。如果是 0 则说明是中断，直接返回即可；如果是 1 说明是任务切换，需要保存当前任务信息并载入之前的任务（之前任务的 TSS 位置被记录在当前任务的 TSS 中）。&lt;/p&gt;
&lt;p&gt;这里也仅仅是简单地做一个介绍，实际的运行过程中还有相应的权限检测等内容。有需要的话请参照手册学习。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;任务是一个运行中的程序的逻辑表达。也就是所谓的进程。&lt;/li&gt;
&lt;li&gt;为了支持多任务，我们引入了 TSS 来存放一个任务的运行环境信息，以及 LDT 来进行任务间的空间隔离。&lt;/li&gt;
&lt;li&gt;处理器在任务切换的时候会自动地帮助我们存储当前任务的信息，并载入新任务。但是什么时候进行任务切换，切换哪个任务，是由操作系统控制的。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/computer-organization/" term="Computer Organization" label="Computer Organization" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note - x86 汇编语言（2）：保护模式下的分段寻址方法和权限设计</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/" />
            <id>https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</id>
            <updated>2022-07-10T17:00:05&#43;08:00</updated>
            <published>2022-07-05T17:16:52&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">这是本系列笔记的第二个部分。这个部分里，我们来看一看保护模式的一大改变——寻址方法，…</summary>
            
                <content type="html">&lt;hr&gt;
&lt;p&gt;这是本系列笔记的第二个部分。这个部分里，我们来看一看保护模式的一大改变——寻址方法，以及配合而来的保护功能。另一个大改变是对多任务的支持，我们在之后的篇章里再谈。&lt;/p&gt;
&lt;p&gt;多提一句，保护模式，protected mode，其实是 protected virtual-address mode 的略称。虚拟内存在保护模式下成为可能，不过这一篇中我们不谈它。另外，本文描述的保护机制皆基于段而言，分页模式下有另外的保护方法。&lt;/p&gt;
&lt;p&gt;其余章节如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86&#34;&gt;第一部分: 计算机基础和实模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86&#34;&gt;第二部分：保护模式下的分段寻址和权限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86&#34;&gt;第三部分：多任务支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86&#34;&gt;第四部分：分页机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;段寄存器的改变&#34;&gt;段寄存器的改变&lt;/h2&gt;
&lt;p&gt;上一篇中我们提到，8086 为了增强自己的寻址能力，提出了分段模型。它在段寄存器里存储段基地址，然后利用此地址与偏移地址结合实现寻址。&lt;/p&gt;
&lt;p&gt;Intel 在 80286 处理器中提出了新的段选择器设计。80826 还是一个 16 位的处理器，但是它的地址线被继续扩充至 24 位。如此一来，分段访问是不可避免的。
但是，处理器的设计者没有选择之前的左移策略，而是提出了一个新的段寄存器设计以及随之而来的新的寻址方式，提升了寻址的速度。先来看看新的段寄存器的结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/32bits-registers.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，段寄存器被分为了两个部分：第一个部分是 16 位的&lt;em&gt;段选择器&lt;/em&gt;，还有一个对我们隐藏的&lt;em&gt;描述符高速缓存器&lt;/em&gt;。我们可以向段选择器部分赋值，但是高速缓存器部分是由处理器自己控制的。&lt;/p&gt;
&lt;p&gt;在这个结构之下，我们不再是直接向段选择器赋值段基地址，而是给与一个叫做&lt;em&gt;段选择子&lt;/em&gt;的值。然后通过它找到对应的段描述符，最终通过描述符中的记录的段基实现寻址。也就是说，我们现在需要间接性的获得段基地址。&lt;/p&gt;
&lt;p&gt;有些人把 80286 的这种工作模式叫做“16 位保护模式”。这样的方式使得 80286 不再需要使用左移的方式来获得段基地址，也同时提供了保护能力。因为 80286 还是 16 位的处理器，虽然寻址能力增强了，但是每个段内还是只能有 16 位的寻址能力。但因为当时软件的缘故，很多人没有利用 286 的保护模式。所以，一般谈到保护模式的时候，都是以 80386 的 32 位架构为例，本文也是如此。但是 286 才是第一个引入保护模式和多任务的处理器，是一个颇具影响力的处理器。&lt;/p&gt;
&lt;h2 id=&#34;新结构下的访问方法&#34;&gt;新结构下的访问方法&lt;/h2&gt;
&lt;p&gt;上面提到，在新的结构下，我们通过段选择子来找到段描述符，最终实现寻址。那么从哪里找呢？答案是&lt;em&gt;描述符表&lt;/em&gt;。我们得先在内存中创建一个描述符表，然后将描述符放入其中，在此之后，才能够进行正常的寻址，这也是处理器总是从实模式开始运行的原因。&lt;/p&gt;
&lt;p&gt;描述符的大小是固定的。依据描述符表的基地址以及描述符在表中的 index，就能找到该描述符。描述符在表中的 index 由段选择子提供，而为了记录描述符表的基地址，处理器增加了新的寄存器。&lt;/p&gt;
&lt;p&gt;首先，对于整个系统，有一个全局的描述符表，叫做 global descriptor table (GDT)。为了存放它的起始地址，我们有了一个新的寄存器，GDTR。之所以叫“全局”，是相对任务私有空间而言，在这个篇章内，我们先按下不表。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/gdt-gdtr.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;当我们为段寄存器赋值新的段选择子时，处理器会自动的从 GDTR 获取基地址，并根据计算获得相应的段描述符。获取到的段描述符被缓存在段寄存器的高速缓存区中。在不改变段选择子的情况下，处理器会直接使用缓存区中的描述符，而不是每次都访问 GDT。&lt;/p&gt;
&lt;p&gt;通过描述符表和描述符来进行寻址的方法还可以任务之间的隔离。这一篇章我们先集中于寻址方法，多任务在下一篇再说。&lt;/p&gt;
&lt;h2 id=&#34;保护模式保护些什么&#34;&gt;保护模式保护些什么？&lt;/h2&gt;
&lt;p&gt;在 Intel 提出 IA32 的 32 位处理器架构之后，数据线和地址线的长度得到了统一，都是 32 位。如此一来，不需要分段模型，我们也可以随意地访问 32 位数值所能表达的内存空间（4GB）。但是，处理器的设计者没有抛弃分段模型，而是将其目的由增强寻址能力，变为了提供保护能力。&lt;/p&gt;
&lt;p&gt;那么保护模式保护了些什么呢？最重要的是三个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;访问范围的保护&lt;/li&gt;
&lt;li&gt;类型保护&lt;/li&gt;
&lt;li&gt;访问权限保护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是段描述符内的各种信息情况。可以看到除了我们需要的段基地址之外，描述符中还添加了许多其他的域。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/segment-descriptor.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;段界限&lt;/code&gt;部分告诉我们段的长度是多少。处理器会在我们发送请求的时候检测我们给出的 offset 是否超出了段的范围，这是访问范围的保护。&lt;/p&gt;
&lt;p&gt;图中还可以看到一个 &lt;code&gt;Type&lt;/code&gt; 区域。它表明该段的类型，是数据段，还是代码段？如果我们想赋值一个数据段给 CS 寄存器从而运行里面的内容，那是被禁止的。这是类型保护。&lt;/p&gt;
&lt;p&gt;最后，在图中还有一个 &lt;code&gt;DPL&lt;/code&gt; 区域，即 descriptor privilege level，代表该段的访问权限。处理器会在更改段寄存器内容前，检测访问请求者是否有访问该内存段的权限。&lt;/p&gt;
&lt;p&gt;对于各个部分的保护，其具体的检查方式就不在此赘述。大家可以参阅书籍以及处理器手册。&lt;/p&gt;
&lt;h2 id=&#34;保护模式的权级保护&#34;&gt;保护模式的权级保护&lt;/h2&gt;
&lt;p&gt;从刚才的图中我们可以看到 DPL 一共是 2 位。所以，我们可以有四个不同的等级，数值越低，则权限等级越高。即，0 级是最高的等级。&lt;/p&gt;
&lt;p&gt;权限等级设计使得程序指令所能访问的空间可以得到限制。这是我们想要的，特别是 i32 作为一个为多任务而生的架构，防止恶意程序对其他任务搞破坏是很重要的。&lt;/p&gt;
&lt;p&gt;例如，操作系统内核的代码和数据十分重要。将内核所使用的内存段设为高权限，而让一般的程序运行在低权限，如此一来，一般的程序不能访问到高权限的内存地址，也就不能接触内核，从而使得系统的安全性得到了提高。&lt;/p&gt;
&lt;p&gt;那么，随之而来的问题是，我们知道段有了自己的访问权限，并且在段描述符中得以表达。可是访问请求者，即&lt;strong&gt;指令的权级是如何表达的呢&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;这个问题乍一看有些没头脑。我们都知道，操作系统它有高权限等级，一般用户程序的进程有低权限等级，这不就分开了呗？可是，对于处理器来说，它可不知道什么操作系统和用户程序的区别，这些是我们抽象的逻辑单位。所有程序都是指令流，指令本身可没什么高低的说法。&lt;/p&gt;
&lt;p&gt;应对这个问题，聪明的处理器设计者想出了一个法子，就是让 DPL 分饰两角。即，指令所在的段的等级，就是指令的等级。&lt;/p&gt;
&lt;p&gt;可执行的指令被存放在代码段中，代码段被赋值给 CS 寄存器。CS 和 IP 两个寄存器引导着下一个将要执行的指令的位置。当一个代码段的描述符被加载到 CS 寄存器后，该段的 DPL 就成为了该段内指令的等级。CS 寄存器中描述符的 DPL 就有了一个新名字，叫 CPL，current privilege level。&lt;/p&gt;
&lt;h2 id=&#34;特权级转换&#34;&gt;特权级转换&lt;/h2&gt;
&lt;p&gt;上面我们举过一个例子，我们将内核程序的段描述符的 DPL 设置为高等级，将一般用户程序的段描述符的 DPL 设置为低等级。如此一来，就算一般用户知道内核程序的代码和数据在哪，处理器也会阻止它进行访问。&lt;/p&gt;
&lt;p&gt;但是这也带来了一个问题，那就是，其实有些内核的内容我们是想要给一般用户程序使用的。例如，内核提供给其他程序的公共函数。现在，这些内容没法被用户程序直接使用了。&lt;/p&gt;
&lt;p&gt;解决这个问题的方法倒也直接，把用户指令的权限等级给提高了就行了呗。但是，用户程序是不可能自己给自己提升权限等级的，不然谁想在低等级呢？处理器设计者给出的方案是，提供一些特定的入口，使得正在运行的代码通过这些入口后特权级得到转换。&lt;/p&gt;
&lt;p&gt;这种入口有两个，分别是&lt;em&gt;依从代码&lt;/em&gt;和&lt;em&gt;门&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依从代码：代码段的描述符中 &lt;code&gt;Type&lt;/code&gt; 域中有一个 &lt;code&gt;C&lt;/code&gt; 位，表明该代码段是否是依存代码。
&lt;ul&gt;
&lt;li&gt;依存代码可以被比它低等级的程序直接调用。&lt;/li&gt;
&lt;li&gt;在跳转控制，载入代码段时，处理器不会改变 CS 中的 CPL 字段。使得依从代码在其调用者的权限等级下运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;门：是一种描述符。和段描述符的使用方法类似，也是被存放在描述符表中。门描述符内存放着目标函数所在代码段的段选择子和段内偏移。（简直是描述符的连接大联欢^.^)
&lt;ul&gt;
&lt;li&gt;我们可以使用 &lt;code&gt;jmp far&lt;/code&gt; 和 &lt;code&gt;call far&lt;/code&gt; 来调用门，实现代码控制转移。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jmp far&lt;/code&gt; 和依从代码类似，处理器在转移时，不会更改 CS 的 CPL 字段，使得被调用函数工作在请求者的特权级上。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;call far&lt;/code&gt; 则使用目标代码段的特权级别运行。而且，除了 return 之外，不能把控制转移到低特权级的代码段（即，高级别函数不能调用低级别函数）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了配合特权级的切换和数据保护，每个任务针对每个特权级别都有单独的栈空间。在切换特权级的同时，也会切换到相应等级的栈。&lt;/p&gt;
&lt;h2 id=&#34;权级保护的最后一块拼图rpl&#34;&gt;权级保护的最后一块拼图——RPL&lt;/h2&gt;
&lt;p&gt;根据之前的描述，在拥有了 CPL 和 DPL 控制访问权限，也拥有了依从代码和门使得低特权程序能够调用操作系统提供的公共函数，似乎保护模式已经完整了。可惜的是，我们没有考虑一个特殊情况，就是有人会搞破坏！&lt;/p&gt;
&lt;p&gt;以一个 I/O 请求为例，它是通过操作系统提供的接口来实现的。用户程序调用 I/O 请求函数，同时给出读写的磁盘地址和内存地址。我们通过门调用了系统的 I/O 函数，相应函数的地址被导入 CS 寄存器，CPL 也随着被更改到更高的等级。I/O 函数由操作系统提供，我们暂且简单地认为操作系统是可靠的。那么，整个函数的运行过程是安全的。&lt;/p&gt;
&lt;p&gt;但是，I/O 请求函数的运行过程中，并不是所有的元素都是操作系统控制的。具体的来说，读写请求的磁盘地址和内存地址是由用户程序提供的。如下图所示，如果低特权程序将高特权级的内存地址作为参数给到 I/O 请求函数，因为该函数拥有最高特权，它就代替了低特权程序访问到了该程序本不该访问到的内存空间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cpl-and-rpl.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这其实就是保护模式工作原理的一个漏洞。即，保护模式仅仅在修改寄存器的时候检测当前指令的特权级。而在上面的例子中，用户程序没有直接访问高级别的数据段，处理器就提供不了保护。&lt;/p&gt;
&lt;p&gt;上述漏洞的问题在于，处理器只是单纯的执行指令，它可不知道这个指令属于谁。为了解决这个问题，处理器设计者提出了一个软硬结合方案，即让操作系统参与到权限检测的过程中来。&lt;/p&gt;
&lt;p&gt;具体来说，因为是操作系统提供的高特权函数，它可以在函数代码中包含一些信息，使得处理器可以知道数据的来源。这个信息就是 RPL，request priviledge level. 因为我们依靠段选择子来进行寻址，所以，RPL 是被嵌入到段选择子中的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/segment-selector.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;在引入了 RPL 之后，低特权程序在调用操作系统提供的函数时，函数中会设置参数中的选择子的 RPL 为低特权程序的权级。处理器在使用选择子的时候，除了比较 DPL 和 CPL 之外，也会比较 DPL 和 RPL。如此一来，就杜绝了上述系统函数代替低特权程序访问其不能访问的空间的问题。用 Intel 的话来说，RPL 的引入“确保了特权代码不会代替应用程序访问一个段，除非应用程序自己拥有访问那个段的权限”。&lt;/p&gt;
&lt;p&gt;需要注意的是，处理器本身仅仅是添加了检查 RPL 的步骤而已。设置 RPL 是软件自己来进行的。&lt;/p&gt;
&lt;p&gt;基本的特权级检查规则可以请查看本书的 14.1 节或者处理器手册。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;新架构中，段寄存器的结构进行了升级。由段选择器和缓存区组成。&lt;/li&gt;
&lt;li&gt;开启保护模式之后，我们使用段选择子，到描述符表中获取段描述符的方式进行寻址。&lt;/li&gt;
&lt;li&gt;通过描述符中的 DPL，CS 寄存器中的 CPL，以及段选择子中的 RPL，我们得以实现了保护功能。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/computer-organization/" term="Computer Organization" label="Computer Organization" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note - x86 汇编语言（1）：计算机基础和实模式</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/" />
            <id>https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/</id>
            <updated>2022-07-10T16:59:52&#43;08:00</updated>
            <published>2022-07-03T17:29:01&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">最近读完了《x86 汇编语言：从实模式到保护模式》一书。感觉此书是了解 Intel 架构 CPU 运行方式…</summary>
            
                <content type="html">&lt;hr&gt;
&lt;p&gt;最近读完了《x86 汇编语言：从实模式到保护模式》一书。感觉此书是了解 Intel 架构 CPU 运行方式的好材料。所以，我准备用四个篇章来写一份读书笔记，简略地介绍一下内存寻址方式，以及处理器对多任务的支持。笔记的内容意在指出书中的框架重点，作为一个快速复习和检索的工具，而非详实的内容。&lt;/p&gt;
&lt;p&gt;本篇是该系列的第一个章节，专注于计算机的运行，8086 处理器，以及实模式。内容较为简单直接。其他章节如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86&#34;&gt;第一部分: 计算机基础和实模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86&#34;&gt;第二部分：保护模式下的分段寻址和权限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86&#34;&gt;第三部分：多任务支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/articles/x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86&#34;&gt;第四部分：分页机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;计算机的基本结构和运行本质&#34;&gt;计算机的基本结构和运行本质&lt;/h2&gt;
&lt;p&gt;我们当今使用的计算机结构为冯诺依曼结构。这个结构的特点就是以存储为中心。具体些说，这个结构中，我们将程序指令和数据以二进制的形式，不加区分地放在内存中。&lt;/p&gt;
&lt;p&gt;程序的编写者将提前编写好的程序载入内存，并告诉计算机将要执行的指令所在的位置，计算机便会到指定的地方找到指令并根据指令运行。抽象来说，计算机运行有 4 个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;取指&lt;/li&gt;
&lt;li&gt;译码&lt;/li&gt;
&lt;li&gt;执行&lt;/li&gt;
&lt;li&gt;回写&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可见，整个运行的过程主要就是两个内容：内存的读写，和计算。为了支持这个运作方式，硬件上，我们的计算机有五个重要的部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;运算器：进行计算&lt;/li&gt;
&lt;li&gt;控制器：引导指令运行顺序&lt;/li&gt;
&lt;li&gt;储存器：存放指令和数据&lt;/li&gt;
&lt;li&gt;输入设备：和计算机进行沟通&lt;/li&gt;
&lt;li&gt;输出设备：同上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然我们每天接触的计算机看似非凡超能，但是它其实是一个十分单纯的机器。从通电的一刻开始，计算机就是在不断地重复我们上面讲的四个运行步骤。&lt;/p&gt;
&lt;h2 id=&#34;以计算机的启动过程为例&#34;&gt;以计算机的启动过程为例&lt;/h2&gt;
&lt;p&gt;我们使用的计算机，在每次通电的时候，都会从一个提前指定好的内存地址开始，进行取指和运算。&lt;/p&gt;
&lt;p&gt;那个起始位置，存放的就是 &lt;a href=&#34;https://en.wikipedia.org/wiki/BIOS&#34;&gt;BIOS&lt;/a&gt; 的第一行指令。BIOS 在完成自己的工作后，其最后的一部分指令就是把其他的程序读取到指定的内存位置，并让计算机从那个位置继续运行。&lt;/p&gt;
&lt;p&gt;一般来说，BIOS 的最后一部分指令会将启动磁盘的第一个扇区读取到内存中，并跳转执行。也是因为这个原因，我们把启动磁盘的第一个扇区叫做主引导扇区。&lt;/p&gt;
&lt;p&gt;因为我们可以方便地向磁盘写入数据，主引导扇区是我们最先能接触到的代码区域。它的容量不大，其中的指令一般是继续从磁盘中读取其他程序并运行。例如，它可以载入 bootloader 并最终引导操作系统。&lt;/p&gt;
&lt;h2 id=&#34;8086-的基本结构&#34;&gt;8086 的基本结构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/8086-architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;8086 是一枚极其成功的处理器，它是应用最广泛个人计算机行业的基础，x86 的 86 就是说这个 86。它极大地影响了 Intel 接下来的处理器功能设计。可以说，更加新型的 Intel 处理器都是在 8086 的底子上进行功能的完善和添加。&lt;/p&gt;
&lt;p&gt;8086 是一枚 16 位的处理器。它的寄存器，以及内外部的数据线的位宽都是 16 位，不过外部地址线是 20 位的，这给了它更强的寻址能力，也引出了分段模型。&lt;/p&gt;
&lt;p&gt;8086 一共有三种不同的寻址模式 (即数据的读取和存放方式)：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寄存器寻址&lt;/li&gt;
&lt;li&gt;立即寻址&lt;/li&gt;
&lt;li&gt;内存寻址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从寻址模式上可以看出，除了内存之外，寄存器也是一个相当重要部件。而对于这个章节的内容来说，最重要的寄存器就是指令寄存器 IP，以及多个段寄存器。&lt;/p&gt;
&lt;p&gt;指令寄存器之所以重要，是因为处理器根据它的内容来进行取指。而段寄存器之所以重要，是因为我们需要借助它们来进行内存寻址。&lt;/p&gt;
&lt;h2 id=&#34;8086-的分段模型&#34;&gt;8086 的分段模型&lt;/h2&gt;
&lt;p&gt;上面我们提到除了外部地址线是 20 位的，8086 的其他部件都是 16 位。之所以提出内存的分段访问模型，就是为了能够实现 20 位的寻址能力。&lt;/p&gt;
&lt;p&gt;其方式为：段寄存器内的值左移 4 位，加上 16 位段内偏移。如此一来，便拼凑出了 20 位的地址。处理器会根据此地址直接和内存沟通，实现数据的读写。&lt;/p&gt;
&lt;p&gt;段寄存器内的值叫做&lt;em&gt;段基地址&lt;/em&gt;。通过修改段寄存器内的段基地址，我们就可以在逻辑上将内存分段，每个段最大有 16 位的寻址能力。分段不仅使得我们有了更强的寻址能力，也让程序载入重定位变得更加容易：程序编写的时候使用相对某个段的相对地址，使得载入位置变得随意。&lt;/p&gt;
&lt;p&gt;具体的寻址指令以及处理器的其他信息就不在此叙述，手册才是它们的归宿。&lt;/p&gt;
&lt;h2 id=&#34;intel-处理器的实模式&#34;&gt;Intel 处理器的实模式&lt;/h2&gt;
&lt;p&gt;实模式，又叫实地址模式。它其实就是 8086 的运行模式。这个“实”字其实有两个含义：&lt;/p&gt;
&lt;p&gt;第一个含义是，我们在指令中给出的内存地址，即为处理器使用的地址。我们直接向段寄存器赋值段基地址，然后与段内偏移结合，获得内存物理地址。&lt;/p&gt;
&lt;p&gt;实模式的另一个含义，其实也是上面提到的 8086 的特点，就是它无条件的相信我们给出的指令。在实模式下，所有的段都可以随意读、写，其中存放的内容也可以被随意执行。&lt;/p&gt;
&lt;p&gt;接下来我们要讲的保护模式，就是对针对各个段的读写，以及执行加以权限控制的模式。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;我们现在使用的冯诺依曼结构计算机，将数据和指令无差别地以二进制存放在存储装置中的。&lt;/li&gt;
&lt;li&gt;计算机很单纯。它就是在不断地读取指令，运行指令，然后将运行结果存放。&lt;/li&gt;
&lt;li&gt;8086 处理器通过段寄存器实现了分段模型进行内存访问，作为一个 16 位处理器，提供了 20 位的寻址能力。&lt;/li&gt;
&lt;li&gt;8086 很单纯。它允许我们任意地读、写、执行内存中的内容。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/computer-organization/" term="Computer Organization" label="Computer Organization" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">SOSP&#39;97 - Frangipani: a scalable distributed file system</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/hekkath-1997-frangipaniscalabledistributed/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/hekkath-1997-frangipaniscalabledistributed/</id>
            <updated>2022-07-01T21:36:44&#43;08:00</updated>
            <published>2022-07-01T21:30:21&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution A real system that shows us how to do cache coherence and distributed transactions.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;File system administration for a large, growing computer installation is a laborious task.&lt;/li&gt;
&lt;li&gt;A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A real system that shows us how to do cache coherence and distributed transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.&lt;/li&gt;
&lt;li&gt;Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.&lt;/li&gt;
&lt;li&gt;All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.&lt;/li&gt;
&lt;li&gt;Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)&lt;/li&gt;
&lt;li&gt;Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.&lt;/li&gt;
&lt;li&gt;The underlying distributed storage and the lock server are both using Paxos to ensure availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.&lt;/li&gt;
&lt;li&gt;They tested both the local performance and the scalability of the proposed system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-main-finding-of-the-paper&#34;&gt;The Main Finding of the Paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Complex clients sharing simple storage can have better scalability.&lt;/li&gt;
&lt;li&gt;Distributed lock can be used to achieve cache coherence and distributed transaction.&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">VLDB&#39;17 - An empirical evaluation of in-memory multi-version concurrency control</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/wu-2017-empiricalevaluationinmemory/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/wu-2017-empiricalevaluationinmemory/</id>
            <updated>2022-07-01T20:41:44&#43;08:00</updated>
            <published>2022-07-01T20:36:05&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation MVCC is the most popular scheme used in DBMSs developed in the last decade. However, there is no standards about how to implement MVCC. Many DBMSs tell people that they use MVCC and how they implement it. There are several design choices people need to make, however, no one tells why they choose such way to implement their MVCC.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MVCC is the most popular scheme used in DBMSs developed in the last decade. However, there is no standards about how to implement MVCC.&lt;/li&gt;
&lt;li&gt;Many DBMSs tell people that they use MVCC and how they implement it. There are several design choices people need to make, however, no one tells why they choose such way to implement their MVCC.&lt;/li&gt;
&lt;li&gt;This paper gives a comprehensive evaluation about MVCC in main memory DBMSs and shows the trade-offs of different design choices.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A good introduction about MVCC.&lt;/li&gt;
&lt;li&gt;Evaluation about how concurrency control protocol, version storage, garbage collection, and index management affect performance on a real in-memory DBMS.&lt;/li&gt;
&lt;li&gt;Evaluation of different configurations that used by main stream DBMSs.&lt;/li&gt;
&lt;li&gt;Advisings about how to achieve higher scalability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This is an evaluation paper. There is no &amp;quot;solution&amp;quot; but evaluation analyses.&lt;/li&gt;
&lt;li&gt;People mainly focus on concurrency control protocols when they talk about scalability. However, the evaluation results show that the version storage scheme is also one of the most important components to scaling an in-memory MVCC DBMS in a multi-core environment.&lt;/li&gt;
&lt;li&gt;Delta storage scheme is good for write-intensive workloads especially only a subset of the attributes is modified. However, delta storage can have slow performance on read-heavy analytical workloads because it spends more time on traversing version chains.&lt;/li&gt;
&lt;li&gt;Most DBMSs choose to use tuple-level GC. However, the evaluation result shows that transaction-level can provide higher performance (at least in main memory DBMS) because it has smaller memory footprint.&lt;/li&gt;
&lt;li&gt;In terms of index management, logical pointer is always a better choice than physical pointers.&lt;/li&gt;
&lt;li&gt;The design choices that Oracle/MySQL and NuoDB made seems have the best performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;They uses a DBMS implemented in CMU, called Peloton.&lt;/li&gt;
&lt;li&gt;The evaluation platform has 40 cores with 128 GB memory.&lt;/li&gt;
&lt;li&gt;Workloads are YCSB and TPC-C (both OLTP)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-main-finding-of-this-paper&#34;&gt;The main finding of this paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you want to learn MVCC, read this one.&lt;/li&gt;
&lt;li&gt;Still, &amp;quot;Measure, Then build&amp;quot;.&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">VLDB&#39;14 - Staring into the abyss: An evaluation of concurrency control with one thousand cores</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/yu-2014-staringabyssevaluation/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/yu-2014-staringabyssevaluation/</id>
            <updated>2022-07-01T20:33:27&#43;08:00</updated>
            <published>2022-07-01T20:21:38&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.
To understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.&lt;/p&gt;
&lt;p&gt;To understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.&lt;/p&gt;
&lt;p&gt;This paper focus on concurrency control schemes.
(spoiler:  the [following research]([[@An empirical evaluation of in-memory multi-version concurrency control]]) of [[Andrew Pavlo]] found out concurrency control schemes are not the most important component that affect the scalability of main memory DBMS on many-core environment )&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of the scalability of seven commonly used concurrency control schemes (OLTP, in-memory DBMS)&lt;/li&gt;
&lt;li&gt;Evaluation is processed on a simulated machine with 1000 cores (actually is a cluster of 22 real machines)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is evaluation paper, thus there is no solutions but evaluation analyses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All concurrency control schemes fail to scale to a large number of cores. The bottlenecks are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;lock thrashing&lt;/li&gt;
&lt;li&gt;preemptive aborts&lt;/li&gt;
&lt;li&gt;deadlocks&lt;/li&gt;
&lt;li&gt;timestamp allocation&lt;/li&gt;
&lt;li&gt;memory-to-memory copying&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lock thrashing happens in any waiting-based algorithm. Using the &amp;quot;non-waiting&amp;quot; can alleviate this problem, however, we will have more aborting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In high contention workloads, a non-waiting deadlock prevention scheme is better than deadlock detection. (Restart is fast in main memory DBMS)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory allocation (includes timestamp allocation) is usually managed by a centric data structure, which becomes the bottleneck. Avoiding shared, centric data structure is important to achieve higher scalability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add new hardware to offload some tasks (e.g., memory copying) from CPU is a feasible way to improve the scalability&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The evaluation platform is based on a CPU simulator, Graphite (from MIT).&lt;/li&gt;
&lt;li&gt;The authors implemented a lightweight main memory DBMS only for this testing.&lt;/li&gt;
&lt;li&gt;Workloads are YCSB and TPC-C&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;Main finding of the paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Like other systems, to improve the scalability, how to avoid shared data is the key.&lt;/li&gt;
&lt;li&gt;&amp;quot;Measure, Then Build&amp;quot;, and a thorough measurement is always needed. This paper is good enough on analyzing concurrency control schemes, however, the following research revels that this is not the most important part...&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">ATC&#39;10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/hunt-2010-zookeeperwaitfreecoordination/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/hunt-2010-zookeeperwaitfreecoordination/</id>
            <updated>2022-07-01T20:33:27&#43;08:00</updated>
            <published>2022-05-23T17:00:39&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Motivation Large-scale distributed applications require different forms of coordination.
Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.
Contribution Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Large-scale distributed applications require different forms of coordination.&lt;/p&gt;
&lt;p&gt;Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.&lt;/li&gt;
&lt;li&gt;Achieve high performance by relaxing consistency guarantees&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;
&lt;p&gt;ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.&lt;/p&gt;
&lt;p&gt;ZooKeeper provides a set of simplified APIs to access these data nodes. Application develops can use these APIs to implement different forms of coordination that they want.&lt;/p&gt;
&lt;p&gt;ZooKeeper servers use an atomic broadcast protocol, Zab (similar to Raft) to sync state between each other.&lt;/p&gt;
&lt;p&gt;It is vague about how to implement ZooKeeper. However, since it is an open source project, the source code is always available.&lt;/p&gt;
&lt;p&gt;In short, ZooKeeper guarantees correct coordination with high performance through:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using wait-free data objects instead of blocking.&lt;/li&gt;
&lt;li&gt;FIFO client ordering of all operations.&lt;/li&gt;
&lt;li&gt;Linearizing all writes to the leader, then propagate to other servers (they call this A-linearizability).&lt;/li&gt;
&lt;li&gt;Read locally. Can have stale data, however, much faster.&lt;/li&gt;
&lt;li&gt;Asynchronous enables batching to reduce networking and storage I/O overhead.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;Main finding of the paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Abstraction of basic building components can help create more general systems.&lt;/li&gt;
&lt;li&gt;ZooKeeper is one of the best examples that shows us how to weak consistency in favor of higher performance.&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: The design of a practical system for fault-tolerant virtual machines</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-23T17:00:39&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION 1 A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation-1&#34;&gt;MOTIVATION &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the &amp;quot;state-machine approach&amp;quot;, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.&lt;/p&gt;
&lt;p&gt;There are three challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;correctly capturing all the input and non-determinism&lt;/li&gt;
&lt;li&gt;correctly applying the inputs and non-determinism to the backup&lt;/li&gt;
&lt;li&gt;low performance degradation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To handle these challenges, they implemented a fault-tolerant virtual machines in VMware vSphere 4.0. This system reduces the performance of real applications by less than 10%, and needs less than 20 Mb/s network bandwidth.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There is one primary VM and one backup VM on different hosts. All input goes to the primary. And the input is sent to the backup VM via logging channel (network).&lt;/li&gt;
&lt;li&gt;A VM has a broad set of inputs and some additional information is needed for non-deterministic operations. They use the &lt;em&gt;VMware deterministic replay&lt;/em&gt; to records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file.&lt;/li&gt;
&lt;li&gt;They design a protocol to ensure the failover (i.e., switching to the backup) is transparent to the clients. The core idea is that before send an output to the external world, we must need to make sure the backup VM has received the log entry that produces the output.&lt;/li&gt;
&lt;li&gt;The failure detection is handled by UDP heartbeating messages and  logging traffic.&lt;/li&gt;
&lt;li&gt;To avoid &amp;quot;split-brain&amp;quot; situation, they store a flag in the shared storage, so VMs can know if there is any other running primary.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitation&#34;&gt;LIMITATION&lt;/h2&gt;
&lt;p&gt;The limitation is that the implementation only works for uni-processor machines because recording and replaying the execution of a multi-processor VM can lead to significant performance issues (accessing to shared memory is non-deterministic operation).&lt;/p&gt;
&lt;h2 id=&#34;main-takeaway&#34;&gt;MAIN TAKEAWAY&lt;/h2&gt;
&lt;p&gt;It is helpful to distinguish between internal and external and internal events of the system. For an infrastructure, only external events can really affect other applications.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;D. J. Scales, M. Nelson, and G. Venkitachalam, “The design of a practical system for fault-tolerant virtual machines,” SIGOPS Oper. Syst. Rev., vol. 44, no. 4, pp. 30–39, Dec. 2010, doi: 10.1145/1899928.1899932.&amp;#160;&lt;a href=&#34;https://z1ggy-o.github.io/posts/paper-notes/scales-2010-designpracticalsystem/#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: The Google file system</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/ghemawat-2003-googlefilesystem/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/ghemawat-2003-googlefilesystem/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-22T19:11:56&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).
Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;MOTIVATION&lt;/h2&gt;
&lt;p&gt;Google needs a new distributed file system to meet the rapidly growing demands of Google’s data processing needs (e.g., the MapReduce).&lt;/p&gt;
&lt;p&gt;Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.&lt;/p&gt;
&lt;p&gt;This system is developed only for Google’s own programs. As a result, GFS does not provide POSIX APIs. Programs are designed and implemented based on GFS, which simplifies the design of GFS.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GFS uses a single master multiple chunkservers architecture. The master maintains all file system metadata and the chunkservers handle the file data. The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.&lt;/li&gt;
&lt;li&gt;Considering the characteristics of the workloads in Google (append only, sequential read), they decide to divide files into fixed-size chunks (64MB). Each chunk has a globally unique chunk handle assigned by the master, that’s how we can find a chunk of a specific file.&lt;/li&gt;
&lt;li&gt;The client gives file name and in file offset to the master. Then, the master will send back the corresponding chunk handle and the chunkservers that have that chunk. After that, clients will communicate with chunkservers directly. This approach avoids the single master becoming the bottleneck.&lt;/li&gt;
&lt;li&gt;To ensure high availability and also improve parallelism, each file chunk is replicated on multiple chunservers on different racks. The metadata in master is protected by the operation log, also this log is replicated on multiple machines.&lt;/li&gt;
&lt;li&gt;When write happens, the data mutation propagates along the chunkservers incrementally. As a result, the write becomes faster, but clients can read stale data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;EVALUATION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Micro-benchmarks on a small cluster with 16 chunkserver. Tested the read, write, and record append performance.&lt;/li&gt;
&lt;li&gt;Two real world clusters in Google. One for production, another one for research and development.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitation&#34;&gt;LIMITATION&lt;/h2&gt;
&lt;p&gt;With the increasing volume of data, the single master design can no longer cope with the demands.&lt;/p&gt;
&lt;h2 id=&#34;main-takeaway&#34;&gt;MAIN TAKEAWAY&lt;/h2&gt;
&lt;p&gt;There are times when we can discard generalization and design a dedicated system for a specific scenario, which leads to a more simply design.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">MIT 6.824 Lab1: MapReduce</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/mit6.824-lab1/" />
            <id>https://z1ggy-o.github.io/posts/projects/mit6.824-lab1/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-21T13:37:58&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">In this lab, we implement a simplified MapReduce framework.
There are one coordinator process and multiple worker processes. The coordinator manages tasks and the worker pass the input data to the given user map and reduce function.
There is no pipeline, which means we will finish all map tasks first then start working on reduce tasks.</summary>
            
                <content type="html">&lt;p&gt;In this lab, we implement a simplified MapReduce framework.&lt;/p&gt;
&lt;p&gt;There are one coordinator process and multiple worker processes. The coordinator manages tasks and the worker pass the input data to the given user map and reduce function.&lt;/p&gt;
&lt;p&gt;There is no pipeline, which means we will finish all map tasks first then start working on reduce tasks.&lt;/p&gt;
&lt;p&gt;This lab is simply, if you are unfamiliar with Go as I am, then most of your time will spend on learning Go, not implementing MR.&lt;/p&gt;
&lt;h2 id=&#34;coordinator&#34;&gt;Coordinator&lt;/h2&gt;
&lt;p&gt;Information to maintain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Idle tasks&lt;/li&gt;
&lt;li&gt;The state of each running task (pending tasks)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Things need to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The coordinator is actually only a server that provides RPC methods for worker processes&lt;/li&gt;
&lt;li&gt;As a result, the coordinator is actually a &amp;quot;remote&amp;quot; shared resource. We only check task states when workers call the cooresponding methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are basically only two methods we need to provide:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A query method:
&lt;ul&gt;
&lt;li&gt;Give a task to the worker (either map or reduce)&lt;/li&gt;
&lt;li&gt;Record the state of the task, include starting time (or deadline).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A finish method:
&lt;ul&gt;
&lt;li&gt;A worker use this method to ack the end of a task.&lt;/li&gt;
&lt;li&gt;Change process phases can also be handled here.&lt;/li&gt;
&lt;li&gt;If all tasks are finished, we can exit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;worker&#34;&gt;Worker&lt;/h2&gt;
&lt;p&gt;Workers are very straightforward. They keep asking for a task from the coordinator. When the task is finished, worker send a ack to the coordinator.&lt;/p&gt;
&lt;p&gt;If we do not care graceful exit, the worker process can exit if the &lt;code&gt;call()&lt;/code&gt; function return false.&lt;/p&gt;
&lt;h3 id=&#34;map-task&#34;&gt;Map Task&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Get filename with a unique id&lt;/li&gt;
&lt;li&gt;Read the file and call &lt;code&gt;mapf()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Separate KV pairs into different groups (buckets) and store them into different files
3.1 Need get nReduce from the coordinator
3.2 Filename is created by task id and bucket id, e.g., &amp;quot;im-taskid-bucketid&amp;quot; or &amp;quot;mr-X-Y&amp;quot; as the instruction hints told us&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;reduce-task&#34;&gt;Reduce Task&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Read intermediate data from files that created by map tasks&lt;/li&gt;
&lt;li&gt;Sort the read in KV pairs (we can use map directly in this lab because data is fit in the memory)&lt;/li&gt;
&lt;li&gt;Call reduce for each distinct key&lt;/li&gt;
&lt;li&gt;Write the result out filename &amp;quot;mr-out-n&amp;quot;, n is the task unique id&lt;/li&gt;
&lt;/ol&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: MapReduce: simplified data processing on large clusters</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/dean-2004-mapreducesimplifieddata/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/dean-2004-mapreducesimplifieddata/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2022-05-16T17:44:18&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">MOTIVATION Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.
To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex.</summary>
            
                <content type="html">&lt;h2 id=&#34;motivation&#34;&gt;MOTIVATION&lt;/h2&gt;
&lt;p&gt;Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;CONTRIBUTION&lt;/h2&gt;
&lt;p&gt;They implement a library that can provide a simple interface that enables automatic parallelization and distribution of large-scale computations. In addition, the library handles machine failures without interaction with programmers.&lt;/p&gt;
&lt;h2 id=&#34;solution&#34;&gt;SOLUTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; primitives present in functional languages, they provide a restricted programming model that parallelizes the computation automatically (the computation must be deterministic).&lt;/li&gt;
&lt;li&gt;Both Map and Reduce functions are written by the user. The input data will be partitioned into M splits, and each Map task handles one of them. The intermediate output of the Map function is divided into R pieces. The Reduce task will read the intermediate output of the Map function and generate the final output files.&lt;/li&gt;
&lt;li&gt;For a cluster of machines, we have only one master machine, and the others are worker machines. The master machine assigns tasks (Map or Reduce) to workers and tracks the state of each task. The worker machines communicate with the master when work is finished or communicate with other workers to read data to process.&lt;/li&gt;
&lt;li&gt;Fault tolerance is handled by periodical communication and re-computation when a machine fails. Because the computation is deterministic, recomputing the same task is okay.&lt;/li&gt;
&lt;li&gt;Many optimizations are applied in their implementation. Check the paper for details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;EVALUATION&lt;/h2&gt;
&lt;p&gt;They used a cluster that consisted of around 1800 machines. Each machine has only 4GB of memory, and two 160GB IDE disks with a gigabit Ethernet link.&lt;/p&gt;
&lt;p&gt;They tasted grep and sort workloads with 10^{10} 100-byte records (around 1TB). The results are shown in the terms of data throughput with the timeline.&lt;/p&gt;
&lt;h2 id=&#34;main-finding-of-the-paper&#34;&gt;MAIN FINDING OF THE PAPER&lt;/h2&gt;
&lt;p&gt;It is useful to abstract a common pattern of certain computing tasks, and create an infrastructure to handle the common issues.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/distributed-systems/" term="distributed-systems" label="distributed-systems" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">幻读，到底是怎么一回事儿</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/isolation-levels-and-phtantom/" />
            <id>https://z1ggy-o.github.io/posts/articles/isolation-levels-and-phtantom/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-12T15:11:54&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个…</summary>
            
                <content type="html">&lt;p&gt;幻读 (phantom phenomenon) 是数据库使用中经常提到的一个问题。一个常见的面试问题就是：某个数据库，在某个 isolation level 下，会不会出现幻读，以及其解决方法。&lt;/p&gt;
&lt;p&gt;网上有许多关于幻读的文章，但是在读完之后发现，大多数的说明都浮于表面，好像作者们自己也并没有弄清楚幻读的本质。在本文中，我想利用数据库的一些高层抽象概念，来阐述幻读的本质。虽然不涉及任何的具体实现，但相信你在了解到这些概念之后，可以很快地理解幻读，以及各种幻读的处理方法。&lt;/p&gt;
&lt;p&gt;幻读以及其解决方法核心都可以浓缩在一句话中，即:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中的关键词为: &lt;em&gt;predicate read&lt;/em&gt;, &lt;em&gt;insert or update&lt;/em&gt;, 和 &lt;em&gt;nonserializable execution&lt;/em&gt;. 我们将以这些关键词为突破口，来理解幻读问题。&lt;/p&gt;
&lt;h2 id=&#34;并行可能性isolation-和-conflict-serializability&#34;&gt;并行可能性：Isolation 和 Conflict Serializability&lt;/h2&gt;
&lt;p&gt;ACID 是数据库事务的重要特性。其中的 I，即 isolation，指的是“多个事务同时运行的时候，它们之间是相互孤立的”。&lt;/p&gt;
&lt;p&gt;上面对 isolation 的标准定义我个人并不喜欢，因为它太过于抽象。依照我个人的理解，更具体一点儿说，isolation 的目标是保证多个事务同时运行的时候，不会因 race condtion 而使得数据库的一致性(consistency) 被破坏。&lt;/p&gt;
&lt;p&gt;那么如何实现 isolation 呢？其关键就是并行事务运行的 serializability.&lt;/p&gt;
&lt;h3 id=&#34;schedules-and-serial-schedules&#34;&gt;Schedules and Serial Schedules&lt;/h3&gt;
&lt;p&gt;当多个事务同时运行的时候，因为任务调度是由操作系统在负责，这些事务所包含的操作会交错运行。这些操作的一个实际运行顺序被称为一个 &lt;em&gt;schedule&lt;/em&gt;。可见，对于一组同时运行的事务，它们可能出现的 schedules 是非常多的 (有 n 个事务的话，会有 $n!$ 种可能)。&lt;/p&gt;
&lt;p&gt;依据我们对 race condition 的理解，对于同一个 data item，如果并行访问中有写操作参与，就会有 race condition 的出现。那么，对于一组并行事务来说，如果其中有两个或以上的事务对同一个 data item 进行访问，且其中包含写操作，我们就说这些访问操作之间是会有冲突的 (conflict)。我们需要对其进行运行顺序进行控制，否则，最终的结果是不可控的，可能会破坏数据库的一致性。&lt;/p&gt;
&lt;h3 id=&#34;conflict-serializability&#34;&gt;Conflict Serializability&lt;/h3&gt;
&lt;p&gt;想要控制事务运行顺序，最简单的方式就是顺序运行。即，一个事务运行完成之后，才运行另一个事务。这种运行顺序，就是 &lt;em&gt;serial schedules&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Serial schedules 虽然保证了数据库的一致性，但事务的并行也随之消失了。为了提高系统的性能，我们还是想要事务在不破坏一致性的前提下并行工作。如果一些 schedules，它能够支持并行，且能保证其运行结果和 serial schedules 的结果相同，我们就叫它 &lt;em&gt;serializable schedules&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;我们之前提到过，在一个 schedule 中分属于不同事务的两个连续操作，如果它们访问同一个 data item，且其中有一个是写操作，那么它们之间就是冲突的。即，在这个 scheule 中，它们两者的运行顺序是不能改变的。但，如果不冲突，这两个操作的顺序就可以交换。&lt;/p&gt;
&lt;p&gt;通过交换一个 schedule 中不冲突的连续操作，我们可以得到新的 schedule。如果一个 schedule，通过不断的交换各个事务间不冲突的操作，能得到一个 serial schedule，我们就说这个 schedule 具有 &lt;em&gt;conflict serializability&lt;/em&gt; ，或者说它是 &lt;em&gt;conflict serializable&lt;/em&gt; 的。&lt;/p&gt;
&lt;p&gt;无疑，conflict serializable 的事务运行顺序，就是我们想要的运行顺序，因为在允许事务并行运行的同时，它的运行结果和事务依次顺序运行时的结果一致。&lt;/p&gt;
&lt;p&gt;需要追加说明的是，serializability 不止 conflict serializability 一种。例如，还有 view serializability。但因为实现的难度等原因，几乎所有的数据库都是使用 conflict serializability。&lt;/p&gt;
&lt;h2 id=&#34;一致性弱化isolation-levels&#34;&gt;一致性弱化：Isolation Levels&lt;/h2&gt;
&lt;p&gt;Serializability 固然可以保证数据库在事务并行时的一致性，但因为它的限制较多，使得系统的整体并行性能受到了压制。&lt;/p&gt;
&lt;p&gt;有些时候，我们为了性能，根据实际的应用场景，可以牺牲一些对一致性的要求。这就是 isolation levels 的意义所在。SQL 标准中给出了四个不同级别的 isolation levels：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Serializable&lt;/strong&gt;: 最高级别，保证之前提到的 serializable execution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeatable read&lt;/strong&gt;: 一个事务只能读取到其他事务 commit 后的值。而且，如果一个事务会对同一个 data item 读取多次，那么该事务完成最后一个读取之前，其他事务不能更改该 data item。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read committed&lt;/strong&gt;: 仅保证只能读取到其他事务 commit 后的值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read uncommitted&lt;/strong&gt;: 未 commit 的数据也能读取。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;四个等级的主要差距在 read 上，因为它们都保证没有 dirty write。即，如果一个 data item 已经被一个事务修改了，在该事务 commit 或 abort 之前，其他事务不能修改该 data item。&lt;/p&gt;
&lt;h2 id=&#34;一致性保障concurrency-control-protocols&#34;&gt;一致性保障：Concurrency Control Protocols&lt;/h2&gt;
&lt;p&gt;数据库的并行控制 (concurrency control) 子系统的作用，就是保证在多个事务运行的时候，可能出现的运行顺序 (schedules) 能够符合所指定的 isolation level 的要求。&lt;/p&gt;
&lt;p&gt;Concurrency control protocols 是一些并行控制规则，依据这些规则工作，我们就可以控制可能出现的并行事务运行顺序。常见的规则类型有 &lt;em&gt;lock-based protocol&lt;/em&gt;, &lt;em&gt;timestamp-based protocols&lt;/em&gt;, 以及 &lt;em&gt;validation-based protocol&lt;/em&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lock-based protocol 是通过给想要访问的 data item 上锁的方式来实现并行控制。这是一个比较通用的控制方式，在数据库以外的领域也被大量使用。&lt;/li&gt;
&lt;li&gt;Timestamp-based protocol 则是通过记录每个 data item 的读、写 timestamp ，并以之与事务的 timestamp 进行比较的方式来进行并行控制。&lt;/li&gt;
&lt;li&gt;Validation-based protocol 算是 timestamp-based protocol 的一个扩展。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这里，我不对这些 protocol 的具体内容进行阐述。我们只需要知道一点，这些 protocol 的工作原理，不论是上锁，还是添加 timestamp，都有一个前提，就是&lt;strong&gt;目标 data item 是已存在的&lt;/strong&gt;。这个隐性的前提看起来理所当然，但它就是幻读问题的根本所在。&lt;/p&gt;
&lt;h2 id=&#34;终于幻读read-repeatable-level-下会有幻读吗&#34;&gt;终于，幻读：Read repeatable level 下，会有幻读吗？&lt;/h2&gt;
&lt;h3 id=&#34;幻读-phantom-phenomenon&#34;&gt;幻读 Phantom phenomenon&lt;/h3&gt;
&lt;p&gt;幻读现象指的是，在一个事务中，多次运行同一个 query 所得到的结果不同。而这个结果，是其他事务添加或删除被读取的 tuple 而发生的。&lt;/p&gt;
&lt;p&gt;幻读在 read uncommitted，read committed，以及 read repeatable level 下都会出现。但是 read uncommitted 和 read committed 本身还有 dirty read 和 unrepeatable read 的现象。为了避免混淆，我们以 read repeatable (RR) 下的情况来做例子，这也是许多面试题的假定条件。&lt;/p&gt;
&lt;p&gt;要理解幻读出现的原因，先让我们再读一次文章开头的句子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The phantom phenomenon, where conflicts between a predicate read and an insert or update are not detected, can allow nonserializable executions to occurs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可见，幻读的出现是有具体的场景的。第一，是在做 &lt;strong&gt;predicate read&lt;/strong&gt; 的时候；第二，对于前面读取操作的目标 data item，有与其冲突的 &lt;strong&gt;insert&lt;/strong&gt; 或 &lt;strong&gt;update&lt;/strong&gt; 发生。&lt;/p&gt;
&lt;p&gt;根据之前的内容我们已经知道，当多个并行事务对同一 data item 有相邻的读写访问时，这些读写访问操作之间是冲突的。这些冲突在 concurrency control protocols 的帮助下，是可以得到解决，使得数据库的一致性得到某种程度的保障。&lt;/p&gt;
&lt;p&gt;既然读写是冲突的，我们也有 concurrency control 的帮忙，为什么幻读还会出现呢？答案，就在前面提到的各个 concurrency control protocols 的工作方式上。
我们以 lock-based protocol 中的 two-phase lock 为例来看看到底是怎么一回事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当事务进行读取请求的时候，该事务会对将要访问的 tuple 加上共享锁，并维持到事务结束 (RR 条件下)。此时，如果有另外的事务想要修改被上锁的 tuple，需要获得该 tuple 的排他锁。所以，修改无法发生。没有问题。&lt;/li&gt;
&lt;li&gt;但是，如果是不满足 predicate read 的限制范围的 tuple 被 update，因为该 tuple 没有被上锁，这个请求可以立即执行。假如 update 后的 tuple 满足了 predicate read 的限制条件。当我们再次做相同的读取请求时，就会有新的 tuple 被读取。&lt;/li&gt;
&lt;li&gt;同理，insert 请求也可以直接运行。如果其他的事务新添加了满足限制范围的 tuple，那么再次运行相同的读取请求时，也会发现读取结果发生了变化。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可见，幻读出现的原因在于， 两个事务实际上有冲突，但&lt;strong&gt;因为冲突的对象是还不存在，concurrency control 无法对其访问进行限制&lt;/strong&gt;，所以 unserializable schedules 得以出现。&lt;/p&gt;
&lt;h3 id=&#34;避免幻读的方法&#34;&gt;避免幻读的方法&lt;/h3&gt;
&lt;p&gt;幻读发生的原因是我们无法探知到发生在还未存在的数据上的请求冲突。那么，解决此问题的核心就是，&lt;strong&gt;将请求冲突转移到已存在的数据上&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;例如，可以将冲突转移到 table 的 metadata 上，或者转移到 index 上。即，将 table metadata 或 index 纳入 concurrency control 的管理范围。Serializable level 下不会有幻读问题，因为它同时只让一个事务访问 table，也就是将冲突转移到 table metadata。&lt;/p&gt;
&lt;p&gt;将冲突转移到 table metadata 的方式会大大降低并行度，我们大都不会采用。转移到 index 上是一个不错的选择， &lt;em&gt;index-locking&lt;/em&gt; 就是其中著名的方式之一。&lt;/p&gt;
&lt;p&gt;根据 index concurrency control 的方式不同，我们可以有不同的方式来避免幻读。但其核心离不开“将请求冲突转移到已存在的数据上”，以保证只有 serializable schedules 能够出现。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据库通过一定的 concurrency control protocols 来保证多个事务并行运行时结果的正确性。&lt;/li&gt;
&lt;li&gt;幻读的发生，是因为基本的 concurrency control protocols 不能探知到发生在还未存在的数据对象上的访问冲突。&lt;/li&gt;
&lt;li&gt;解决幻读的方法是，将对未存在的数据对象上的访问冲突，转移到已存在的数据上去。著名的方法有 index-locking，next-key locking 等。&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/articles/" term="articles" label="articles" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#4</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445-project04/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445-project04/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-05-09T22:54:38&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">We are implementing a lock-based concurrency control scheme in this project. More specifically, the strict two phase locking protocol.
The concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.</summary>
            
                <content type="html">&lt;p&gt;We are implementing a lock-based concurrency control scheme in this project. More specifically, the &lt;strong&gt;strict two phase locking protocol&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.&lt;/p&gt;
&lt;p&gt;You need to read the source code carefully. The instruction of this project is kind vague, or even wrong.&lt;/p&gt;
&lt;h2 id=&#34;task-1---lock-manager&#34;&gt;Task 1 - Lock Manager&lt;/h2&gt;
&lt;p&gt;Read &lt;code&gt;transaction.h&lt;/code&gt;, &lt;code&gt;transaction_manager.h&lt;/code&gt;, and &lt;code&gt;log_manager.h&lt;/code&gt; to learn the APIs first.&lt;/p&gt;
&lt;p&gt;The log manager only communicates with transactions and the transaction manager. As the textbook says, the log manager tracks all the lock requests for different tuples; a transaction tracks all the locks it holds for different tuples.&lt;/p&gt;
&lt;p&gt;All the works are around the management of the hash table in the &lt;code&gt;log_manager&lt;/code&gt; and the two sets that track locks in &lt;code&gt;transaction&lt;/code&gt;. We only need to implement the basic logic structure of each API in this task because we will modify them in the following tasks.&lt;/p&gt;
&lt;h2 id=&#34;task-2---deadlock-prevention&#34;&gt;Task 2 - Deadlock Prevention&lt;/h2&gt;
&lt;p&gt;We use &lt;strong&gt;wound-wait&lt;/strong&gt; here, which means, when requesting a lock on a data item, the &amp;quot;younger&amp;quot; transactions wait for the &amp;quot;older&amp;quot; transactions, while the &amp;quot;older&amp;quot; transactions kill the &amp;quot;younger&amp;quot; transactions.&lt;/p&gt;
&lt;p&gt;Because the &lt;code&gt;log_manager&lt;/code&gt; and &lt;code&gt;transaction&lt;/code&gt; track the lock requests for each tuple and each transaction, this part of work is&lt;/p&gt;
&lt;h2 id=&#34;task-3-concurrency-control&#34;&gt;Task 3 Concurrency Control&lt;/h2&gt;
&lt;p&gt;The instruction and code are inconsistent. The instruction asks us to maintain the write sets in transactions. However, the table write sets have already been handled by the APIs in &lt;code&gt;table_heap.cpp&lt;/code&gt;.  As a result, actually, we do not need to maintain the tuple write sets by ourself.&lt;/p&gt;
&lt;p&gt;Each transaction can execute several queries. We should consider this and modify our lock manager. For example, what should we do if a transaction inserts some tuples, then read them?&lt;/p&gt;
&lt;p&gt;To achieve different isolation level with strict 2PL protocol, we need to use these locks properly in different executor. According to the lecture slides, we should:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Serializable&lt;/strong&gt;: Obtain all locks first; ;plus index locks, plus strict 2PL.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeatable Reads&lt;/strong&gt;: Same as above, but no index locks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Committed&lt;/strong&gt;: Same as above, but share locks are released immediately.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Uncommitted&lt;/strong&gt;: Same as above but allows dirty reads (no share locks).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some exceptions that are thrown from the lock manager cannot be fetched by the test code. So, letting the caller of the lock manager to handle lock fails is a better choice. This costed me few hours to debug..&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project03-grades.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#3</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445-project03/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445-project03/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-04-30T20:37:56&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">In this project, we will Implement executors for taking query plan nodes and executing them.
We are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling Next on its children to retrieve tuples and process them one-by-one.</summary>
            
                <content type="html">&lt;p&gt;In this project, we will Implement &lt;strong&gt;executors&lt;/strong&gt; for taking query plan nodes and executing them.&lt;/p&gt;
&lt;p&gt;We are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling &lt;code&gt;Next&lt;/code&gt; on its children to retrieve tuples and process them one-by-one.&lt;/p&gt;
&lt;h2 id=&#34;how-executor-works&#34;&gt;How &lt;code&gt;executor&lt;/code&gt; works&lt;/h2&gt;
&lt;p&gt;Before start coding, we need to learn a lot from the related source code first. The instruction does not show all the details and I believe they did this intentionally.&lt;/p&gt;
&lt;p&gt;As discussed in the lecture, DBMSs will convert a SQL statement into a query plan, which is a tree consisted by operator nodes. The executors that we implement define how we process these operators.&lt;/p&gt;
&lt;p&gt;Inside an operator, we get the expression of the operator. Thus, to implement an executor, we need to get the expression of that plan node, and evaluate these expression in the right way to get the result.&lt;/p&gt;
&lt;p&gt;In this project, I recommend you to read the test code first before anything else. The test code tells us the structure of the code base and how they are combined together.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;plan node&lt;/em&gt; and &lt;em&gt;execution engine&lt;/em&gt; are the most important components.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ExecutionEngine&lt;/code&gt; is defined in &lt;code&gt;execution_engine.h&lt;/code&gt; and there is only one single API -- &lt;code&gt;Execute()&lt;/code&gt;. In this function, it calls &lt;code&gt;ExecuteFactory&lt;/code&gt; to create executor object and return a smart pointer to the caller.&lt;/li&gt;
&lt;li&gt;There is a &lt;code&gt;SetUp()&lt;/code&gt; function in &lt;code&gt;executor_test_util.h&lt;/code&gt; that does all the initializations for us. That&#39;s why we can use &lt;code&gt;GetExecutionEngine()&lt;/code&gt; and &lt;code&gt;GetExecutorContext()&lt;/code&gt; in the test file directly. (I was very curious about this.)&lt;/li&gt;
&lt;li&gt;Different operators have their own plan node. The executor can fetch information it needs from the passed in plan node. Thus, do check all the members of the plan node.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sequential-scan&#34;&gt;Sequential Scan&lt;/h2&gt;
&lt;p&gt;This task needs us to read a lot code to know how these components work. In short, what we need to do is read all the tuples from the &lt;code&gt;TableHeap&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you know how to read a tuple from a given table, this task is almost done. However, do remember to use the output scheme to build the output tuple. Also, use the given predicate to filtrate out unsatisfied tuples.&lt;/p&gt;
&lt;h2 id=&#34;insertion&#34;&gt;Insertion&lt;/h2&gt;
&lt;p&gt;In this part we need to do both:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insert new tuples into the table&lt;/li&gt;
&lt;li&gt;Insert new indexes for these new tuples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There can have several tuples to be inserted in one query, and each table can have several indexes (based on different index keys).&lt;/p&gt;
&lt;p&gt;All the components that we need to insert tuples and indexes can be find through the &lt;code&gt;catalog&lt;/code&gt; of the table. More specifically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can get table information from the catalog. Through the table information, we can get the container of the table (&lt;code&gt;TableHeap&lt;/code&gt;), which provides the APIs to modify the table.&lt;/li&gt;
&lt;li&gt;We can get index information from the catalog. Similarly, through the index information, we can get the container of the index (&lt;code&gt;Index&lt;/code&gt;), which provides the APIs to modify the index.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because all the APIs are provided by the code base or ourself (i.e., the underlying buffer pool management and extendible hash index), there is no much code we need to write for insertion.&lt;/p&gt;
&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;
&lt;p&gt;Update is very similar with insertion. As described in the project instruction, the APIs that create updated tuples has been provided, so the tuple update part is very simple.&lt;/p&gt;
&lt;p&gt;We need to consider when and how to update the indexes. A hint for this is that the index of attributes in the table index is the same in the table, which means we can know if the index keys are updated.&lt;/p&gt;
&lt;h2 id=&#34;delete&#34;&gt;Delete&lt;/h2&gt;
&lt;p&gt;Delete itself is very straightforward. Just delete the tuple and related indexes.&lt;/p&gt;
&lt;h2 id=&#34;nested-loop-join&#34;&gt;Nested Loop Join&lt;/h2&gt;
&lt;p&gt;Need to read the test case code to learn how to build the joined tuples. More specifically, there are two &lt;code&gt;Expression&lt;/code&gt;s that we need in this task, one is from the predicate, which is used for check if two tuples are matched; another is from the output schema columns, we need to use it to create the output tuples.&lt;/p&gt;
&lt;h2 id=&#34;hash-join&#34;&gt;Hash Join&lt;/h2&gt;
&lt;p&gt;Hash join is more complicated than nested loop join. The good news (?) is that, we assume the hash table can fit in the memory, so the basic hash join algorithm is enough.&lt;/p&gt;
&lt;p&gt;The basic hash join algorithm has two phases: build and probe.
Before we check any tuple of the inner table, we need to build the hash table for the outer table first. This phase should be done at the beginning.&lt;/p&gt;
&lt;p&gt;Then, for each tuple of the inner table, we can use the hash table to find the matched tuples.
We need to create a hash table by ourself. This is the most difficult part. As the instruction told us, we should check &lt;code&gt;SimpleAggregationHashTable&lt;/code&gt; to learn how to create one for hash joining. You should also check &lt;code&gt;aggregation_plan.h&lt;/code&gt;, which contains some components that &lt;code&gt;SimpleAggregationHashTable&lt;/code&gt; uses. This part of work may need deeper knowledge about C++ than other tasks.&lt;/p&gt;
&lt;p&gt;We also need to get the hash keys using the given expressions. (P.S. the instruction in the website may not be updated. There is no &lt;code&gt;GetLeftJoinKey()&lt;/code&gt; and &lt;code&gt;GetRightJoinKey()&lt;/code&gt; member functions in the code base that I am using.)&lt;/p&gt;
&lt;h2 id=&#34;aggregation&#34;&gt;Aggregation&lt;/h2&gt;
&lt;p&gt;Since the hash table is given by the code base, we only need to use these expressions that the plan node gives to us.
We only need to care about the &lt;code&gt;GROUP BY&lt;/code&gt; and &lt;code&gt;HAVING&lt;/code&gt; clauses. Aggregations are handled by the given code.&lt;/p&gt;
&lt;h2 id=&#34;distinct&#34;&gt;Distinct&lt;/h2&gt;
&lt;p&gt;After we know how to build our own hash table through the exercise of hash join, this task becomes quite easy. We only need to create another hash table, and use it to get distinct tuples.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project03-grades.png&#34; alt=&#34;image.png&#34;&gt;
At the time of my submission, I was ranked first on the leaderboard. ^^&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#2</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445_project2/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445_project2/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-03-19T15:58:46&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.</summary>
            
                <content type="html">&lt;blockquote&gt;
&lt;p&gt;Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;task-1---page-layouts&#34;&gt;Task #1 - Page Layouts&lt;/h2&gt;
&lt;p&gt;Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.&lt;/p&gt;
&lt;p&gt;We have implemented the &lt;code&gt;BufferPoolManager&lt;/code&gt; in previous project. Buffer pool itself only allocate page frame for us to use. However, which kind of &lt;code&gt;Page&lt;/code&gt; is stored in the page frame? In this task, we need to create two kinds of &lt;code&gt;Page&lt;/code&gt;s for our hash table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hash table directory page&lt;/li&gt;
&lt;li&gt;Hash table bucket page&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we are using the previous allocated memory space (we cast the &lt;code&gt;data_&lt;/code&gt; field of &lt;code&gt;Page&lt;/code&gt; to directory page or bucket page), understanding of memory management and how C/C++ pointer operation works are necessary.&lt;/p&gt;
&lt;p&gt;For bitwise operations, because the bitmap is based on char arrays, we can only do bitwise operations char by char (at least, this is what I find).&lt;/p&gt;
&lt;h3 id=&#34;hash-table-directory-page&#34;&gt;Hash Table Directory Page&lt;/h3&gt;
&lt;p&gt;This kind of page stores metadata for the hash table. The most important part is the &lt;strong&gt;bucket address table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At this stage, only implement the necessary functions, and they are very simple. We will come back to add more functions in the following tasks.&lt;/p&gt;
&lt;h3 id=&#34;hash-bucket-page&#34;&gt;Hash Bucket Page&lt;/h3&gt;
&lt;p&gt;Stores key-value pairs, with some metadata that helps track space usages. Here, we only support fixed-length KV pairs.&lt;/p&gt;
&lt;p&gt;There are two bitmaps that we used to indicate if a slot contains valid KV:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;readable_&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;occupied_&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;occupied_&lt;/code&gt; actually has no specially meaning in extendible hashing because we do not need tombstone. However, we still need to maintain it, because there are some related test cases. I assume the teaching team still leave this part here so that they do not need to modify the test cases.&lt;/p&gt;
&lt;p&gt;The page layout itself is very straightforward. Only the bitwise operations are a little annoying.&lt;/p&gt;
&lt;h2 id=&#34;task-2---hash-table-implementation&#34;&gt;Task #2 - Hash Table Implementation&lt;/h2&gt;
&lt;p&gt;This task is very interesting because we use the buffer pool manager that we implemented previously to handle the storage part for thus. We only need to use these APIs to allocate pages and store the hash table in these pages.&lt;/p&gt;
&lt;p&gt;I recommend to implement the search at the very beginning then other functions, since other operations also need search to check if a specific KV pair is existed.&lt;/p&gt;
&lt;h3 id=&#34;search&#34;&gt;Search&lt;/h3&gt;
&lt;p&gt;For a given &lt;code&gt;key&lt;/code&gt;, we use the given hash function to get the hash value, then through the directory (bucket address table) to get the corresponding bucket page. After that, we do a sequential search to find the key.&lt;/p&gt;
&lt;p&gt;Because we support duplicated keys, we need to find all the KV pairs that has the given key. Do not stop at the first matched pair.&lt;/p&gt;
&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;p&gt;The core components of insertion part is split. Highly recommend to use pen and paper to figure how bucket split works before coding.&lt;/p&gt;
&lt;p&gt;The insertion procedure is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find the right bucket&lt;/li&gt;
&lt;li&gt;If there is room in the bucket, insert the KV pair&lt;/li&gt;
&lt;li&gt;If there is no room -&amp;gt; split the bucket&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How to split one bucket? Assume we call the split target &lt;em&gt;split bucket&lt;/em&gt; and the newly created bucket &lt;em&gt;image bucket&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If $ global\_depth == local\_depth $:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increase the &lt;code&gt;global_depth&lt;/code&gt; by 1, so double the table size&lt;/li&gt;
&lt;li&gt;The following steps are same as situation $ global\_depth &amp;gt; local\_depth $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $global\_depth &amp;gt; local\_depth$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allocate a new page for the image bucket&lt;/li&gt;
&lt;li&gt;Adjust the entries in the bucket address table
&lt;ul&gt;
&lt;li&gt;leave the half of the entries pointing to the split bucket&lt;/li&gt;
&lt;li&gt;set all the remaining entries to point to the image bucket&lt;/li&gt;
&lt;li&gt;also increase the &lt;code&gt;local_depth&lt;/code&gt; by 1 because we need one more bit to separate them&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rehash KV pairs in the split bucket&lt;/li&gt;
&lt;li&gt;Re-attemp the insertion
&lt;ul&gt;
&lt;li&gt;Should use the &lt;code&gt;Insert()&lt;/code&gt; function because we may need more splits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Add your own test cases. The given test case is so small and cannot cover all situations.&lt;/p&gt;
&lt;h3 id=&#34;remove&#34;&gt;Remove&lt;/h3&gt;
&lt;p&gt;Remove itself is very simple. The complicated part is the merge procedure. The good thing is that, after finished split, the logic of merge became clear to us.&lt;/p&gt;
&lt;p&gt;The project description gives a fairly thorough instructions for merge. Follow the instruction is enough.&lt;/p&gt;
&lt;p&gt;Shrinking the table is very straightforward since we use LSB to assign KV pairs to different buckets.&lt;/p&gt;
&lt;h2 id=&#34;task-3-concurrency-control&#34;&gt;Task #3 Concurrency Control&lt;/h2&gt;
&lt;p&gt;Try coarse-grained latch (lock) first, then reduce the latch range.&lt;/p&gt;
&lt;p&gt;No special comments for this. You can do it!&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static_resources/main/img/cmu15445-project02-grades.png&#34; alt=&#34;grades&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">CMU 15-445 2021 Fall Project#1</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/projects/cmu15445_project1/" />
            <id>https://z1ggy-o.github.io/posts/projects/cmu15445_project1/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2022-03-14T21:23:00&#43;08:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.</summary>
            
                <content type="html">&lt;blockquote&gt;
&lt;p&gt;Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&#39;t think that would be very different from public the source code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;task-1---lru-replacement-policy&#34;&gt;Task #1 - LRU Replacement Policy&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManger&lt;/code&gt; contains all the frames.
&lt;code&gt;LRUReplacer&lt;/code&gt; is an implementation of the &lt;code&gt;Replacer&lt;/code&gt; and it helps &lt;code&gt;BufferPoolManger&lt;/code&gt; to manage these frames.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;LRU&lt;/code&gt; policy is not very &amp;quot;LRU&amp;quot; in my opinion. Refer the test cases we can see, if we &lt;code&gt;Unpin&lt;/code&gt; the same frame twice, we do not update the access time for this frame. Which means that we only need to inserte/delete page frames to/from the LRU container. There is no positon modification (assume we use conventional list + hash table implementation).&lt;/p&gt;
&lt;p&gt;You may need to read the code associated with task 1 and task 2 before start programming for task 1. Thus, you can understand how &lt;code&gt;BufferPoolManager&lt;/code&gt; utlizes the &lt;code&gt;LRUReplacer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Actually, it is the &lt;code&gt;BufferPoolMangerInstance&lt;/code&gt; managing the pages in the buffer. The &lt;code&gt;LRUReplacer&lt;/code&gt; itself only contains page frames that we can use for storing new pages.
In other words, the reference (pin) count of pages that existed in the frames that in the &lt;code&gt;LRUReplacer&lt;/code&gt; is zero, and we can swap them out in anytime.&lt;/p&gt;
&lt;p&gt;Since we need to handle the concurret access, latches (or locks) are necessary. If you are not fimilar with locks in C++ like me, you can check the &lt;code&gt;include/common/rwlatch.h&lt;/code&gt; to learn how Bustub (i.e., the DBMS that we are implementing) uses them.&lt;/p&gt;
&lt;h2 id=&#34;task-2---buffer-pool-manager-instance&#34;&gt;Task #2 - Buffer Pool Manager Instance&lt;/h2&gt;
&lt;p&gt;We use &lt;code&gt;Page&lt;/code&gt; as the container to manage the pages of our DB storage engine. &lt;code&gt;Page&lt;/code&gt; objects are pre-allocated for each frame in the buffer pool. We reuse existed &lt;code&gt;Page&lt;/code&gt; objects instead of creating a new one for every newly read in pages.&lt;/p&gt;
&lt;p&gt;We &lt;span class=&#34;underline&#34;&gt;pin&lt;/span&gt; a page when we want to use it, and we &lt;span class=&#34;underline&#34;&gt;unpin&lt;/span&gt; a page when we do not need it anymore. Because we are using the page, the buffer pool manager will not move this page out. Thus, &lt;span class=&#34;underline&#34;&gt;pin&lt;/span&gt; and &lt;span class=&#34;underline&#34;&gt;unpin&lt;/span&gt; are hints to tell the pool manager which page it can swap out if there is no free space.&lt;/p&gt;
&lt;p&gt;Caution, &lt;code&gt;frame&lt;/code&gt; and &lt;code&gt;page&lt;/code&gt; are refering to different concepts. &lt;code&gt;page&lt;/code&gt; is a chunk of data that stored in our DBMS; &lt;code&gt;frame&lt;/code&gt; is a slot in the page buffer that has the same size as the &lt;code&gt;page&lt;/code&gt;. So, use &lt;code&gt;frame_id_t&lt;/code&gt; and &lt;code&gt;page_id_t&lt;/code&gt; at the right place.&lt;/p&gt;
&lt;p&gt;The comments in the base code is not very clear. They use &amp;quot;page&amp;quot; to refer both data pages and page frames, which is confusing. Make sure which is the target that you want before coding.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManager&lt;/code&gt; uses four components to manage pages and frames:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;page_table_&lt;/code&gt;: a map that stores the mapping relationship between &lt;code&gt;page_id&lt;/code&gt; and &lt;code&gt;frame_id&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;free_list_&lt;/code&gt;: a linked-list that stores the free frames.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;replacer_&lt;/code&gt;: a &lt;code&gt;LRUReplacer&lt;/code&gt; that stores used frames with zero pin count.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pages_&lt;/code&gt;: stores pre-allocated &lt;code&gt;Page&lt;/code&gt; objects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of concurrency control, because we only have one latch for a whole buffer, the coarse-grained lock is unavoidable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BufferPoolManager&lt;/code&gt; is the &lt;code&gt;friend&lt;/code&gt; of &lt;code&gt;Page&lt;/code&gt;, so we can access the &lt;code&gt;private&lt;/code&gt; members of &lt;code&gt;Page&lt;/code&gt;. (This is a good example about when to use &lt;code&gt;friend&lt;/code&gt; -- when we need to change some member variables but we do not want give setters so that every one can change them.)&lt;/p&gt;
&lt;p&gt;If we can do three things right, this task is not that difficult:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move page to/from LRU.&lt;/li&gt;
&lt;li&gt;Know when to flush a page. (Read points are very clear).&lt;/li&gt;
&lt;li&gt;Which page metadata we need to update.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Critical hints:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do read the header file and make sure your return value fits the function description. (I wasted few hours just because I returned &lt;code&gt;false&lt;/code&gt; in a function, however, they assume we should return &lt;code&gt;true&lt;/code&gt;  in that case. Do not use your own judgement, just follow the description.)&lt;/li&gt;
&lt;li&gt;What will happen if we &lt;code&gt;NewPage()&lt;/code&gt; then &lt;code&gt;Unpin()&lt;/code&gt; the same page immediately?&lt;/li&gt;
&lt;li&gt;Do not use the iterator after you erased the corresponding element. The iterator is invalided, however, the compiler will not warn you.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;task-3---parallel-buffer-pool-manager&#34;&gt;Task #3 - Parallel Buffer Pool Manager&lt;/h2&gt;
&lt;p&gt;Task 3 is very straightforward. If our &lt;code&gt;BufferPoolManagerInstance&lt;/code&gt; is implemented in the right way, the only thing we need to do here is to allocate mutiple buffer instance and call corresponding member functions for each instance.&lt;/p&gt;
&lt;p&gt;Some people have problems with the start index assignment and update. Please make sure you do everything right as the describtion told us.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;Passed all test cases with full grades.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/z1ggy-o/static%5Fresources/main/img/202203142113296.png&#34; alt=&#34;Project#1 grades&#34;&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/projects/" term="projects" label="projects" />
                            
                        
                    
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/tags/dbms/" term="DBMS" label="DBMS" />
                            
                        
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/aghayev-2019-filesystemunfit/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/aghayev-2019-filesystemunfit/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2021-01-17T00:29:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary This paper mostly consists of two parts. The first part tells us why the FileStore has performance issues. And the second part tells us how Ceph team build BlueStore based on the lessons that they learnt from FileStore.
The main ideas of BlueStore are:</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;This paper mostly consists of two parts. The first part tells us why the &lt;code&gt;FileStore&lt;/code&gt; has performance issues.
And the second part tells us how Ceph team build &lt;code&gt;BlueStore&lt;/code&gt; based on the
lessons that they learnt from &lt;code&gt;FileStore&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The main ideas of &lt;code&gt;BlueStore&lt;/code&gt; are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Avoid using local file system to store and represent Ceph objects&lt;/li&gt;
&lt;li&gt;Use KV-store to provide transaction mechanism instead of build it by ourself&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-s-the-problem&#34;&gt;What&#39;s the problem&lt;/h2&gt;
&lt;p&gt;There is a software called &lt;code&gt;storage backend&lt;/code&gt; in Ceph. The &lt;code&gt;storage backend&lt;/code&gt; is
responsible to accept requests from upper layer of Ceph and do the real I/O on
storage devices.&lt;/p&gt;
&lt;p&gt;Ceph used to use commonly used local file systems based storage backend (i.e.,
FileStore). FileStore works, but not that well. There are mainly three
drawbacks in FileStore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hard to implement efficient transactions on top of existing file systems&lt;/li&gt;
&lt;li&gt;The local file system&#39; metadata performance is not great (e.g., enumerating
directories, ordering in the return result)&lt;/li&gt;
&lt;li&gt;Hard to adopt emerging storage hardware that abandon the venrable block
interface (e.g., Zone divecies)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting&#34;&gt;Why the problem is interesting&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Because storage backends do the real I/O job, the performance storage
backends domains the performance of the whole Ceph system.&lt;/li&gt;
&lt;li&gt;For years, the developers of Ceph have had a lot of troubles when using local
file systems to build storage backends&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-core-ideas&#34;&gt;The Core Ideas&lt;/h2&gt;
&lt;h3 id=&#34;the-problems-of-filestore&#34;&gt;The problems of FileStore&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transaction&lt;/strong&gt;: in Ceph, all writes are transactions. There are three options for providing transaction on top of local file systems: leveraging file system internal transaction; implementing the write-ahead logging (WAL) in user space; and using a key-value store as the WAL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;leveraging file system&#39;s transaction is not a good idea because they usually lack of some functionalities. And also, file system is in the kernel side, it is hard to expose the interfaces.&lt;/li&gt;
&lt;li&gt;User space WAL: has consistency problem (not atom operations, because it is
a logical WAL, it use read/write system calls to write/read data to/from
logging part). The cost for handle the problem is expensive. Also slow
read-modify-write and double-write (logging all data) problems.&lt;/li&gt;
&lt;li&gt;Using KV-store: This is the cure. However, there is still some unnecessary
file system overhead like &lt;em&gt;journaling of journal&lt;/em&gt; problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Slow metadata operations&lt;/strong&gt;: enumeration is slow. The read result from a object
sets should in order, which file systems do not do. We need to do sorting
after read. To reduce the sorting overhead, Ceph limits the number of files in
a directory, which introduces &lt;em&gt;directory splition&lt;/em&gt;. The dir splition has
overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Does not support new storage hardware&lt;/strong&gt;: new storage devices may need
modifications in the existing file systems. If Ceph uses local file systems,
the Ceph team can only wait for the developers of the file systems to adopt
the new storage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the paper, there are more details. In summary, the reasons of above problems are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;File system overhead&lt;/li&gt;
&lt;li&gt;Ceph uses file system metadata to represent Ceph object metadata. (i.e.,
object to file, object group to diectory) The file system metadata operations
are not fast and also may have some consistent issues.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bluestore&#34;&gt;BlueStore&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Does not use local file system anymore. Instead, store Ceph objects into raw
storage directly. This method avoids the overhead of file systems.&lt;/li&gt;
&lt;li&gt;Use KV-store (i.e. RocksDB) to provide transaction and manage Ceph metadata.
This method provides much faster metadata operations and also avoid building
transtion mechanism by Ceph developer.&lt;/li&gt;
&lt;li&gt;Because RocksDB runs on top of file systems. BlueStore has a very simply file
system that only works for RocksDB called BlueFS. The BlueFS stores all the
contents in logging space and cleans invalid data periodly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you understand the reason of why FileStore performs not well, you can simply
understand the choices they did when build BlueStore.&lt;/p&gt;
&lt;p&gt;BlueStore still has some issues. For example, because BlueStore do not use file
systems, it cannot leverage the OS page cache and need to build the cache by
itself. However, build a effective cache is hard.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/wang-2020-bcw/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/wang-2020-bcw/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2020-12-31T22:37:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.
The authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full.</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.&lt;/p&gt;
&lt;p&gt;The authors of this paper find that the write requests can have $μ$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer.&lt;/p&gt;
&lt;p&gt;This strategy can reduce SSD pressure which prolongs SSD life and still provide relative good
performance.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-problem&#34;&gt;What is the problem&lt;/h2&gt;
&lt;p&gt;In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.&lt;/p&gt;
&lt;p&gt;More specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is
moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer
requests first, so consumer requests can have shorter latency.&lt;/p&gt;
&lt;p&gt;However, in a real system, SSDs handle most of the write requests and HDDs are idle in more
than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation.
Also, deep queue depth makes requests suffering long latency even when we using SSDs.&lt;/p&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting--important&#34;&gt;Why the problem is interesting (important)?&lt;/h2&gt;
&lt;p&gt;The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer.
The request latency of HDD can be classified as three categories: &lt;em&gt;fast, middle&lt;/em&gt;, and &lt;em&gt;slow&lt;/em&gt;.
Write requests data is put to the buffer first, then to the disk. When the buffer is full,
HDD will block the coming requests until it flushes all the data in the buffer into disk.
When there are free space in the buffer, request latency is in fast or middle range, otherwise
in slow range.&lt;/p&gt;
&lt;p&gt;The fast and middle latency is in $μ s$-level which similar with the performance of SSD.
If we can control the buffer in disk to handle requests which their size is in the buffer
size range, then we can get SSD-level performance when using HDD to handle small write
requests.&lt;/p&gt;
&lt;h2 id=&#34;the-idea&#34;&gt;The idea&lt;/h2&gt;
&lt;p&gt;Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also
provides reasonable performance.&lt;/p&gt;
&lt;p&gt;To achieve the goal, there are two key components in this paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure requests to HDD are in the fast and middle latency range&lt;/li&gt;
&lt;li&gt;Determining which write requests should be dispatch to HDD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To handle the first challenge, the authors provided a prediction model. The model itself
is simply comparing the current request size with pre-defined threshold.
We cannot know the write buffer size of HDD directly. However, we can get an approximate
value of the buffer size through profiling. The threshold are the cumulative amount of written data for the
fast/mid/slow stages.&lt;/p&gt;
&lt;p&gt;Since we only want to use the fast and middle stages, we need to skip the slow stage.
There are two methods to do this. First, &lt;code&gt;sync&lt;/code&gt; system call from host can enforce the
buffer flush; second, HDD controller will flush the buffer when the buffer is full.
&lt;code&gt;sync&lt;/code&gt; is a expensive operation, so the authors choose to use &lt;em&gt;padding data&lt;/em&gt; to full fill
the buffer, which can let controller to flush the data in the buffer.&lt;/p&gt;
&lt;p&gt;The second reason of why we need padding data is we want to make sure the prediction model
working well. That means the prediction model needs a sequential continuous write requests.
When HDD is idle, the controller will empty the buffer even when the buffer is not full,
which break the prediction. Read requests also break the prediction.
Using padding data can help the system to maintain and adjust the prediction.
More specifically, when HDD is idle, the system use small size padding data to avoid disk
control flush the buffer; when read requests finished, since we cannot know if the disk
controller flushes the buffer, the system use large size padding data to quickly full fill
the buffer, which can help recorrect the prediction model.
These padding data will be remove during the GC procedure.&lt;/p&gt;
&lt;p&gt;Steering requests to HDDs is much easier to understand. The latency of request is related
to the I/O queue depth.
We do profiling to find the relation between SSD&#39;s queue depth and the request request latency.
In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs
fast stage. We use the queue depth value as the threshold.
When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.&lt;/p&gt;
&lt;h2 id=&#34;drawbacks-and-personal-questions-about-the-study&#34;&gt;Drawbacks and personal questions about the study&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Only works for small size of write requests&lt;/li&gt;
&lt;li&gt;The consistency is not guaranteed&lt;/li&gt;
&lt;li&gt;The disk cannot be managed as RAID (can we?)&lt;/li&gt;
&lt;li&gt;GC is still a problem&lt;/li&gt;
&lt;/ul&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Note: Read as Needed: Building WiSER, a Flash-Optimized Search Engine</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/paper-notes/he-2020-readasneeded/" />
            <id>https://z1ggy-o.github.io/posts/paper-notes/he-2020-readasneeded/</id>
            <updated>2022-05-23T17:14:14&#43;08:00</updated>
            <published>2020-12-26T20:15:00&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Short Summary This paper proposed a NAND-flash SSD-friendly full text engine. This engine can achieve better performance than existing engines with much less memory requested.
They reduce the unnecessary I/O (both the number of I/O and the volume). The engine does not cache data into memory, instead, read data every time when query arrive.</summary>
            
                <content type="html">&lt;h2 id=&#34;short-summary&#34;&gt;Short Summary&lt;/h2&gt;
&lt;p&gt;This paper proposed a NAND-flash SSD-friendly full text engine. This engine can
achieve better performance than existing engines with much less memory requested.&lt;/p&gt;
&lt;p&gt;They reduce the unnecessary I/O (both the number of I/O and the volume). The
engine does not cache data into memory, instead, read data every time when query
arrive.&lt;/p&gt;
&lt;p&gt;They also tried to increase the request size to exploit SSD internal parallelism.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-problem&#34;&gt;What Is the Problem&lt;/h2&gt;
&lt;p&gt;Search engines pose great challenges to storage systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;low latency&lt;/li&gt;
&lt;li&gt;high data throughput&lt;/li&gt;
&lt;li&gt;high scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The datasets become too large to fit into the RAM. Simply use RAM as a cache
cannot achieve the goal.&lt;/p&gt;
&lt;p&gt;SSD and NVRAM can boost performance well. For example, flash-based SSDs provide
much higher throughput and lower latency compared to HDD.
However, since SSDs exhibit vastly different characteristic from HDDs, we need
to evolve the software on top of the storage stack to exploit the full potential
of SSDs.&lt;/p&gt;
&lt;p&gt;In this paper, the authors rebuild a search engine to better utilize SSDs to
achieve the necessary performance goals with main memory that is significantly
smaller than the data set.&lt;/p&gt;
&lt;h2 id=&#34;why-the-problem-is-interesting&#34;&gt;Why the Problem Is Interesting&lt;/h2&gt;
&lt;p&gt;There are already many studies that work on making SSD-friendly softwares within storage stack. For example, RocksDB, Wisckey (KV-store); FlashGraph, Mosaic (graphs processing); and SFS, F2fS (file system).&lt;/p&gt;
&lt;p&gt;However, there is no optimization for full-text search engines.
Also the challenge of making SSD-friendly search engine is different with other categories of SSD-friendly softwares.&lt;/p&gt;
&lt;h2 id=&#34;the-idea&#34;&gt;The Idea&lt;/h2&gt;
&lt;p&gt;The key idea is: &lt;strong&gt;read as needed&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The reason behind of the idea is SSD can provide millisecond-level read latency,
which is fast enough to avoid cache data into main memory.&lt;/p&gt;
&lt;p&gt;There are three challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;reduce read amplification&lt;/li&gt;
&lt;li&gt;hide I/O latency&lt;/li&gt;
&lt;li&gt;issue large requests to exploit SSD performance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(This work is focus on the data structure, inverted index. This is a commonly used data structure in the information retrieve system. There are some brief introduction about the inverted index in the paper, and I do not repeat the content here.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Techniques&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Reduce read amplification&lt;/td&gt;
&lt;td&gt;- cross-stage data grouping&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;- two-way cost-aware bloom filters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;- trade disk space for I/O&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hide I/O latency&lt;/td&gt;
&lt;td&gt;adaptive prefetching&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Issue large requests&lt;/td&gt;
&lt;td&gt;cross-stage data grouping&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;cross-stage-data-grouping&#34;&gt;Cross-stage data grouping&lt;/h3&gt;
&lt;p&gt;This technique is used to reduce read amplification and issue large requests.&lt;/p&gt;
&lt;p&gt;WiSER puts data needed for different stages of a query into continuous and compact blocks on the storage device, which increases block utilization when transferring data for query.
Inverted index of WiSER places data of different stages in the order that it will be accessed.&lt;/p&gt;
&lt;p&gt;Previous search engines used to store data of different stages into different range, which reduces the I/O size and increases the number of I/O requests.&lt;/p&gt;
&lt;h3 id=&#34;two-way-cost-aware-filters&#34;&gt;Two-way Cost-aware Filters&lt;/h3&gt;
&lt;p&gt;When doing phrase queries we need to use the position information to check the terms around a specific term to improve search precision.&lt;/p&gt;
&lt;p&gt;The naive approach is to read the positions from all the terms in the phrase
then iterate the position list.
To reduce the unnecessary reading, WiSER employs a bitmap-based bloom filter.
The reason to use bitmap is to reduce the size of bloom filter. There are many
empty entries in the filter array, use bitmap can avoid the waste.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cost-aware&lt;/em&gt; means comparing the size of position list with that of the bloom
filters. If the size of position list is smaller than that of bloom filters,
WiSER reads the position list directly.&lt;/p&gt;
&lt;p&gt;Two-way filters shares the same idea. WiSER chooses to read the smaller bloom
filter to reduce the read amplification.&lt;/p&gt;
&lt;h3 id=&#34;adaptive-prefetching&#34;&gt;Adaptive Prefetching&lt;/h3&gt;
&lt;p&gt;Prefetching is one of the commonly used technique to hide the I/O latency.
Even though, the read latency of SSD is small. Compare to DRAM, the read latency
of SSD still much larger.&lt;/p&gt;
&lt;p&gt;Previous search engines (e.g. Elasticsearch) use fix-sized prefecthing (i.e.
Linux readahead) which increases the read amplification.
WiSER defines an area called &lt;em&gt;prefetch zone&lt;/em&gt;. A prefetch zone is further divided
into prefetch segments to avoid accessing too much data at a time. It prefetches when all prefetch zones involved in a query are larger than a threshold.&lt;/p&gt;
&lt;p&gt;To enable adaptive prefetch, WiSER hides the size of the prefetch zone in the highest 16 bits of the offset in TermMap and calls &lt;code&gt;madvise()&lt;/code&gt; with the &lt;code&gt;MADV_SEQUENTIAL&lt;/code&gt; hint to readahead in the prefetch zone.&lt;/p&gt;
&lt;h3 id=&#34;trade-disk-space-for-i-o&#34;&gt;Trade Disk Space for I/O&lt;/h3&gt;
&lt;p&gt;Engines like Elasticsearch put documents into a buffer and compresses all data in the buffer together. WiSER, instead, compresses each document by themselves.&lt;/p&gt;
&lt;p&gt;Compressing all data in the buffer together achieves better compression.
However, decompressing a document requires reading and decompressing all documents compressed before the document, leading to more I/O and computation. WiSER trades space for less I/O by using more space but reducing the I/O while processing queries.&lt;/p&gt;
&lt;p&gt;In the evaluation, WiSER uses 25% more storage than Elasticsearch. The authors argue that the trade-off is acceptable in spite of the low RAM requirement of WiSER.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://z1ggy-o.github.io/categories/paper-notes/" term="paper notes" label="paper notes" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">My First Post</title>
            <link rel="alternate" type="text/html" href="https://z1ggy-o.github.io/posts/articles/my-first-post/" />
            <id>https://z1ggy-o.github.io/posts/articles/my-first-post/</id>
            <updated>2022-05-21T14:20:11&#43;08:00</updated>
            <published>2020-11-08T09:48:14&#43;09:00</published>
            <author>
                    <name>Ziggy</name>
                    <uri>https:github.z1ggy-o.io</uri>
                    <email>gy.zhu29@outlook.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">I rebuilt my personal blog by using hugo.
I have cleaned all my old blogs (not so much actually).
I will write something related to my research reading or personal learning. Hope I can meet you soon.</summary>
            
                <content type="html">&lt;p&gt;I rebuilt my personal blog by using hugo.&lt;/p&gt;
&lt;p&gt;I have cleaned all my old blogs (not so much actually).&lt;/p&gt;
&lt;p&gt;I will write something related to my research reading or personal learning.
Hope I can meet you soon.&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>
