<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server | Gy's Blog</title>
<meta name=keywords content><meta name=description content="Short Summary
HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.
The authors of this paper find that the write requests can have $μ$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer."><meta name=author content="Me"><link rel=canonical href=https://z1ggy-o.github.io/papers/wang-2020-bcw/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://z1ggy-o.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://z1ggy-o.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://z1ggy-o.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://z1ggy-o.github.io/apple-touch-icon.png><link rel=mask-icon href=https://z1ggy-o.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://z1ggy-o.github.io/papers/wang-2020-bcw/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://z1ggy-o.github.io/papers/wang-2020-bcw/"><meta property="og:site_name" content="Gy's Blog"><meta property="og:title" content="FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server"><meta property="og:description" content="Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.
The authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="papers"><meta property="article:published_time" content="2023-09-17T12:51:01+00:00"><meta property="article:modified_time" content="2024-12-28T18:26:22+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server"><meta name=twitter:description content="Short Summary
HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.
The authors of this paper find that the write requests can have $μ$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Papers","item":"https://z1ggy-o.github.io/papers/"},{"@type":"ListItem","position":2,"name":"FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","item":"https://z1ggy-o.github.io/papers/wang-2020-bcw/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","name":"FAST\u002720 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server","description":"Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.\nThe authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer.\n","keywords":[],"articleBody":"Short Summary HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of the requests. This shorts the life of SSDs and also wants the utilization of HDDs.\nThe authors of this paper find that the write requests can have $μ$s-level latency when using HDD if the buffer in HDD is not full. They leverage this finding to let HDD to handle write requests if the requests can fit into the in disk buffer.\nThis strategy can reduce SSD pressure which prolongs SSD life and still provide relative good performance.\nWhat is the problem In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.\nMore specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer requests first, so consumer requests can have shorter latency.\nHowever, in a real system, SSDs handle most of the write requests and HDDs are idle in more than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation. Also, deep queue depth makes requests suffering long latency even when we using SSDs.\nWhy the problem is interesting (important)? The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer. The request latency of HDD can be classified as three categories: fast, middle, and slow. Write requests data is put to the buffer first, then to the disk. When the buffer is full, HDD will block the coming requests until it flushes all the data in the buffer into disk. When there are free space in the buffer, request latency is in fast or middle range, otherwise in slow range.\nThe fast and middle latency is in $μ s$-level which similar with the performance of SSD. If we can control the buffer in disk to handle requests which their size is in the buffer size range, then we can get SSD-level performance when using HDD to handle small write requests.\nThe idea Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also provides reasonable performance.\nTo achieve the goal, there are two key components in this paper:\nMake sure requests to HDD are in the fast and middle latency range Determining which write requests should be dispatch to HDD To handle the first challenge, the authors provided a prediction model. The model itself is simply comparing the current request size with pre-defined threshold. We cannot know the write buffer size of HDD directly. However, we can get an approximate value of the buffer size through profiling. The threshold are the cumulative amount of written data for the fast/mid/slow stages.\nSince we only want to use the fast and middle stages, we need to skip the slow stage. There are two methods to do this. First, sync system call from host can enforce the buffer flush; second, HDD controller will flush the buffer when the buffer is full. sync is a expensive operation, so the authors choose to use padding data to full fill the buffer, which can let controller to flush the data in the buffer.\nThe second reason of why we need padding data is we want to make sure the prediction model working well. That means the prediction model needs a sequential continuous write requests. When HDD is idle, the controller will empty the buffer even when the buffer is not full, which break the prediction. Read requests also break the prediction. Using padding data can help the system to maintain and adjust the prediction. More specifically, when HDD is idle, the system use small size padding data to avoid disk control flush the buffer; when read requests finished, since we cannot know if the disk controller flushes the buffer, the system use large size padding data to quickly full fill the buffer, which can help recorrect the prediction model. These padding data will be remove during the GC procedure.\nSteering requests to HDDs is much easier to understand. The latency of request is related to the I/O queue depth. We do profiling to find the relation between SSD’s queue depth and the request request latency. In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs fast stage. We use the queue depth value as the threshold. When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.\nDrawbacks and personal questions about the study Only works for small size of write requests The consistency is not guaranteed The disk cannot be managed as RAID (can we?) GC is still a problem ","wordCount":"797","inLanguage":"en","datePublished":"2023-09-17T12:51:01Z","dateModified":"2024-12-28T18:26:22+08:00","author":[{"@type":"Person","name":"Me"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://z1ggy-o.github.io/papers/wang-2020-bcw/"},"publisher":{"@type":"Organization","name":"Gy's Blog","logo":{"@type":"ImageObject","url":"https://z1ggy-o.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://z1ggy-o.github.io/ accesskey=h title="Gy's Blog (Alt + H)">Gy's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://z1ggy-o.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://z1ggy-o.github.io/weekly/ title=Logs><span>Logs</span></a></li><li><a href=https://z1ggy-o.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://z1ggy-o.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://z1ggy-o.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://z1ggy-o.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://z1ggy-o.github.io/papers/>Papers</a></div><h1 class="post-title entry-hint-parent">FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</h1><div class=post-meta><span title='2023-09-17 12:51:01 +0000 UTC'>September 17, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Me</div></header><div class=post-content><h2 id=short-summary>Short Summary<a hidden class=anchor aria-hidden=true href=#short-summary>#</a></h2><p>HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.</p><p>The authors of this paper find that the write requests can have $μ$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer.</p><p>This strategy can reduce SSD pressure which prolongs SSD life and still provide relative good
performance.</p><h2 id=what-is-the-problem>What is the problem<a hidden class=anchor aria-hidden=true href=#what-is-the-problem>#</a></h2><p>In hybrid storage system (e.g. SSDs with HDDs), there is an unbalancing storage utilization problem.</p><p>More specifically, SSDs are used as the cache layer of the whole system, then data in SSDs is
moved to HDDs. The original idea is to leverage the high performance of SSD to serve consumer
requests first, so consumer requests can have shorter latency.</p><p>However, in a real system, SSDs handle most of the write requests and HDDs are idle in more
than 90% of the time. This is a waste of HDD and SSD because SSD has short life limitation.
Also, deep queue depth makes requests suffering long latency even when we using SSDs.</p><h2 id=why-the-problem-is-interesting--important>Why the problem is interesting (important)?<a hidden class=anchor aria-hidden=true href=#why-the-problem-is-interesting--important>#</a></h2><p>The authors find a latency pattern of requests on HDD, which is introduced by the using of the in disk buffer.
The request latency of HDD can be classified as three categories: <em>fast, middle</em>, and <em>slow</em>.
Write requests data is put to the buffer first, then to the disk. When the buffer is full,
HDD will block the coming requests until it flushes all the data in the buffer into disk.
When there are free space in the buffer, request latency is in fast or middle range, otherwise
in slow range.</p><p>The fast and middle latency is in $μ s$-level which similar with the performance of SSD.
If we can control the buffer in disk to handle requests which their size is in the buffer
size range, then we can get SSD-level performance when using HDD to handle small write
requests.</p><h2 id=the-idea>The idea<a hidden class=anchor aria-hidden=true href=#the-idea>#</a></h2><p>Dynamically dispatch write requests to SSD and HDD, which reduces SSD pressure and also
provides reasonable performance.</p><p>To achieve the goal, there are two key components in this paper:</p><ul><li>Make sure requests to HDD are in the fast and middle latency range</li><li>Determining which write requests should be dispatch to HDD</li></ul><p>To handle the first challenge, the authors provided a prediction model. The model itself
is simply comparing the current request size with pre-defined threshold.
We cannot know the write buffer size of HDD directly. However, we can get an approximate
value of the buffer size through profiling. The threshold are the cumulative amount of written data for the
fast/mid/slow stages.</p><p>Since we only want to use the fast and middle stages, we need to skip the slow stage.
There are two methods to do this. First, <code>sync</code> system call from host can enforce the
buffer flush; second, HDD controller will flush the buffer when the buffer is full.
<code>sync</code> is a expensive operation, so the authors choose to use <em>padding data</em> to full fill
the buffer, which can let controller to flush the data in the buffer.</p><p>The second reason of why we need padding data is we want to make sure the prediction model
working well. That means the prediction model needs a sequential continuous write requests.
When HDD is idle, the controller will empty the buffer even when the buffer is not full,
which break the prediction. Read requests also break the prediction.
Using padding data can help the system to maintain and adjust the prediction.
More specifically, when HDD is idle, the system use small size padding data to avoid disk
control flush the buffer; when read requests finished, since we cannot know if the disk
controller flushes the buffer, the system use large size padding data to quickly full fill
the buffer, which can help recorrect the prediction model.
These padding data will be remove during the GC procedure.</p><p>Steering requests to HDDs is much easier to understand. The latency of request is related
to the I/O queue depth.
We do profiling to find the relation between SSD&rsquo;s queue depth and the request request latency.
In a certain queue depth, the request latency on SSD will be greater than the latency of HDDs
fast stage. We use the queue depth value as the threshold.
When queue depth exceeds the threshold, the system stores data to the HDD instead of SSD.</p><h2 id=drawbacks-and-personal-questions-about-the-study>Drawbacks and personal questions about the study<a hidden class=anchor aria-hidden=true href=#drawbacks-and-personal-questions-about-the-study>#</a></h2><ul><li>Only works for small size of write requests</li><li>The consistency is not guaranteed</li><li>The disk cannot be managed as RAID (can we?)</li><li>GC is still a problem</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://z1ggy-o.github.io/>Gy's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>