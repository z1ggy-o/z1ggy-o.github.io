<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SOSP'97 - Frangipani: a scalable distributed file system | Gy's Blog</title>
<meta name=keywords content="distributed-systems"><meta name=description content="Motivation

File system administration for a large, growing computer installation is a laborious task.
A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.

Contribution

A real system that shows us how to do cache coherence and distributed transactions.

Solution

The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.
Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.
All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.
Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)
Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.
The underlying distributed storage and the lock server are both using Paxos to ensure availability.

Evaluation

They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.
They tested both the local performance and the scalability of the proposed system.

The Main Finding of the Paper

Complex clients sharing simple storage can have better scalability.
Distributed lock can be used to achieve cache coherence and distributed transaction.
"><meta name=author content="map[link:https://github.com/z1ggy-o name:gyzhu]"><link rel=canonical href=https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://z1ggy-o.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://z1ggy-o.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://z1ggy-o.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://z1ggy-o.github.io/apple-touch-icon.png><link rel=mask-icon href=https://z1ggy-o.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:url" content="https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/"><meta property="og:site_name" content="Gy's Blog"><meta property="og:title" content="SOSP'97 - Frangipani: a scalable distributed file system"><meta property="og:description" content="Motivation File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution A real system that shows us how to do cache coherence and distributed transactions. Solution The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache. Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers. All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured. Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.) Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily. The underlying distributed storage and the lock server are both using Paxos to ensure availability. Evaluation They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c. They tested both the local performance and the scalability of the proposed system. The Main Finding of the Paper Complex clients sharing simple storage can have better scalability. Distributed lock can be used to achieve cache coherence and distributed transaction. "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="papers"><meta property="article:published_time" content="2022-07-01T13:30:21+00:00"><meta property="article:modified_time" content="2024-12-28T14:09:27+08:00"><meta property="article:tag" content="Distributed-Systems"><meta name=twitter:card content="summary"><meta name=twitter:title content="SOSP'97 - Frangipani: a scalable distributed file system"><meta name=twitter:description content="Motivation

File system administration for a large, growing computer installation is a laborious task.
A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.

Contribution

A real system that shows us how to do cache coherence and distributed transactions.

Solution

The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.
Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.
All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.
Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)
Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.
The underlying distributed storage and the lock server are both using Paxos to ensure availability.

Evaluation

They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.
They tested both the local performance and the scalability of the proposed system.

The Main Finding of the Paper

Complex clients sharing simple storage can have better scalability.
Distributed lock can be used to achieve cache coherence and distributed transaction.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Papers","item":"https://z1ggy-o.github.io/papers/"},{"@type":"ListItem","position":2,"name":"SOSP'97 - Frangipani: a scalable distributed file system","item":"https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SOSP'97 - Frangipani: a scalable distributed file system","name":"SOSP\u002797 - Frangipani: a scalable distributed file system","description":"Motivation File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution A real system that shows us how to do cache coherence and distributed transactions. Solution The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache. Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers. All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured. Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.) Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily. The underlying distributed storage and the lock server are both using Paxos to ensure availability. Evaluation They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c. They tested both the local performance and the scalability of the proposed system. The Main Finding of the Paper Complex clients sharing simple storage can have better scalability. Distributed lock can be used to achieve cache coherence and distributed transaction. ","keywords":["distributed-systems"],"articleBody":"Motivation File system administration for a large, growing computer installation is a laborious task. A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want. Contribution A real system that shows us how to do cache coherence and distributed transactions. Solution The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache. Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers. All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured. Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.) Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily. The underlying distributed storage and the lock server are both using Paxos to ensure availability. Evaluation They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c. They tested both the local performance and the scalability of the proposed system. The Main Finding of the Paper Complex clients sharing simple storage can have better scalability. Distributed lock can be used to achieve cache coherence and distributed transaction. ","wordCount":"273","inLanguage":"en","datePublished":"2022-07-01T13:30:21Z","dateModified":"2024-12-28T14:09:27+08:00","author":{"@type":"Person","name":{"link":"https://github.com/z1ggy-o","name":"gyzhu"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/"},"publisher":{"@type":"Organization","name":"Gy's Blog","logo":{"@type":"ImageObject","url":"https://z1ggy-o.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://z1ggy-o.github.io/ accesskey=h title="Gy's Blog (Alt + H)">Gy's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://z1ggy-o.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://z1ggy-o.github.io/weekly/ title=Logs><span>Logs</span></a></li><li><a href=https://z1ggy-o.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://z1ggy-o.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://z1ggy-o.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://z1ggy-o.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://z1ggy-o.github.io/papers/>Papers</a></div><h1 class="post-title entry-hint-parent">SOSP'97 - Frangipani: a scalable distributed file system</h1><div class=post-meta><span title='2022-07-01 13:30:21 +0000 UTC'>July 1, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;map[link:https://github.com/z1ggy-o name:gyzhu]</div></header><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><ul><li>File system administration for a large, growing computer installation is a laborious task.</li><li>A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.</li></ul><h2 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h2><ul><li>A real system that shows us how to do cache coherence and distributed transactions.</li></ul><h2 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h2><ul><li>The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.</li><li>Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.</li><li>All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.</li><li>Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)</li><li>Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.</li><li>The underlying distributed storage and the lock server are both using Paxos to ensure availability.</li></ul><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><ul><li>They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.</li><li>They tested both the local performance and the scalability of the proposed system.</li></ul><h2 id=the-main-finding-of-the-paper>The Main Finding of the Paper<a hidden class=anchor aria-hidden=true href=#the-main-finding-of-the-paper>#</a></h2><ul><li>Complex clients sharing simple storage can have better scalability.</li><li>Distributed lock can be used to achieve cache coherence and distributed transaction.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://z1ggy-o.github.io/tags/distributed-systems/>Distributed-Systems</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://z1ggy-o.github.io/>Gy's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>