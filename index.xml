<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Gy's Blog</title><link>https://z1ggy-o.github.io/</link><description>Recent content on Gy's Blog</description><generator>Hugo -- 0.140.1</generator><language>en-us</language><lastBuildDate>Sat, 28 Dec 2024 15:58:20 +0000</lastBuildDate><atom:link href="https://z1ggy-o.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>30å²äººç”Ÿå¼€æŒ‚7æ­¥æ³• -- è‡ªé’</title><link>https://z1ggy-o.github.io/books/30%E5%B2%81%E4%BA%BA%E7%94%9F%E5%BC%80%E6%8C%827%E6%AD%A5%E6%B3%95-%E8%87%AA%E9%9D%92/</link><pubDate>Sat, 28 Dec 2024 23:39:12 +0800</pubDate><guid>https://z1ggy-o.github.io/books/30%E5%B2%81%E4%BA%BA%E7%94%9F%E5%BC%80%E6%8C%827%E6%AD%A5%E6%B3%95-%E8%87%AA%E9%9D%92/</guid><description>&lt;blockquote>
&lt;p>æ ‡é¢˜: 30å²äººç”Ÿå¼€æŒ‚7æ­¥æ³•: æ²¡èµ„æºã€æ²¡èƒŒæ™¯çš„90åå®…ç”·æ¯æœˆè·åˆ©50ä¸‡å…ƒçš„ç®€å•æ–¹æ³•&lt;/p>
&lt;p>ä½œè€…: è‡ªé’ï¼ˆéŸ©ï¼‰&lt;/p>
&lt;/blockquote>
&lt;p>è¿™æ˜¯ä¸€æœ¬éŸ©å›½çš„ç•…é”€ä¹¦ã€‚ç•…é”€ä¹¦æœ¬æ¥ä¸€èˆ¬éƒ½æ˜¯æ¯”è¾ƒå¥—è·¯çš„å†™æ³•ï¼Œæµ“ç¼©äº†å…¶ä»–ä¸€äº›ç»å…¸ä¹¦ç±çš„åè¨€ã€‚è¿™æœ¬ä¹¦ä¹Ÿä¸åˆ—å¤–ï¼Œä½†æˆ‘è¿˜æ˜¯æŠŠå®ƒè¯»å®Œå¹¶ä¸”è®°å½•ä¸‹æ¥æœ‰ä¸¤ä¸ªåŸå› ã€‚ä¸€ä¸ªæ˜¯æœ€è¿‘ç”Ÿæ´»ä¸åœ¨çŠ¶æ€ï¼Œéœ€è¦ä¸€äº›æ­¤ç±»ä¹¦ç±æ¥æ‰“æ‰“é¸¡è¡€ï¼›äºŒæ˜¯æœ¬ä¹¦çš„ä½œè€…å¾ˆè¯šæ³ï¼Œä¸ç©ºæ´ã€‚ä»–ç”¨è‡ªå·±çš„æ•…äº‹ä½œä¸ºæ¿€åŠ±ï¼Œä»å¿ƒç†å’Œè¡ŒåŠ¨ä¸Šç»™å‡ºäº†å‡ ä¸ªç«‹å³å¯è¡Œçš„æ–¹æ¡ˆï¼Œé¼“åŠ±å¤§å®¶éƒ½ç«‹åˆ»åŠ¨èµ·æ¥ã€‚&lt;/p>
&lt;h2 id="what-i-learned">What I learned&lt;/h2>
&lt;ul>
&lt;li>è‡ªæˆ‘æ„è¯†æ˜¯æŒ‡æˆ‘ä»¬å¯¹è‡ªå·±èº«ä½“ï¼Œæƒ…æ„Ÿï¼Œè¡Œä¸ºä»¥äºå¤–ç•Œå…³ç³»çš„ç†è§£ã€‚è‡ªæˆ‘æ„è¯†æ˜¯é«˜é˜¶çš„æ€ç»´ï¼Œå¸®åŠ©æˆ‘ä»¬ç»´æŒè‡ªå·±å­˜åœ¨çš„åˆç†æ€§ã€‚ä½†è¿‡å‰©çš„è‡ªæˆ‘æ„è¯†å°±ä¼šå¯¼è‡´è‡ªè´Ÿæˆ–è€…è‡ªå‘ï¼Œåè€Œé˜»æ‹¦æˆ‘ä»¬è·å¾—è‡ªç”±ã€‚è¿›åŒ–çš„ç›®çš„ä¸æ˜¯å®Œç¾ï¼Œè€Œæ˜¯ç”Ÿå­˜ã€‚æˆ‘ä»¬çš„è¿›åŒ–å°±åƒæ˜¯ kluge æœºå™¨ï¼Œæ˜¯åœ¨æ—§çš„ç³»ç»Ÿä¸Šæ‰“è¡¥ä¸ï¼Œéª¨å­é‡Œçš„å¾ˆå¤šæœ¬èƒ½ååº”åœ¨å½“ä»Šç¤¾ä¼šéƒ½å¹¶ä¸èƒ½å¸¦æ¥æœ€ä¼˜çš„ç»“æœï¼Œæ¯”å¦‚æˆ‘ä»¬çš„ææƒ§å’Œå¿§è™‘å¿ƒç†ã€‚è¦é€šè¿‡è§‚å¯Ÿå’Œåæ€æ¥é¿å…è¿™äº›æœ¬èƒ½ååº”æ‰€å¸¦æ¥çš„ä¸è‰¯ç»“æœã€‚&lt;/li>
&lt;li>å¡‘é€ èº«ä»½è®¤åŒã€‚æ¢å¥è¯æ¥è¯´å°±æ˜¯è‡ªä¿¡ï¼Œç›¸ä¿¡æˆ‘ä»¬èƒ½å¤Ÿè¾¾æˆæˆ‘ä»¬æƒ³è¦è¾¾æˆçš„ä»»ä½•ç›®æ ‡ã€‚å®é™…è¡ŒåŠ¨æ‰èƒ½å¸¦æ¥å®è´¨æ”¹å˜ã€‚èº«ä»½è®¤åŒæ˜¯ä¸€ç§ç‡ƒæ–™ï¼Œè®©æˆ‘ä»¬æ•¢æƒ³æ•¢åšã€‚æ„å»ºèº«ä»½è®¤åŒæœ€å¥½çš„æ–¹å¼å°±æ˜¯åˆ©ç”¨ç¯å¢ƒï¼Œå› ä¸ºè¾“å…¥å½±å“è¾“å…¥ï¼Œå½“æˆ‘ä»¬æŠŠè‡ªå·±ä¸¢åˆ°é€‚å½“çš„ç¾¤ä½“ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„ç¾¤ä½“æ½œæ„è¯†ä¼šè‡ªç„¶è€Œç„¶åœ°å¼•å¯¼æˆ‘ä»¬è¿›è¡Œæ”¹å˜&lt;/li>
&lt;li>è®­ç»ƒå¤§è„‘å¸¦æ¥çš„æ•ˆç›Šæ˜¯æœ‰å¤åˆ©æ•ˆåº”çš„ï¼Œå› ä¸ºå¤§è„‘èƒ½åŠ›çš„æå‡æ˜¯åŸºæœ¬èƒ½åŠ›çš„æå‡ï¼Œä¸€ä¸ªé¢†åŸŸä¸­çš„çŸ¥è¯†æœ‰å¯èƒ½è¿ç§»åˆ°å¦å¤–çš„é¢†åŸŸï¼Œå¸®åŠ©å…¶ä»–é¢†åŸŸå‘å±•å¾—æ›´å¿«æ›´å¥½ã€‚åšæŒæ¯å¤©ä¸¤ä¸ªå°æ—¶çš„è¯»å†™å°±èƒ½è§åˆ°æ„æƒ³ä¸åˆ°çš„æˆé•¿ã€‚&lt;/li>
&lt;li>æˆåŠŸè€…çš„å·¥å…·ï¼šè‚¯ä»˜å‡ºçš„äººæ‰èƒ½æ›´åˆ°æ›´å¤šçš„å›æŠ¥ï¼Œå› ä¸ºä»–ä»¬é‡è§†ä»·å€¼ã€‚ç†æ€§å†³ç­–ï¼ŒåªæŠ¼æ³¨æ¦‚ç‡ã€‚ä¸è¦è¿½æ±‚å®Œç¾ï¼Œè€Œæ˜¯åº”è¯¥åˆ©ç”¨28åŸåˆ™ï¼ŒæŠ“ä½æ›´å¤šæ–¹é¢çš„ 80%ï¼Œå¹¶åˆ©ç”¨è¿™äº›å¤šæ–¹é¢çš„ç»„åˆæ¥è·å¾—æ›´å¤§çš„å›æŠ¥ã€‚æå‡å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œé¿å…ä¸»è§‚åˆ¤æ–­ã€‚æ‰§è¡Œï¼Œæ‰§è¡Œï¼Œæ‰§è¡Œï¼Œè¿˜æ˜¯æ‰§è¡Œï¼ç»å¤§å¤šæ•°äººéƒ½æ˜¯åªæƒ³ä¸åšã€‚&lt;/li>
&lt;/ul>
&lt;h2 id="what-i-can-do">What I Can Do&lt;/h2>
&lt;ul>
&lt;li>è®­ç»ƒè‡ªå·±åœ¨å¯¹å¤–ååº”ä¹‹å‰åœé¡¿ä¸€ä¸‹ï¼Œç¡®è®¤è‡ªå·±çš„å›åº”æ¥è‡ªç†æ€§çš„æ€è€ƒè€Œä¸æ˜¯è‡ªæˆ‘æ„è¯†è‡ªç„¶çš„ååº”ã€‚ã€Šæ¸…æ™°æ€è€ƒã€‹ä¸€ä¹¦é‡Œæè¿‡çš„ä¸€ä¸ªæ–¹æ³•æ˜¯ï¼Œå…»æˆåœ¨å›åº”å‰æ·±å¸ä¸€å£æ°”çš„ä¹ æƒ¯ã€‚&lt;/li>
&lt;li>é€šè¿‡åŠ å…¥åœˆå­çš„æ–¹å¼æ¥å¸®åŠ©è‡ªå·±å¡‘é€ èº«ä»½è®¤åŒï¼Œå¢å¼ºæˆ‘ä»¬çš„æ‰§è¡ŒåŠ›ã€‚å¦‚æœæ²¡æ³•æ‰¾åˆ°åœˆå­ï¼Œå°±æ‰¾ 20 æœ¬å¯¹åº”é¢†åŸŸçš„ä¹¦ç±æ¥é˜…è¯»ã€‚&lt;/li>
&lt;li>å†³ç­–çš„æ—¶å€™åªæŠ¼æ³¨æ¦‚ç‡ï¼Œä¸è¦çº ç»“äºç‹¬ç«‹äº‹ä»¶çš„ç»“æœã€‚åªè¦åšæŒæŠ¼æ³¨æ¦‚ç‡ï¼Œæœ€ç»ˆèµ¢çš„å°±ä¼šæ›´å¤šã€‚&lt;/li>
&lt;li>ä¸è¦è¿½æ±‚å®Œç¾ï¼Œè¦å…ˆåŠ¨èµ·æ¥ã€‚ä¸è®ºæƒ³åˆ°è¾¾æˆä½•ç§ç›®æ ‡ï¼Œå…³é”®éƒ½åœ¨äºåšã€‚å…‰æƒ³ä¸åšï¼Œä¸€åˆ‡éƒ½æ˜¯å¾’åŠ³ã€‚&lt;/li>
&lt;/ul></description></item><item><title>Weekly Digest: 2024-12-28</title><link>https://z1ggy-o.github.io/weekly/2024/24-12-28/</link><pubDate>Sat, 28 Dec 2024 14:47:25 +0800</pubDate><guid>https://z1ggy-o.github.io/weekly/2024/24-12-28/</guid><description>&lt;h2 id="å­¦åˆ°å•¥">å­¦åˆ°å•¥&lt;/h2>
&lt;ul>
&lt;li>Neovim TJ Advent ç³»åˆ—
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=bTWWFQZqzyI">LSP in Neovim&lt;/a>
&lt;ul>
&lt;li>&lt;code>help: vim.lsp.*&lt;/code> æŸ¥çœ‹ neovim å†…å»º LSP client æ‰€æä¾›çš„ API&lt;/li>
&lt;li>&lt;code>nvim-lspconfig&lt;/code> package ä¸ºå„ç±»è¯­è¨€çš„ LSP client é…ç½®æä¾›äº†åŸºæœ¬çš„æ”¯æŒ
&lt;ul>
&lt;li>&lt;code>:help lspconfig-all&lt;/code> å¯ä»¥æŸ¥çœ‹æƒ³è¦å¯¹åº”è¯­è¨€çš„ server å®‰è£…æ–¹æ³•å’Œé…ç½®æ–¹æ³•&lt;/li>
&lt;li>ä¸€èˆ¬ç°åœ¨éƒ½ä¼šæ­é… &lt;code>mason.nvim&lt;/code> package æ¥è‡ªåŠ¨ä¸‹è½½ï¼Œä½†æ˜¯äº†è§£æœ€åŸºæœ¬çš„æ–¹æ³•æ›´å¥½ä¸€äº›&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=mEqqkHLhlGY">LSP Format&lt;/a>
&lt;ul>
&lt;li>å°±æ˜¯ç”± &lt;code>vim.lsp.buf.format()&lt;/code> æ¥å¯¹å½“å‰ buffer å®ç°çš„ format
&lt;ul>
&lt;li>å› ä¸ºæ˜¯ LSP æä¾›çš„åŠŸèƒ½ï¼Œæœ‰å¯èƒ½æœ‰ä¸€äº› server å¹¶ä¸æ”¯æŒ&lt;/li>
&lt;li>TJ ä»‹ç»äº†ä¸€ä¸ª autocmd çš„åœ¨ server attach çš„æ—¶å€™ç¡®è®¤æ˜¯å¦æ”¯æŒ formatï¼Œå¹¶åŠ¨æ€çš„ç»‘å®šé”®ä½&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>editorconfig&lt;/code> æ˜¯ä¸€ä¸ªæ›´åŠ é€šç”¨çš„æ ¼å¼å®šä¹‰å·¥å…·ï¼Œæ¯”èµ·ç›´æ¥å¯¹ LSP è¿›è¡Œé…ç½®ï¼Œæ›´æ¨èè¿™æ ·çš„æ–¹å¼ï¼Œå› ä¸ºå¾ˆå¤šæˆ‘ä»¬çŸ¥é“çš„ editorï¼Œæ¯”å¦‚ neovim å°±æ”¯æŒå¯¹å®ƒçš„è§£æã€‚è¿™æ ·ï¼Œä¹¦å†™ä¸€æ¬¡è§„åˆ™ï¼Œå°±å¯ä»¥åœ¨å„ä¸ª editor ä¸­éƒ½ä½¿ç”¨äº†&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=xdXE1tOT-qg">Telescope&lt;/a>
&lt;ul>
&lt;li>telescope è¿™ä¸ª fuzzy finder çš„æ ¸å¿ƒå°±æ˜¯ finder, sorter, previewer ä¸‰ä¸ªéƒ¨ä»¶ã€‚Finder æœç´¢å†…å®¹ï¼Œsorter å¯¹æœç´¢åˆ°çš„å†…å®¹è¿›è¡Œæ’åºï¼Œpreviewer å±•ç¤ºå†…å®¹ï¼Œä¸‰è€…ç»„åˆèµ·æ¥å°±æˆäº†ä¸€ä¸ªå¯ç”¨çš„ picker äº†ã€‚&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=xdXE1tOT-qg">Advanced Telescope&lt;/a> é‡Œå°±å±•ç¤ºäº†å¦‚ä½•å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ picker æ¥æ»¡è¶³è‡ªå·±çš„æœç´¢éœ€æ±‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="å¹²äº†å•¥">å¹²äº†å•¥&lt;/h2>
&lt;ul>
&lt;li>è¿™å‘¨é‡æ–°æŠŠ blog ç»„ç»‡èµ·æ¥ã€‚è¿™æ¬¡çš„ç›®æ ‡æ˜¯ä½¿ç”¨ç®€å•çš„å·¥å…·ï¼ŒæŠŠæ—¶é—´ä¸“æ³¨åˆ°å†…å®¹è€Œä¸æ˜¯æ ¼å¼ä¸Šå»ã€‚æœ¬ç¯‡å°±æ˜¯è¿™å”¯ä¸€æ›´æ–°çš„å†…å®¹ :)&lt;/li>
&lt;li>è„šå¯èƒ½æ˜¯ä¸Šå‘¨æœ«å›¢å»ºçš„æ—¶å€™æ‰­ä¼¤äº†ï¼Œå¯¼è‡´åªè·‘äº†ä¸€æ¬¡æ­¥ã€‚ä¼°è®¡ä¸‹å‘¨ä¹Ÿæ²¡æ³•è·‘äº†
&lt;ul>
&lt;li>æ‰åˆšåˆšä¸ºå†¬å¤©è·‘æ­¥ä¹°äº†è£…å¤‡å‘¢ :(&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>æˆ¿å­é‡æ–°ç»­çº¦ã€‚ä¸è®©ä¸­é—´å•†èµšå·®ä»·ï¼Œæ¯ä¸ªæœˆçœäº†å‡ ç™¾å—çš„ç§Ÿé‡‘ğŸ˜&lt;/li>
&lt;/ul>
&lt;h2 id="å·¥ä½œå‘¢">å·¥ä½œå‘¢&lt;/h2>
&lt;ul>
&lt;li>é¡¹ç›®è¿›å…¥é˜¶æ®µå°¾å£°ï¼Œæ²¡æœ‰ä»€ä¹ˆå¯ä»¥å…¬å¼€åˆ†äº«çš„ã€‚å­¦ä¹ äº†ä¸€äº›å†…éƒ¨ä»£ç &lt;/li>
&lt;/ul></description></item><item><title>FAST'20 - BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</title><link>https://z1ggy-o.github.io/papers/wang-2020-bcw/</link><pubDate>Sun, 17 Sep 2023 12:51:01 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/wang-2020-bcw/</guid><description>&lt;h2 id="short-summary">Short Summary&lt;/h2>
&lt;p>HDDs are under utilized in hybrid cloud storage systems which makes SSD to handle most of
the requests.
This shorts the life of SSDs and also wants the utilization of HDDs.&lt;/p>
&lt;p>The authors of this paper find that the write requests can have $Î¼$s-level latency when
using HDD if the buffer in HDD is not full.
They leverage this finding to let HDD to handle write requests if the requests can fit into
the in disk buffer.&lt;/p></description></item><item><title>LevelDB æºç åˆ†æç¬”è®°</title><link>https://z1ggy-o.github.io/posts/leveldb-analysis/</link><pubDate>Mon, 22 Aug 2022 13:30:19 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/leveldb-analysis/</guid><description>&lt;p>æœ€è¿‘ä¸€å‘¨èŠ±äº†äº›æ—¶é—´é˜…è¯»äº† LevelDB çš„ä»£ç ï¼Œäº†è§£ä¸€ä¸‹ LSM-Tree çš„å…·ä½“å®ç°ã€‚åœ¨é˜…è¯»çš„è¿‡ç¨‹ä¸­ç®€ç•¥çš„è®°å½•äº†ä¸€äº›ç¬”è®°ï¼Œæ”¾åœ¨äº†æˆ‘çš„ &lt;a href="https://github.com/z1ggy-o/LevelDB-Notes/tree/main/Notes">Github repo&lt;/a> é‡Œã€‚ç¬”è®°æ˜¯ç”¨è¹©è„šè‹±è¯­å†™çš„ï¼Œä¹Ÿæ²¡æœ‰åšä»»ä½•çš„æ’å›¾ï¼Œå»ºè®®é…åˆ&lt;a href="https://leveldb-handbook.readthedocs.io/zh/latest/basic.html">æ­¤åˆ†ææ–‡æ¡£&lt;/a>ä¸€èµ·é£Ÿç”¨ã€‚&lt;/p></description></item><item><title>OSDI'22 - BlockFlex: Enabling Storage Harvesting with Software-Defined Flash in Modern Cloud Platforms</title><link>https://z1ggy-o.github.io/papers/reidys-2022-blockflexenablingstorage/</link><pubDate>Sun, 07 Aug 2022 11:39:27 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/reidys-2022-blockflexenablingstorage/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Cloud computing system is high virtualised for higher resource utilization.
Recently, cloud providers have developed harvesting techniques to allow evictable virtual machines to use unallocated resources.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>People mostly focus on how to use unallocated computing resources and memory resources, however, there is no harvesting techniques that built for storage resources.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This paper proposes a harvesting framework for storage resources that provides isolation of space, bandwidth, and data security.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>They conduct a characterization study of the storage efficiency in different cloud platforms through trace logs provided by these platforms.&lt;/p></description></item><item><title>TOCS'13 - Spanner: Googleâ€™s Globally Distributed Database</title><link>https://z1ggy-o.github.io/papers/2.c-2013-spanner/</link><pubDate>Sun, 31 Jul 2022 05:40:38 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/2.c-2013-spanner/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Some applications need relation data model and strong consistency which BigTable cannot gives.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>So, Google want to develop a system that focuses on managing cross-datacenter replicate data with database features.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Provides a globally distributed database that shards data across many sets of Paxos state machine in datacenters.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Provide SQL-like interface, strong consistency, and high read performance.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Replication&lt;/p>
&lt;ul>
&lt;li>Use Paxos to replicate data to several nodes, which provides higher availability.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Local Transactions (within a paxos group)&lt;/p></description></item><item><title>Note - x86 æ±‡ç¼–è¯­è¨€ï¼ˆ4ï¼‰ï¼šåˆ†é¡µæœºåˆ¶</title><link>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/04.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/</link><pubDate>Sun, 10 Jul 2022 08:39:25 +0000</pubDate><guid>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/04.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/</guid><description>&lt;hr>
&lt;p>åˆ°äº†è¯¥ç³»åˆ—ç¬”è®°çš„æœ€åä¸€ä¸ªç« èŠ‚äº†ã€‚ä¹‹å‰çš„å‡ ä¸ªç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ€ç›´ç™½çš„ 8086 åˆ†æ®µè®¿é—®æ¨¡å‹ï¼Œåˆè®²åˆ°äº†ä¿æŠ¤æ¨¡å¼ä¸‹çš„åˆ†æ®µè®¿é—®æ¨¡å¼ï¼Œç°åœ¨æˆ‘ä»¬æ¥è®²ä¸€è®²å®é™…ç”Ÿæ´»ä¸­çš„é»˜è®¤å†…å­˜ç®¡ç†æ–¹å¼&amp;ndash;åˆ†é¡µã€‚&lt;/p></description></item><item><title>Note - x86 æ±‡ç¼–è¯­è¨€ï¼ˆ3ï¼‰ï¼šå¤šä»»åŠ¡æ”¯æŒ</title><link>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/03.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/</link><pubDate>Wed, 06 Jul 2022 08:44:20 +0000</pubDate><guid>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/03.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/</guid><description>&lt;hr>
&lt;p>è¿™æ˜¯ç³»åˆ—ç¬”è®°çš„ç¬¬ä¸‰ç¯‡ã€‚åœ¨ä¸Šä¸€ç¯‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¿æŠ¤æ¨¡å¼åŸºäºæ®µé€‰æ‹©å­çš„å¯»å€æ–¹å¼ï¼Œè¿™ä¸€ç¯‡ä¸­æˆ‘ä»¬æ¥è®²ä¸€è®²è¯¥æ¶æ„çš„å¦ä¸€ä¸ªç‰¹ç‚¹ï¼Œå¤šä»»åŠ¡æ”¯æŒã€‚&lt;/p>
&lt;p>å…¶ä½™ç« èŠ‚å¦‚ä¸‹ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../01.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/">ç¬¬ä¸€éƒ¨åˆ†: è®¡ç®—æœºåŸºç¡€å’Œå®æ¨¡å¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="02.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/">ç¬¬äºŒéƒ¨åˆ†ï¼šä¿æŠ¤æ¨¡å¼ä¸‹çš„åˆ†æ®µå¯»å€å’Œæƒé™&lt;/a>&lt;/li>
&lt;li>&lt;a href="03.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/">ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤šä»»åŠ¡æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="04.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86/">ç¬¬å››éƒ¨åˆ†ï¼šåˆ†é¡µæœºåˆ¶&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="ä»»åŠ¡è¿è¡Œç¨‹åºçš„è¡¨è¾¾">ä»»åŠ¡ï¼šè¿è¡Œç¨‹åºçš„è¡¨è¾¾&lt;/h2>
&lt;p>åœ¨ä¹‹å‰çš„ç¯‡ç« ä¸­æˆ‘ä»¬å°½é‡é¿å…äº†ä»»åŠ¡æ¦‚å¿µçš„å‡ºç°ã€‚åœ¨ 80286 ä¹‹å‰ï¼Œè®¡ç®—æœºä¸»è¦ä»¥å•ä»»åŠ¡ï¼ˆsingle-taskingï¼‰çš„æ–¹å¼è¿è¡Œã€‚å³ï¼Œå…ˆé›†ä¸­äºå®Œæˆä¸€ä¸ªç¨‹åºçš„è¿è¡Œï¼Œå†è¿è¡Œä¸‹ä¸€ä¸ªç¨‹åºã€‚æœ‰äº›ç±»ä¼¼äºæœ€åˆ mainframe çš„ batching çš„æ„Ÿè§‰ã€‚80286 é¦–æ¬¡åœ¨å¤„ç†å™¨ç¡¬ä»¶ä¸Šæä¾›äº†å¯¹å¤šä»»åŠ¡çš„æ”¯æŒï¼Œ80386 æ—¶æœŸå¤šä»»åŠ¡å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚&lt;/p></description></item><item><title>Note - x86 æ±‡ç¼–è¯­è¨€ï¼ˆ2ï¼‰ï¼šä¿æŠ¤æ¨¡å¼ä¸‹çš„åˆ†æ®µå¯»å€æ–¹æ³•å’Œæƒé™è®¾è®¡</title><link>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/02.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</link><pubDate>Tue, 05 Jul 2022 09:16:52 +0000</pubDate><guid>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/02.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</guid><description>&lt;hr>
&lt;p>è¿™æ˜¯æœ¬ç³»åˆ—ç¬”è®°çš„ç¬¬äºŒä¸ªéƒ¨åˆ†ã€‚è¿™ä¸ªéƒ¨åˆ†é‡Œï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹ä¿æŠ¤æ¨¡å¼çš„ä¸€å¤§æ”¹å˜â€”â€”å¯»å€æ–¹æ³•ï¼Œä»¥åŠé…åˆè€Œæ¥çš„ä¿æŠ¤åŠŸèƒ½ã€‚å¦ä¸€ä¸ªå¤§æ”¹å˜æ˜¯å¯¹å¤šä»»åŠ¡çš„æ”¯æŒï¼Œæˆ‘ä»¬åœ¨ä¹‹åçš„ç¯‡ç« é‡Œå†è°ˆã€‚&lt;/p></description></item><item><title>Note - x86 æ±‡ç¼–è¯­è¨€ï¼ˆ1ï¼‰ï¼šè®¡ç®—æœºåŸºç¡€å’Œå®æ¨¡å¼</title><link>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/01.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/</link><pubDate>Sun, 03 Jul 2022 09:29:01 +0000</pubDate><guid>https://z1ggy-o.github.io/books/x86-assembly-shuangwang/01.x86%E6%B1%87%E7%BC%96%E7%AC%94%E8%AE%B0%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/</guid><description>&lt;hr>
&lt;p>æœ€è¿‘è¯»å®Œäº†ã€Šx86 æ±‡ç¼–è¯­è¨€ï¼šä»å®æ¨¡å¼åˆ°ä¿æŠ¤æ¨¡å¼ã€‹ä¸€ä¹¦ã€‚æ„Ÿè§‰æ­¤ä¹¦æ˜¯äº†è§£ Intel æ¶æ„ CPU è¿è¡Œæ–¹å¼çš„å¥½ææ–™ã€‚æ‰€ä»¥ï¼Œæˆ‘å‡†å¤‡ç”¨å››ä¸ªç¯‡ç« æ¥å†™ä¸€ä»½è¯»ä¹¦ç¬”è®°ï¼Œç®€ç•¥åœ°ä»‹ç»ä¸€ä¸‹å†…å­˜å¯»å€æ–¹å¼ï¼Œä»¥åŠå¤„ç†å™¨å¯¹å¤šä»»åŠ¡çš„æ”¯æŒã€‚ç¬”è®°çš„å†…å®¹æ„åœ¨æŒ‡å‡ºä¹¦ä¸­çš„æ¡†æ¶é‡ç‚¹ï¼Œä½œä¸ºä¸€ä¸ªå¿«é€Ÿå¤ä¹ å’Œæ£€ç´¢çš„å·¥å…·ï¼Œè€Œéè¯¦å®çš„å†…å®¹ã€‚&lt;/p></description></item><item><title>SOSP'97 - Frangipani: a scalable distributed file system</title><link>https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/</link><pubDate>Fri, 01 Jul 2022 13:30:21 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/hekkath-1997-frangipaniscalabledistributed/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>File system administration for a large, growing computer installation is a laborious task.&lt;/li>
&lt;li>A scalable distributed file system that can handle system recovery, reconfiguration, and load balancing with little human involvement is what we want.&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>A real system that shows us how to do cache coherence and distributed transactions.&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>The file system part is implemented as a kernel module, which enables the file system to take advantage of all the existing OS functionalities, e.g., page cache.&lt;/li>
&lt;li>Data is stored into a virtual disks, which is a distributed storage pool that shared by all file system servers.&lt;/li>
&lt;li>All file system operations are protected by a lock server. Client will invalid the cache when it release the lock. As a result, client can only see clean data and the cache coherence is ensured.&lt;/li>
&lt;li>Because the client decides when to give the lock back, Frangipani can provide transactional file-system operations. (take locks on all data object we need first, only release these locks when all operations are finished.)&lt;/li>
&lt;li>Write-ahead logging is used for crash recovery. Because all the data are stored in the shared distributed storage, the log can be read by other servers and recover easily.&lt;/li>
&lt;li>The underlying distributed storage and the lock server are both using Paxos to ensure availability.&lt;/li>
&lt;/ul>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;ul>
&lt;li>They evaluated the system with some basic file system operations, such as create directories, copy files, and scan files e.t.c.&lt;/li>
&lt;li>They tested both the local performance and the scalability of the proposed system.&lt;/li>
&lt;/ul>
&lt;h2 id="the-main-finding-of-the-paper">The Main Finding of the Paper&lt;/h2>
&lt;ul>
&lt;li>Complex clients sharing simple storage can have better scalability.&lt;/li>
&lt;li>Distributed lock can be used to achieve cache coherence and distributed transaction.&lt;/li>
&lt;/ul></description></item><item><title>VLDB'17 - An empirical evaluation of in-memory multi-version concurrency control</title><link>https://z1ggy-o.github.io/papers/wu-2017-empiricalevaluationinmemory/</link><pubDate>Fri, 01 Jul 2022 12:36:05 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/wu-2017-empiricalevaluationinmemory/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;ul>
&lt;li>MVCC is the most popular scheme used in DBMSs developed in the last decade. However, there is no standards about how to implement MVCC.&lt;/li>
&lt;li>Many DBMSs tell people that they use MVCC and how they implement it. There are several design choices people need to make, however, no one tells why they choose such way to implement their MVCC.&lt;/li>
&lt;li>This paper gives a comprehensive evaluation about MVCC in main memory DBMSs and shows the trade-offs of different design choices.&lt;/li>
&lt;/ul>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>A good introduction about MVCC.&lt;/li>
&lt;li>Evaluation about how concurrency control protocol, version storage, garbage collection, and index management affect performance on a real in-memory DBMS.&lt;/li>
&lt;li>Evaluation of different configurations that used by main stream DBMSs.&lt;/li>
&lt;li>Advisings about how to achieve higher scalability.&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>This is an evaluation paper. There is no &amp;ldquo;solution&amp;rdquo; but evaluation analyses.&lt;/li>
&lt;li>People mainly focus on concurrency control protocols when they talk about scalability. However, the evaluation results show that the version storage scheme is also one of the most important components to scaling an in-memory MVCC DBMS in a multi-core environment.&lt;/li>
&lt;li>Delta storage scheme is good for write-intensive workloads especially only a subset of the attributes is modified. However, delta storage can have slow performance on read-heavy analytical workloads because it spends more time on traversing version chains.&lt;/li>
&lt;li>Most DBMSs choose to use tuple-level GC. However, the evaluation result shows that transaction-level can provide higher performance (at least in main memory DBMS) because it has smaller memory footprint.&lt;/li>
&lt;li>In terms of index management, logical pointer is always a better choice than physical pointers.&lt;/li>
&lt;li>The design choices that Oracle/MySQL and NuoDB made seems have the best performance.&lt;/li>
&lt;/ul>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;ul>
&lt;li>They uses a DBMS implemented in CMU, called Peloton.&lt;/li>
&lt;li>The evaluation platform has 40 cores with 128 GB memory.&lt;/li>
&lt;li>Workloads are YCSB and TPC-C (both OLTP)&lt;/li>
&lt;/ul>
&lt;h2 id="the-main-finding-of-this-paper">The main finding of this paper&lt;/h2>
&lt;ul>
&lt;li>If you want to learn MVCC, read this one.&lt;/li>
&lt;li>Still, &amp;ldquo;Measure, Then build&amp;rdquo;.&lt;/li>
&lt;/ul></description></item><item><title>VLDB'14 - Staring into the abyss: An evaluation of concurrency control with one thousand cores</title><link>https://z1ggy-o.github.io/papers/yu-2014-staringabyssevaluation/</link><pubDate>Fri, 01 Jul 2022 12:21:38 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/yu-2014-staringabyssevaluation/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>We are moving to the many-core architecture era, however, many design of database systems are still based on optimizing of single-threaded performance.&lt;/p>
&lt;p>To understand how to design high performance DBMS for the future many-core architecture to achieve high scalability, addressing bottlenecks in the system is necessary.&lt;/p>
&lt;p>This paper focus on concurrency control schemes.
(spoiler: the [following research]([[@An empirical evaluation of in-memory multi-version concurrency control]]) of [[Andrew Pavlo]] found out concurrency control schemes are not the most important component that affect the scalability of main memory DBMS on many-core environment )&lt;/p></description></item><item><title>ATC'10 - ZooKeeper: Wait-free Coordination for Internet-scale Systems</title><link>https://z1ggy-o.github.io/papers/hunt-2010-zookeeperwaitfreecoordination/</link><pubDate>Mon, 23 May 2022 09:00:39 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/hunt-2010-zookeeperwaitfreecoordination/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Large-scale distributed applications require different forms of coordination.&lt;/p>
&lt;p>Usually, people develop services for each of the different coordination needs. As a result, developers are constrained to a fixed set of primitives.&lt;/p>
&lt;h2 id="contribution">Contribution&lt;/h2>
&lt;ul>
&lt;li>Exposes APIs that enables application developers to implement their own primitives, without changes to the service core.&lt;/li>
&lt;li>Achieve high performance by relaxing consistency guarantees&lt;/li>
&lt;/ul>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>ZooKeeper provides to its clients the abstraction of a set of data nodes (znodes). These data nodes are organized like a traditional file system.&lt;/p></description></item><item><title>SOSP '97 - The design of a practical system for fault-tolerant virtual machines</title><link>https://z1ggy-o.github.io/papers/scales-2010-designpracticalsystem/</link><pubDate>Mon, 23 May 2022 09:00:39 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/scales-2010-designpracticalsystem/</guid><description>&lt;h2 id="motivation-1">MOTIVATION &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;p>A common approach to implementing fault-tolerant servers is the primary/backup approach. One way of replicating the state on the backup server is to ship changes to all state of the primary (e.g., CPU, memory, and I/Os devices) to the backup. However this approach needs high network bandwidth, and also leads to significant performance issues.&lt;/p>
&lt;h2 id="contribution">CONTRIBUTION&lt;/h2>
&lt;p>Shipping all the changes to the backup server asks for high network bandwidth. To reduce the demand of network, we can use the &amp;ldquo;state-machine approach&amp;rdquo;, which models the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.&lt;/p></description></item><item><title>SOSP'03 - The Google file system</title><link>https://z1ggy-o.github.io/papers/ghemawat-2003-googlefilesystem/</link><pubDate>Sun, 22 May 2022 11:11:56 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/ghemawat-2003-googlefilesystem/</guid><description>&lt;h2 id="motivation">MOTIVATION&lt;/h2>
&lt;p>Google needs a new distributed file system to meet the rapidly growing demands of Googleâ€™s data processing needs (e.g., the MapReduce).&lt;/p>
&lt;p>Because Google is facing some technical challenges different with general use cases, they think it is better to develop a system that fits them well instead of following the traditional choices. For example, they chosen to trade off consistency for better performance.&lt;/p>
&lt;h2 id="contribution">CONTRIBUTION&lt;/h2>
&lt;p>They designed and implemented a distributed file system, GFS. This system can leverage clusters consisted with large number of the machines. The design puts a lot of efforts on fault tolerance and availability because they think component failures are the norm rather than the exception.&lt;/p></description></item><item><title>OSDI'04 - MapReduce: simplified data processing on large clusters</title><link>https://z1ggy-o.github.io/papers/dean-2004-mapreducesimplifieddata/</link><pubDate>Mon, 16 May 2022 09:44:18 +0000</pubDate><guid>https://z1ggy-o.github.io/papers/dean-2004-mapreducesimplifieddata/</guid><description>&lt;h2 id="motivation">MOTIVATION&lt;/h2>
&lt;p>Google does a lot of straightforward computations in their workload. However, since the input data is large, they need to distribute the computations in order to finish the job in a reasonable amount of time.&lt;/p>
&lt;p>To parallelize the computation, to distribute the data, and to handle failures make the original simple computation code complex. Thus, we need a better way to handle these issues, so the programmer only needs to focus on the computation task itself and does not need to be a distributed systems expert.&lt;/p></description></item><item><title>å¹»è¯»ï¼Œåˆ°åº•æ˜¯æ€ä¹ˆä¸€å›äº‹å„¿</title><link>https://z1ggy-o.github.io/posts/isolation-levels-and-phtantom/</link><pubDate>Thu, 12 May 2022 07:11:54 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/isolation-levels-and-phtantom/</guid><description>&lt;p>å¹»è¯» (phantom phenomenon) æ˜¯æ•°æ®åº“ä½¿ç”¨ä¸­ç»å¸¸æåˆ°çš„ä¸€ä¸ªé—®é¢˜ã€‚ä¸€ä¸ªå¸¸è§çš„é¢è¯•é—®é¢˜å°±æ˜¯ï¼šæŸä¸ªæ•°æ®åº“ï¼Œåœ¨æŸä¸ª isolation level ä¸‹ï¼Œä¼šä¸ä¼šå‡ºç°å¹»è¯»ï¼Œä»¥åŠå…¶è§£å†³æ–¹æ³•ã€‚&lt;/p>
&lt;p>ç½‘ä¸Šæœ‰è®¸å¤šå…³äºå¹»è¯»çš„æ–‡ç« ï¼Œä½†æ˜¯åœ¨è¯»å®Œä¹‹åå‘ç°ï¼Œå¤§å¤šæ•°çš„è¯´æ˜éƒ½æµ®äºè¡¨é¢ï¼Œå¥½åƒä½œè€…ä»¬è‡ªå·±ä¹Ÿå¹¶æ²¡æœ‰å¼„æ¸…æ¥šå¹»è¯»çš„æœ¬è´¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘æƒ³åˆ©ç”¨æ•°æ®åº“çš„ä¸€äº›é«˜å±‚æŠ½è±¡æ¦‚å¿µï¼Œæ¥é˜è¿°å¹»è¯»çš„æœ¬è´¨ã€‚è™½ç„¶ä¸æ¶‰åŠä»»ä½•çš„å…·ä½“å®ç°ï¼Œä½†ç›¸ä¿¡ä½ åœ¨äº†è§£åˆ°è¿™äº›æ¦‚å¿µä¹‹åï¼Œå¯ä»¥å¾ˆå¿«åœ°ç†è§£å¹»è¯»ï¼Œä»¥åŠå„ç§å¹»è¯»çš„å¤„ç†æ–¹æ³•ã€‚&lt;/p></description></item><item><title>CMU 15-445 2021 Fall Project#4</title><link>https://z1ggy-o.github.io/posts/projects/cmu15445-project04/</link><pubDate>Mon, 09 May 2022 14:54:38 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/projects/cmu15445-project04/</guid><description>&lt;p>We are implementing a lock-based concurrency control scheme in this project. More specifically, the &lt;strong>strict two phase locking protocol&lt;/strong>.&lt;/p>
&lt;p>The concept is not that hard. However, there are many implementation details we need to care about. Especially, sometimes we need to guess what is the test code wants.&lt;/p>
&lt;p>You need to read the source code carefully. The instruction of this project is kind vague, or even wrong.&lt;/p>
&lt;h2 id="task-1---lock-manager">Task 1 - Lock Manager&lt;/h2>
&lt;p>Read &lt;code>transaction.h&lt;/code>, &lt;code>transaction_manager.h&lt;/code>, and &lt;code>log_manager.h&lt;/code> to learn the APIs first.&lt;/p></description></item><item><title>CMU 15-445 2021 Fall Project#3</title><link>https://z1ggy-o.github.io/posts/projects/cmu15445-project03/</link><pubDate>Sat, 30 Apr 2022 12:37:56 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/projects/cmu15445-project03/</guid><description>&lt;p>In this project, we will Implement &lt;strong>executors&lt;/strong> for taking query plan nodes and executing them.&lt;/p>
&lt;p>We are using the iterator query processing model (i.e., the Volcano model). Each executor implements a loop that continues calling &lt;code>Next&lt;/code> on its children to retrieve tuples and process them one-by-one.&lt;/p>
&lt;h2 id="how-executor-works">How &lt;code>executor&lt;/code> works&lt;/h2>
&lt;p>Before start coding, we need to learn a lot from the related source code first. The instruction does not show all the details and I believe they did this intentionally.&lt;/p></description></item><item><title>CMU 15-445 2021 Fall Project#2</title><link>https://z1ggy-o.github.io/posts/projects/cmu15445_project2/</link><pubDate>Sat, 19 Mar 2022 15:58:46 +0800</pubDate><guid>https://z1ggy-o.github.io/posts/projects/cmu15445_project2/</guid><description>&lt;blockquote>
&lt;p>Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&amp;rsquo;t think that would be very different from public the source code.&lt;/p>
&lt;/blockquote>
&lt;h2 id="task-1---page-layouts">Task #1 - Page Layouts&lt;/h2>
&lt;p>Because we want to persist the hash table instead of rebuild it every time, we need to design the layout that we use to store the hash table in the disks.&lt;/p></description></item><item><title>CMU 15-445 2021 Fall Project#1</title><link>https://z1ggy-o.github.io/posts/projects/cmu15445_project1/</link><pubDate>Mon, 14 Mar 2022 21:23:00 +0800</pubDate><guid>https://z1ggy-o.github.io/posts/projects/cmu15445_project1/</guid><description>&lt;blockquote>
&lt;p>Because the course asks us to not sharing source code, here, I will only jot down some hits to help you (or maybe only me, kk) to finish the project. I will not even describe the process of any specific function, because I don&amp;rsquo;t think that would be very different from public the source code.&lt;/p>
&lt;/blockquote>
&lt;h2 id="task-1---lru-replacement-policy">Task #1 - LRU Replacement Policy&lt;/h2>
&lt;p>&lt;code>BufferPoolManger&lt;/code> contains all the frames.
&lt;code>LRUReplacer&lt;/code> is an implementation of the &lt;code>Replacer&lt;/code> and it helps &lt;code>BufferPoolManger&lt;/code> to manage these frames.&lt;/p></description></item><item><title>SOSP'19 - File Systems Unift as Distributed Storage Backends: Lessons from 10 Years of Ceph Evolution</title><link>https://z1ggy-o.github.io/papers/1.aghayev-2019-filesystemunfit/</link><pubDate>Sun, 17 Jan 2021 00:29:00 +0900</pubDate><guid>https://z1ggy-o.github.io/papers/1.aghayev-2019-filesystemunfit/</guid><description>&lt;h2 id="short-summary">Short Summary&lt;/h2>
&lt;p>This paper mostly consists of two parts. The first part tells us why the &lt;code>FileStore&lt;/code> has performance issues.
And the second part tells us how Ceph team build &lt;code>BlueStore&lt;/code> based on the
lessons that they learnt from &lt;code>FileStore&lt;/code>.&lt;/p>
&lt;p>The main ideas of &lt;code>BlueStore&lt;/code> are:&lt;/p>
&lt;ol>
&lt;li>Avoid using local file system to store and represent Ceph objects&lt;/li>
&lt;li>Use KV-store to provide transaction mechanism instead of build it by ourself&lt;/li>
&lt;/ol>
&lt;h2 id="what-s-the-problem">What&amp;rsquo;s the problem&lt;/h2>
&lt;p>There is a software called &lt;code>storage backend&lt;/code> in Ceph. The &lt;code>storage backend&lt;/code> is
responsible to accept requests from upper layer of Ceph and do the real I/O on
storage devices.&lt;/p></description></item><item><title>FAST'20 - Read as Needed: Building WiSER, a Flash-Optimized Search Engine</title><link>https://z1ggy-o.github.io/papers/he-2020-readasneeded/</link><pubDate>Sat, 26 Dec 2020 20:15:00 +0900</pubDate><guid>https://z1ggy-o.github.io/papers/he-2020-readasneeded/</guid><description>&lt;h2 id="short-summary">Short Summary&lt;/h2>
&lt;p>This paper proposed a NAND-flash SSD-friendly full text engine. This engine can
achieve better performance than existing engines with much less memory requested.&lt;/p>
&lt;p>They reduce the unnecessary I/O (both the number of I/O and the volume). The
engine does not cache data into memory, instead, read data every time when query
arrive.&lt;/p>
&lt;p>They also tried to increase the request size to exploit SSD internal parallelism.&lt;/p>
&lt;h2 id="what-is-the-problem">What Is the Problem&lt;/h2>
&lt;p>Search engines pose great challenges to storage systems:&lt;/p></description></item><item><title>2PC v.s. 3PC: ä¸€å¥è¯çš„æ€»ç»“</title><link>https://z1ggy-o.github.io/posts/2pc-vs-3pc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://z1ggy-o.github.io/posts/2pc-vs-3pc/</guid><description>&lt;p>æœ€è¿‘åœ¨å‡†å¤‡é¢è¯•çš„è¿‡ç¨‹ä¸­çœ‹åˆ°æœ‰è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯è®©æ¯”è¾ƒä¸€ä¸‹ 2PC å’Œ 3PCã€‚åœ¨ç½‘ä¸Šæ‰¾äº†ä¸€äº›æ–‡ç« æ¥è¯»ï¼Œæ„Ÿè§‰éƒ½æ²¡æœ‰ååˆ†ç®€æ´åœ°è¯´æ˜ä¸¤è€…ä¹‹é—´æœ€åŸºæœ¬çš„åŒºåˆ«ç‚¹ã€‚æ‰€ä»¥ï¼Œåœ¨è¿™é‡Œå†™ä¸€ç¯‡å°æ–‡ï¼Œè¡¨è¾¾ä¸€ä¸‹è‡ªå·±å¯¹ 2PCï¼Œ3PC æœ€æ ¸å¿ƒåŒºåˆ«çš„ç†è§£ã€‚&lt;/p>
&lt;p>æœ€é‡è¦çš„æœ€å…ˆè¯´ï¼Œåœ¨æˆ‘çœ‹æ¥ä¸¤è€…çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼š&lt;strong>å‚ä¸è€…é—´æ˜¯å¦å¯¹ transaction commit/abort å»ºç«‹äº†å…±è¯†&lt;/strong>ã€‚3PC çš„å‚ä¸è€…ä¹‹é—´å¯¹ commit çš„æˆç«‹æ˜¯å…·æœ‰å…±è¯†çš„ï¼Œ2PC åˆ™æ²¡æœ‰ã€‚&lt;/p>
&lt;h2 id="3pc-ç›¸æ¯”-2pc-å¸¦æ¥äº†ä»€ä¹ˆ">3PC ç›¸æ¯” 2PC å¸¦æ¥äº†ä»€ä¹ˆï¼Ÿ&lt;/h2>
&lt;p>å¤§å®¶éƒ½çŸ¥é“ï¼Œ3PC ç›¸æ¯”äº 2PC æ¥è¯´å¤šäº†ä¸¤ä¸ªä¸œè¥¿ï¼š&lt;/p>
&lt;ol>
&lt;li>å¢åŠ äº† time out&lt;/li>
&lt;li>commit phase è¢«åˆ†å‰²ä¸ºäº† prepare commit å’Œ do commit ä¸¤ä¸ªéƒ¨åˆ†&lt;/li>
&lt;/ol>
&lt;p>å¢åŠ ä¸€ä¸ª prepare commit phase å¸¦æ¥äº†ä»€ä¹ˆå‘¢ï¼Ÿæ˜¯é›†ç¾¤å¯¹ commit è¿™ä¸€å†³å®šçš„å…±è¯†ã€‚&lt;/p>
&lt;p>åœ¨ 2PC åè®®ä¸­ï¼Œåè°ƒè€…å•æ–¹é¢å‘å‚ä¸è€…å‘é€ä¸€æ¬¡ commit æ¶ˆæ¯ã€‚è¿™ä¸ªæ¶ˆæ¯æœ‰ä¸¤ä¸ªå«ä¹‰ï¼š&lt;/p>
&lt;ol>
&lt;li>è®©å‚ä¸è€…è¿›è¡Œ commit&lt;/li>
&lt;li>æ‰€æœ‰å‚ä¸è€…éƒ½è®¤å¯æ­¤ commit&lt;/li>
&lt;/ol>
&lt;p>ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºä¸€ä¸ª transactionï¼Œæ˜¯ commit è¿˜æ˜¯ abort è¿™ä¸ªå†³å®šæœ¬èº«åªæœ‰åè°ƒè€…çŸ¥é“ã€‚ä¸€æ—¦åè°ƒè€…æ•…éšœï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯å°±æ¶ˆå¤±äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ 2PC çš„åè°ƒè€…æ˜¯å•ç‚¹æ•…éšœç‚¹ã€‚&lt;/p>
&lt;p>3PC åè®®ä¸­ï¼Œprepare commit æ¶ˆæ¯è®©ä¸€ä¸ª transaction è¯¥ commit è¿˜æ˜¯ abort è¿™ä¸ªå†³å®šæœ¬èº«è¢«ä¼ å¯¼åˆ°äº†æ‰€æœ‰å‚ä¸è€…å¤„ã€‚å¦‚æ­¤ä¸€æ¥ï¼Œå¦‚æœåè°ƒè€…æ•…éšœï¼Œå‚ä¸è€…ä»¬å¯ä»¥æ ¹æ® prepare commit çš„æƒ…å†µç»§ç»­å·¥ä½œã€‚&lt;/p></description></item></channel></rss>